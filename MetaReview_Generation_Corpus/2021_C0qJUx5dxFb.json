{"year": "2021", "forum": "C0qJUx5dxFb", "title": "Neural networks with late-phase weights", "decision": "Accept (Poster)", "meta_review": "This paper proposes to learn an ensemble of weights given a set of base weights from some point late in normal training. The authors apply this approach to a number of configurations and find modest performance improvements for normal test settings and larger improvements for out of distribution settings. While reviewers had some concerns about the size of the improvement relative to baselines, all reviewers agreed that the proposed method is interesting and will likely impact future work, especially given the new experiments provided by the authors. I recommend that the paper be accepted. ", "reviews": [{"review_id": "C0qJUx5dxFb-0", "review_text": "To improve the generalization performance of SGD methods , this paper proposes to use an efficient ensemble-like approach which computes an average of an ensemble of SGD weights when retrained from some late-phase of SGD dynamics . This idea is different to most recent ensemble-based approaches which aim to average the predictions of the models . The paper focuses on some specific layers of neural networks in order to apply the late-phase training . The batch normalization layers are shown to be simple and effective . Some other layers are also analyzed , including a recently introduced rank-1 multiplicative matrix weights idea for full-connected layers . Section 3 presents the numerical results and show that the generalization of SGD is more-or-less improved on various benchmarks . Explanation of why the generalization is improved in relation with the flatness of energy landscape is also discussed . I find that this approach is quite sensitive the choice of the hyper-parameters , such as the beginning of the late-phase T0 , and the noise perturbation sigma0 . It is written in Section 2.1 that in practice \u2026 sigma0 > 0 yields a set of models \u2026 this results in improved final generalization . However , in the result of ImageNet in Section 3.3 , the sigma0 equals to 0 . Thus , it is not conclusive that sigma0 > 0 is better . As the improvement in Section 3.3 seems marginal compared to the baseline and the standard deviation , it thus does not fully support the effectiveness of the batch normalization layers . I would recommend using some other dataset or models , but with a more consistent set of hyper-parameters . In terms of writing , I would recommend to write out the full algorithm of Alg . 1 or at least in the Appendix , including the variant of the SGD momentum and Adam . The SWA is also worth writing out clearly , which is not clear to the reader . Is the DeepEnsemble result in Table 1 from SGD or SWA ? This is not clear from the text . Overall , I think both the methodology and the writing need to be improved . # # The revisions made by the authors have addressed all my concerns .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . As detailed below , we have tried to address your criticism to the best of our knowledge , and we remain open to any questions that you may have , that can help you raise your score . * * I find that this approach is quite sensitive the choice of the hyper-parameters , such as the beginning of the late-phase T0 , and the noise perturbation sigma0 . It is written in Section 2.1 that in practice \u2026 sigma0 > 0 yields a set of models \u2026 this results in improved final generalization . However , in the result of ImageNet in Section 3.3 , the sigma0 equals to 0 . Thus , it is not conclusive that sigma0 > 0 is better . * Thank you for this comment . We recognize the seeming inconsistency of our choice of $ \\sigma_0 $ , and now present our results on CIFAR and ImageNet in the main text all using a consistent choice of $ \\sigma_0 = 0 $ ( and $ T_0 $ , for non-pretrained models ) . A non-zero $ \\sigma_0 = 0.5 $ is now employed only to generate a diverse late-phase ensemble ( see Section 3.2 , Table 4 ) that is not averaged in weight space . This method shows strong OOD performance in comparison to other comparable techniques that are efficiently-trained but still require to integrate predictions during inference ( SWAG , MC-dropout , BatchEnsemble ) . An overview of the updated performance can be found in the table below . \\ +-+ -- ++-+\\ | |Testacc . ( % ) | & nbsp ; & nbsp ; OOD | mCE |\\ +-+ -- ++-+\\ | Base ( SGD ) |81.35+/-0.16 |0.802+/-0.019 |47.84+/-0.41 |\\ | Dropout ( Mean ) ( SGD ) |81.31+/-0.20 |0.802+/-0.030 |48.97+/-0.33 |\\ | Late-phase BatchNorm ( SGD ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.87+/-0.14 |0.836+/-0.012 |45.59+/-0.25 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | MC-Dropout ( SGD ) & nbsp ; & nbsp ; |81.55+/-0.11 |0.823+/-0.049 |48.09+/-0.36 |\\ | SWAG ( SWA ) & nbsp ; |82.12+/-0.03 |0.828+/-0.027 | & nbsp ; & nbsp ; |\\ | BatchEnsemble ( SGD ) |81.25+/-0.10 |0.829+/-0.019 | & nbsp ; & nbsp ; |\\ | Late-phase BatchNorm ( SGD , non-averaged ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.71+/-0.10 |0.862+/-0.009 |46.21+/-0.29 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | Deepensemble ( SGD Here we show that for a large range of $ T_0 $ , i.e. , $ T_0 > 80 $ , we improve on top of the baseline ( CIFAR-10 - 96.16 % and CIFAR-100 - 81.31 % ) in test set accuracy and out-of-distribution detection . See a small excerpt of the analyses here : ++++ \\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; T_0 & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR10 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR100 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; \\ ++++\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 40 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.34 & nbsp ; +/- & nbsp ; 0.08 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 79.69 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 80 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.50 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 81.72 & nbsp ; +/- & nbsp ; 0.18 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; 100 & nbsp ; & nbsp ; & nbsp ; | & nbsp"}, {"review_id": "C0qJUx5dxFb-1", "review_text": "Summary : The paper proposes a method to improve solutions found by SGD by ensembling subsets of weights in late-phase . A family of low-dimensional late-phase methods are analyzed and shown to improve generalization in CIFAR-10/100 , ImageNet and enwik8 . Authors also analyze the method in more tractable noisy quadratic settings . Contribution of the authors is that rather obtaining ensemble they utilize efficient ensemble to guide SGD training and ultimately obtain a single model . Reason for score : While the paper discusses efficient ways of utilizing late-phase weight ensemble and improving SGD training , the demonstrated benefit is not significant enough for practitioners to pursue the method . Without strong practical application potential , merit of the proposed method is weak since it does not obviously elucidate some aspects of neural network training . Pros : The paper is clearly written and easy to understand the proposed method is . It is well structured that helps to improve the clarity . Proposed method tackles a significant problem in the standard ensemble method in which both training/inference computation can be quite costly . The paper \u2019 s method only ensembles subset of weights therefore added training cost is minimal and since inference is done on averaged weight , it becomes essentially a single model . Among various late-phase schemes , BatchNorm late-phase seems to work well which is widely used among vision models so easily applicable . Also since late-phase can be applied post-pretraining , it can be used to improve pre-trained models . As far as I can tell various experimental conditions are very well controlled and thoughtfully designed . Cons : The idea of weight averaging is not so novel as duly noted by the authors . Main question arises for the paper is whether the proposed method is worth the effort . While all experiments show that the proposed method improves the baseline somewhat , deep ensemble baselines remain strong . Also quoted difference between methods does not mean statistically significant effect ( see Vincent Vanhoucke \u2019 s article on reporting significant figures https : //towardsdatascience.com/digit-significance-in-machine-learning-dea05dd6b85b ) . According to this article , results reported in Table 1 , CIFAR-10 in WRN , a significant figure with a 10k test set should be around 0.2 % and differences between different methods are at best marginal . This can be applied to most tables and except for Deep Ensemble \u2019 s improvement other differences are not very significant . I wonder as discussed by the authors , this is due to mostly the benefit of ensembles is through incorporating different modes as argued in [ Fort et al. , 2020 ] rather than a single mode . I imagine a single mode ensemble could be beneficial when variance within the mode is large , however for models considered by the authors seem to have small model variance which minimizes effect of technique utilizing single mode . While \\sigma_0 and T_0 are hyperparameters of the algorithm , no good way to determine it is explained . The role of section 3.1 is not clear . For one thing , the legend in Figure 1 is confusing where the role of non-integer K is mysterious to me . I would suggest clarifying what the message of the section would be in context of understanding late-phase weight models . Nits and additional feedback : Anonymized link is neither there in the main paper or included as supplementary material . If the authors intended to include the code , this is a note that code can not be found to the reviewers . For models that do not use BatchNorm , I believe most interest to practitioners would be using Transformer based models . I wonder if rank-1 late-phase or LayerNorm late-phase would show improvements in this case . Was \u201c Late-phase classification layers \u201d ever evaluated or discussed in the main paper ? I find some discussion on the appendix but seem to be missing in the main text . I thank the authors for their hard work addressing issues raised by the reviewers . Authors have answered many issues pointed out ( by improved performance and showing robustness to hyperparameters ) and I 've increased my score from 5 to 6 , and support accepting the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thorough review and feedback . We ran a number of new experiments , which include an extended set of OOD results , the study of more network architectures , and new well-known baselines ( dropout , MC-dropout , and BatchEnsemble ) with comparable computational and memory requirements to our method . We reply to your comments point by point : * * BatchNorm late-phase seems to work well which is widely used among vision models so easily applicable . Also since late-phase can be applied post-pretraining , it can be used to improve pre-trained models . * To facilitate broad adoption of our method we will provide a PyTorch drop-in replacement for a standard BatchNorm layer with the final version of the paper . Our aim is to make our method as easy to implement as other well-established complementary techniques like dropout or SWA . * * The idea of weight averaging is not so novel as duly noted by the authors . * While previous optimization algorithms employ various sorts of weight averaging ( notably , SWA and Polyak averaging , which maintain running temporal averages ) , we would like to highlight that our approach differs in that we take a single , simple spatial average in a low-dimensional weight space . This simple averaging is made possible thanks to the late-phase ensembling that we introduce in our paper , as corroborated by our experiments where $ T_0 $ is varied ( Table 12 , Figure 5 ) . * * While the paper discusses efficient ways of utilizing late-phase weight ensemble and improving SGD training , the demonstrated benefit is not significant enough for practitioners to pursue the method . Without strong practical application potential , merit of the proposed method is weak since it does not obviously elucidate some aspects of neural network training . \u2026.Main question arises for the paper is whether the proposed method is worth the effort . While all experiments show that the proposed method improves the baseline somewhat , deep ensemble baselines remain strong . Also quoted difference between methods does not mean statistically significant effect [ ... ] . Results reported in Table 1 , CIFAR-10 in WRN , a significant figure with a 10k test set should be around 0.2 % and differences between different methods are at best marginal . This can be applied to most tables and except for Deep Ensemble \u2019 s improvement other differences are not very significant . * To convince you that our paper is worth accepting we would like to point out our improved results ( Tables 2 , 3 , 4 and 5 in the main text ) . Here , we would like to highlight that on a WRN 28-14 we increase accuracy from 96.75 % ( SWA ) to 97.45 % ( Late-phase+SWA ) on CIFAR-10 , and from 84.01 % ( SWA ) to 85.00 % ( Late-phase+SWA ) on CIFAR-100 . These high accuracies place us among the very best available results for WRNs in PWC [ 1 , 2 ] ( 1st place on CIFAR-100 and 2nd place on CIFAR-10 ) . Furthermore , these results also show our method and SWA , one of the strongest methods for improving generalization in neural networks , are complementary . On the point raised over statistical significance , we stress that our CIFAR results are obtained with a consistent choice of $ T_0 $ and $ \\sigma_0 $ , across a number of different architectures . Our updated results also show that our method achieves strong performance in out-of-distribution detection problems and it is robust to input data corruptions : ( Table 4 and 16 in the paper ) . \\ +-+ -- ++-+\\ | |Testacc . ( % ) | & nbsp ; & nbsp ; OOD | mCE |\\ +-+ -- ++-+\\ | Base ( SGD ) |81.35+/-0.16 |0.802+/-0.019 |47.84+/-0.41 |\\ | Dropout ( Mean ) ( SGD ) |81.31+/-0.20 |0.802+/-0.030 |48.97+/-0.33 |\\ | Late-phase BatchNorm ( SGD ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.87+/-0.14 |0.836+/-0.012 |45.59+/-0.25 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | MC-Dropout ( SGD ) & nbsp ; & nbsp ; |81.55+/-0.11 |0.823+/-0.049 |48.09+/-0.36 |\\ | SWAG ( SWA ) & nbsp ; |82.12+/-0.03 |0.828+/-0.027 | & nbsp ; & nbsp ; |\\ | BatchEnsemble ( SGD ) |81.25+/-0.10 |0.829+/-0.019 | & nbsp ; & nbsp ; |\\ | Late-phase BatchNorm ( SGD , non-averaged ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.71+/-0.10 |0.862+/-0.009 |46.21+/-0.29 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | Deepensemble ( SGD"}, {"review_id": "C0qJUx5dxFb-2", "review_text": "# # # Summary The authors propose late-phase weights , a method of updating the weights near the end of training via a splitting and ensembling mechanism . They analyze the benefits in the noisy quadratic setting . The method improves validation performance on a range of image recognition tasks and on enwiki8 . # # # Comments * The weight interaction functions $ h $ should be more explicitly defined rather than just described in text . * The paper is overall well written and flows smoothly . * I think there should be more discussion on the choice of $ T_0 $ . For example , in table 1 , why does SGD perform worse when $ T_0=0 $ ? It would be good to get a sense of robustness to this hyperparameter . * Good results on CIFAR . Late-phase weights are shown to boost performance over SGD and to be complementary with SWA . There are some benefits in the OOD setting as well . # # # Recommendation / Justification I vote to accept the paper . The idea is interesting , well-motivated , and seems straightforward to incorporate into existing pipelines . However , the improvements seems modest in some settings ( e.g.ImageNet ) and for the best performance , it seems like we should still stick to Deep Ensembles . # # # Questions * On the ImageNet experiments , what is the validation accuracy of the pre-trained model ? * Can you comment on the computaional and memory complexity of your algorithm versus vanilla SGD ? * In the comparisons between late phase weights and SGD , do both algorithms consume the same amount of data ? If so , this would be good to mention . * Could the entire network be treated as `` late-phase weights '' ? Would this help performance ? # # # Minor comments * I would consider alluding to possible choices of the weight interaction functions $ h $ when it is first introduced at the start of 2.1 . * In Algorithm 1 : How does the loss function consume three inputs ? This is different from when it is initially described . * It 's a bit unclear what is being compared in Figure 2 . ( increased score from 6 to 7 )", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thorough and encouraging review . We have carried out additional experiments and reworked the paper following your feedback listed point-to-point below : * * The weight interaction functions should be more explicitly defined rather than just described in text . * Thank you for this comment , indeed the BatchNorm and last-layer late-phase weights were missing explicit formulas , which made the presentation in Sect . 2.2 less clear . We have corrected this . * * I think there should be more discussion on the choice of $ T_0 $ . For example , in table 1 , why does SGD perform worse when $ T_0=0 $ ? It would be good to get a sense of robustness to this hyperparameter . * We have extended our $ T_0 $ sensitivity analysis to CIFAR-10 and searched using a finer step size ( Table 12 , see also Figure 5 ) ; our analysis reveals that this hyperparameter is robust , performance increases as long as it is set to a late-training value . Below , we present a slimmed-down version of Table 12 which shows that for $ T_0 $ > 80 epochs we improve on top of the baseline in both CIFAR10 ( 96.16+/-0.12 ) and CIFAR100 ( 81.31+/-0.16 ) in test set accuracy ( and out-of-distribution detection , see Figure 6 in the appendix ) . ++++ \\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; T_0 & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR10 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR100 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; \\ ++++\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 40 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.34 & nbsp ; +/- & nbsp ; 0.08 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 79.69 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 80 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.50 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 81.72 & nbsp ; +/- & nbsp ; 0.18 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; 100 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp * * Good results on CIFAR . Late-phase weights are shown to boost performance over SGD and to be complementary with SWA . There are some benefits in the OOD setting as well . * Thank you for this appreciation of our results . We have strengthened our CIFAR experiments by considering additional models . On a WRN 28-14 ( CIFAR-10 ) , we increase accuracy from 96.75 % to 97.45 % , on top of SWA , a very high figure for residual networks on this dataset . We obtain an improvement in the order of ~1 % on CIFAR-100 with this model ."}, {"review_id": "C0qJUx5dxFb-3", "review_text": "This work suggests a variant of ensembling that is more compute-efficient . Specifically , it involves forking an ensemble only in the late stage of training , and forming this ensemble via a `` low-dimentional '' family . That is , instead of maintaining independent networks , maintain only `` low-rank '' -style perturbations of the base network ( for various instanciations of `` low-rank '' ) . The experimental results are somewhat limited , but appear to be competitive with current efficient-ensembling approaches like SWA/SWAG . The absolute improvement of this method is not very large ( < 0.3 % on CIFAR , < 0.2 % on imagenet ) , and there is a large gap to Deep Ensembles . I weakly recommend acceptance , because the method appears promising for future work , and the experiments seem correct . There is also a theory section included , though I am generally unconvinced by results in such simple toy examples . ( such settings can usually be contrived to exhibit any desired behavior ) Weaknesses : - The experimental section would be greatly strengthened by additional experiments for different models and settings . There are only 2 architectures tested on CIFAR-10 , for example . It would also be informative to see the performance of these methods in `` harder '' settings -- for example , CIFAR-10 with fewer train samples . - The OOD uncertainty results could be expanded . Uncertainty estimation and robustness are some of the most relevant practical uses of ensemble methods , so it is especially important to evaluate ensembles in this context . Currently aggregate results are shown in Table 4 , but it would be good to explicitly see , for example : how the performance of this method degrades with increasing CIFAR-10C corruption severity , as opposed to Deep Ensembles . Also , reporting the Mean Corruption Error ( mCE ) for each dataset individually will allow standard comparison to prior methods . Comments which do not affect the score : It seems that starting the ensembling at a `` late phase '' in training is the main contribution of this work . This could be applied to any ensemble method , and you propose several explicit instantiations . It could help to focus the writing in terms of this contribution , and also to further investigate the role of T0 ( the time at which ensembling starts ) . Edit after rebuttal : Increased score from 6 to 7 .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thorough review and constructive criticism . We have followed your specific suggestions and expanded our OOD experiments , enlarged our coverage of models , and considered alternative use cases involving ensembling the solutions found with our method . We reply point-to-point below : * * The experimental results are somewhat limited , but appear to be competitive with current efficient-ensembling approaches like SWA/SWAG . The absolute improvement of this method is not very large ( < 0.3 % on CIFAR , < 0.2 % on imagenet ) , and there is a large gap to Deep Ensembles . [ ... ] The experimental section would be greatly strengthened by additional experiments for different models and settings . * We have now extended our experiments to include efficient-ensembling baselines ( dropout , MC-dropout , BatchEnsemble ) and new network architectures showing that our results are in fact strong compared to the performance increases achieved with other methods . Below , we highlight our current results for larger models ( test set accuracy shown in % ) : ++ -- +-+ -- +\\ | & nbsp ; Dataset & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; Model & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; Base & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; Late-phase & nbsp ; |\\ ++ -- +-+ -- +\\ | ImageNet & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; ResNet-152 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 78.37 +/- 0.01 | 78.77 +/- 0.01 |\\ | CIFAR-10 & nbsp ; | WRN28-14 ( SWA ) & nbsp ; | 96.75 +/- 0.05 | 97.45 +/- 0.10 |\\ | CIFAR-100 & nbsp ; | WRN28-14 ( SWA ) & nbsp ; | 84.01 +/- 0.29 | 85.00 +/- 0.25 |\\ ++ -- +-+ -- + These WRN 28-14 gains , obtained on top of SWA , place us among the best results for WRNs reported in PWC [ 1 , 2 ] . The improvement on ImageNet is significant for a fine-tuning method . Further , we would like to highlight that since our method yields a single model ( unlike MC-dropout or BatchEnsemble ) , it can be used to obtain a stronger DeepEnsemble in a straightforward manner . To showcase this we now present a proof-of-concept experiment showing that this is another possible use case , if one has the resources to build a DeepEnsemble ( CIFAR-10 in Table 1 & CIFAR-100 in Table 4 ) . * * I weakly recommend acceptance , because the method appears promising for future work , and the experiments seem correct . * Thank you for the encouraging feedback . In light of your and the other reviewers \u2019 comments we have performed new experiments and added new baselines to our work ( see general comment ) . This has significantly strengthened the paper and we hope that you now find it worthy of a \u2018 clear acceptance \u2019 . * * There is also a theory section included , though I am generally unconvinced by results in such simple toy examples . ( such settings can usually be contrived to exhibit any desired behavior ) . * Thank you for this comment -- we have rewritten Section 3.1 , to clarify and discuss the main contributions and implications of our new theory . We agree that going beyond the NQP would be desirable , but with the current analytical tools that is likely out-of-reach and beyond the scope of this work . * * There are only 2 architectures tested on CIFAR-10 , for example . It would also be informative to see the performance of these methods in `` harder '' settings -- for example , CIFAR-10 with fewer train samples . * We thank you for this comment and now include results when training with less ( a fifth ) CIFAR-10 examples ( Table 15 ) . These complement our new results obtained on a larger WRN ( the WRN 28-14 , Table 3 and table above ) on CIFAR-10 . In both cases , our implicit regularization leads to performance gains ( respectively +0.6 % , +0.7 % ) that are higher than on the WRN 28-10 ( +0.3 % ) . * * The OOD uncertainty results could be expanded . Uncertainty estimation and robustness are some of the most relevant practical uses of ensemble methods , so it is especially important to evaluate ensembles in this context . * We fully agree and have expanded our OOD section in response to your comment . We have improved our scores and considered yet another use case for our model : maintaining our late-phase ensemble at the end of training . Like SWAG , MC-dropout and BatchEnsemble , in this case , we integrate predictions across an efficiently-trained ensemble . This resulted in strongly improved OOD scores for our method ( Table 4 ) ."}], "0": {"review_id": "C0qJUx5dxFb-0", "review_text": "To improve the generalization performance of SGD methods , this paper proposes to use an efficient ensemble-like approach which computes an average of an ensemble of SGD weights when retrained from some late-phase of SGD dynamics . This idea is different to most recent ensemble-based approaches which aim to average the predictions of the models . The paper focuses on some specific layers of neural networks in order to apply the late-phase training . The batch normalization layers are shown to be simple and effective . Some other layers are also analyzed , including a recently introduced rank-1 multiplicative matrix weights idea for full-connected layers . Section 3 presents the numerical results and show that the generalization of SGD is more-or-less improved on various benchmarks . Explanation of why the generalization is improved in relation with the flatness of energy landscape is also discussed . I find that this approach is quite sensitive the choice of the hyper-parameters , such as the beginning of the late-phase T0 , and the noise perturbation sigma0 . It is written in Section 2.1 that in practice \u2026 sigma0 > 0 yields a set of models \u2026 this results in improved final generalization . However , in the result of ImageNet in Section 3.3 , the sigma0 equals to 0 . Thus , it is not conclusive that sigma0 > 0 is better . As the improvement in Section 3.3 seems marginal compared to the baseline and the standard deviation , it thus does not fully support the effectiveness of the batch normalization layers . I would recommend using some other dataset or models , but with a more consistent set of hyper-parameters . In terms of writing , I would recommend to write out the full algorithm of Alg . 1 or at least in the Appendix , including the variant of the SGD momentum and Adam . The SWA is also worth writing out clearly , which is not clear to the reader . Is the DeepEnsemble result in Table 1 from SGD or SWA ? This is not clear from the text . Overall , I think both the methodology and the writing need to be improved . # # The revisions made by the authors have addressed all my concerns .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . As detailed below , we have tried to address your criticism to the best of our knowledge , and we remain open to any questions that you may have , that can help you raise your score . * * I find that this approach is quite sensitive the choice of the hyper-parameters , such as the beginning of the late-phase T0 , and the noise perturbation sigma0 . It is written in Section 2.1 that in practice \u2026 sigma0 > 0 yields a set of models \u2026 this results in improved final generalization . However , in the result of ImageNet in Section 3.3 , the sigma0 equals to 0 . Thus , it is not conclusive that sigma0 > 0 is better . * Thank you for this comment . We recognize the seeming inconsistency of our choice of $ \\sigma_0 $ , and now present our results on CIFAR and ImageNet in the main text all using a consistent choice of $ \\sigma_0 = 0 $ ( and $ T_0 $ , for non-pretrained models ) . A non-zero $ \\sigma_0 = 0.5 $ is now employed only to generate a diverse late-phase ensemble ( see Section 3.2 , Table 4 ) that is not averaged in weight space . This method shows strong OOD performance in comparison to other comparable techniques that are efficiently-trained but still require to integrate predictions during inference ( SWAG , MC-dropout , BatchEnsemble ) . An overview of the updated performance can be found in the table below . \\ +-+ -- ++-+\\ | |Testacc . ( % ) | & nbsp ; & nbsp ; OOD | mCE |\\ +-+ -- ++-+\\ | Base ( SGD ) |81.35+/-0.16 |0.802+/-0.019 |47.84+/-0.41 |\\ | Dropout ( Mean ) ( SGD ) |81.31+/-0.20 |0.802+/-0.030 |48.97+/-0.33 |\\ | Late-phase BatchNorm ( SGD ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.87+/-0.14 |0.836+/-0.012 |45.59+/-0.25 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | MC-Dropout ( SGD ) & nbsp ; & nbsp ; |81.55+/-0.11 |0.823+/-0.049 |48.09+/-0.36 |\\ | SWAG ( SWA ) & nbsp ; |82.12+/-0.03 |0.828+/-0.027 | & nbsp ; & nbsp ; |\\ | BatchEnsemble ( SGD ) |81.25+/-0.10 |0.829+/-0.019 | & nbsp ; & nbsp ; |\\ | Late-phase BatchNorm ( SGD , non-averaged ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.71+/-0.10 |0.862+/-0.009 |46.21+/-0.29 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | Deepensemble ( SGD Here we show that for a large range of $ T_0 $ , i.e. , $ T_0 > 80 $ , we improve on top of the baseline ( CIFAR-10 - 96.16 % and CIFAR-100 - 81.31 % ) in test set accuracy and out-of-distribution detection . See a small excerpt of the analyses here : ++++ \\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; T_0 & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR10 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR100 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; \\ ++++\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 40 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.34 & nbsp ; +/- & nbsp ; 0.08 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 79.69 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 80 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.50 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 81.72 & nbsp ; +/- & nbsp ; 0.18 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; 100 & nbsp ; & nbsp ; & nbsp ; | & nbsp"}, "1": {"review_id": "C0qJUx5dxFb-1", "review_text": "Summary : The paper proposes a method to improve solutions found by SGD by ensembling subsets of weights in late-phase . A family of low-dimensional late-phase methods are analyzed and shown to improve generalization in CIFAR-10/100 , ImageNet and enwik8 . Authors also analyze the method in more tractable noisy quadratic settings . Contribution of the authors is that rather obtaining ensemble they utilize efficient ensemble to guide SGD training and ultimately obtain a single model . Reason for score : While the paper discusses efficient ways of utilizing late-phase weight ensemble and improving SGD training , the demonstrated benefit is not significant enough for practitioners to pursue the method . Without strong practical application potential , merit of the proposed method is weak since it does not obviously elucidate some aspects of neural network training . Pros : The paper is clearly written and easy to understand the proposed method is . It is well structured that helps to improve the clarity . Proposed method tackles a significant problem in the standard ensemble method in which both training/inference computation can be quite costly . The paper \u2019 s method only ensembles subset of weights therefore added training cost is minimal and since inference is done on averaged weight , it becomes essentially a single model . Among various late-phase schemes , BatchNorm late-phase seems to work well which is widely used among vision models so easily applicable . Also since late-phase can be applied post-pretraining , it can be used to improve pre-trained models . As far as I can tell various experimental conditions are very well controlled and thoughtfully designed . Cons : The idea of weight averaging is not so novel as duly noted by the authors . Main question arises for the paper is whether the proposed method is worth the effort . While all experiments show that the proposed method improves the baseline somewhat , deep ensemble baselines remain strong . Also quoted difference between methods does not mean statistically significant effect ( see Vincent Vanhoucke \u2019 s article on reporting significant figures https : //towardsdatascience.com/digit-significance-in-machine-learning-dea05dd6b85b ) . According to this article , results reported in Table 1 , CIFAR-10 in WRN , a significant figure with a 10k test set should be around 0.2 % and differences between different methods are at best marginal . This can be applied to most tables and except for Deep Ensemble \u2019 s improvement other differences are not very significant . I wonder as discussed by the authors , this is due to mostly the benefit of ensembles is through incorporating different modes as argued in [ Fort et al. , 2020 ] rather than a single mode . I imagine a single mode ensemble could be beneficial when variance within the mode is large , however for models considered by the authors seem to have small model variance which minimizes effect of technique utilizing single mode . While \\sigma_0 and T_0 are hyperparameters of the algorithm , no good way to determine it is explained . The role of section 3.1 is not clear . For one thing , the legend in Figure 1 is confusing where the role of non-integer K is mysterious to me . I would suggest clarifying what the message of the section would be in context of understanding late-phase weight models . Nits and additional feedback : Anonymized link is neither there in the main paper or included as supplementary material . If the authors intended to include the code , this is a note that code can not be found to the reviewers . For models that do not use BatchNorm , I believe most interest to practitioners would be using Transformer based models . I wonder if rank-1 late-phase or LayerNorm late-phase would show improvements in this case . Was \u201c Late-phase classification layers \u201d ever evaluated or discussed in the main paper ? I find some discussion on the appendix but seem to be missing in the main text . I thank the authors for their hard work addressing issues raised by the reviewers . Authors have answered many issues pointed out ( by improved performance and showing robustness to hyperparameters ) and I 've increased my score from 5 to 6 , and support accepting the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thorough review and feedback . We ran a number of new experiments , which include an extended set of OOD results , the study of more network architectures , and new well-known baselines ( dropout , MC-dropout , and BatchEnsemble ) with comparable computational and memory requirements to our method . We reply to your comments point by point : * * BatchNorm late-phase seems to work well which is widely used among vision models so easily applicable . Also since late-phase can be applied post-pretraining , it can be used to improve pre-trained models . * To facilitate broad adoption of our method we will provide a PyTorch drop-in replacement for a standard BatchNorm layer with the final version of the paper . Our aim is to make our method as easy to implement as other well-established complementary techniques like dropout or SWA . * * The idea of weight averaging is not so novel as duly noted by the authors . * While previous optimization algorithms employ various sorts of weight averaging ( notably , SWA and Polyak averaging , which maintain running temporal averages ) , we would like to highlight that our approach differs in that we take a single , simple spatial average in a low-dimensional weight space . This simple averaging is made possible thanks to the late-phase ensembling that we introduce in our paper , as corroborated by our experiments where $ T_0 $ is varied ( Table 12 , Figure 5 ) . * * While the paper discusses efficient ways of utilizing late-phase weight ensemble and improving SGD training , the demonstrated benefit is not significant enough for practitioners to pursue the method . Without strong practical application potential , merit of the proposed method is weak since it does not obviously elucidate some aspects of neural network training . \u2026.Main question arises for the paper is whether the proposed method is worth the effort . While all experiments show that the proposed method improves the baseline somewhat , deep ensemble baselines remain strong . Also quoted difference between methods does not mean statistically significant effect [ ... ] . Results reported in Table 1 , CIFAR-10 in WRN , a significant figure with a 10k test set should be around 0.2 % and differences between different methods are at best marginal . This can be applied to most tables and except for Deep Ensemble \u2019 s improvement other differences are not very significant . * To convince you that our paper is worth accepting we would like to point out our improved results ( Tables 2 , 3 , 4 and 5 in the main text ) . Here , we would like to highlight that on a WRN 28-14 we increase accuracy from 96.75 % ( SWA ) to 97.45 % ( Late-phase+SWA ) on CIFAR-10 , and from 84.01 % ( SWA ) to 85.00 % ( Late-phase+SWA ) on CIFAR-100 . These high accuracies place us among the very best available results for WRNs in PWC [ 1 , 2 ] ( 1st place on CIFAR-100 and 2nd place on CIFAR-10 ) . Furthermore , these results also show our method and SWA , one of the strongest methods for improving generalization in neural networks , are complementary . On the point raised over statistical significance , we stress that our CIFAR results are obtained with a consistent choice of $ T_0 $ and $ \\sigma_0 $ , across a number of different architectures . Our updated results also show that our method achieves strong performance in out-of-distribution detection problems and it is robust to input data corruptions : ( Table 4 and 16 in the paper ) . \\ +-+ -- ++-+\\ | |Testacc . ( % ) | & nbsp ; & nbsp ; OOD | mCE |\\ +-+ -- ++-+\\ | Base ( SGD ) |81.35+/-0.16 |0.802+/-0.019 |47.84+/-0.41 |\\ | Dropout ( Mean ) ( SGD ) |81.31+/-0.20 |0.802+/-0.030 |48.97+/-0.33 |\\ | Late-phase BatchNorm ( SGD ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.87+/-0.14 |0.836+/-0.012 |45.59+/-0.25 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | MC-Dropout ( SGD ) & nbsp ; & nbsp ; |81.55+/-0.11 |0.823+/-0.049 |48.09+/-0.36 |\\ | SWAG ( SWA ) & nbsp ; |82.12+/-0.03 |0.828+/-0.027 | & nbsp ; & nbsp ; |\\ | BatchEnsemble ( SGD ) |81.25+/-0.10 |0.829+/-0.019 | & nbsp ; & nbsp ; |\\ | Late-phase BatchNorm ( SGD , non-averaged ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; |82.71+/-0.10 |0.862+/-0.009 |46.21+/-0.29 |\\ | | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; | & nbsp ; & nbsp ; |\\ | Deepensemble ( SGD"}, "2": {"review_id": "C0qJUx5dxFb-2", "review_text": "# # # Summary The authors propose late-phase weights , a method of updating the weights near the end of training via a splitting and ensembling mechanism . They analyze the benefits in the noisy quadratic setting . The method improves validation performance on a range of image recognition tasks and on enwiki8 . # # # Comments * The weight interaction functions $ h $ should be more explicitly defined rather than just described in text . * The paper is overall well written and flows smoothly . * I think there should be more discussion on the choice of $ T_0 $ . For example , in table 1 , why does SGD perform worse when $ T_0=0 $ ? It would be good to get a sense of robustness to this hyperparameter . * Good results on CIFAR . Late-phase weights are shown to boost performance over SGD and to be complementary with SWA . There are some benefits in the OOD setting as well . # # # Recommendation / Justification I vote to accept the paper . The idea is interesting , well-motivated , and seems straightforward to incorporate into existing pipelines . However , the improvements seems modest in some settings ( e.g.ImageNet ) and for the best performance , it seems like we should still stick to Deep Ensembles . # # # Questions * On the ImageNet experiments , what is the validation accuracy of the pre-trained model ? * Can you comment on the computaional and memory complexity of your algorithm versus vanilla SGD ? * In the comparisons between late phase weights and SGD , do both algorithms consume the same amount of data ? If so , this would be good to mention . * Could the entire network be treated as `` late-phase weights '' ? Would this help performance ? # # # Minor comments * I would consider alluding to possible choices of the weight interaction functions $ h $ when it is first introduced at the start of 2.1 . * In Algorithm 1 : How does the loss function consume three inputs ? This is different from when it is initially described . * It 's a bit unclear what is being compared in Figure 2 . ( increased score from 6 to 7 )", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thorough and encouraging review . We have carried out additional experiments and reworked the paper following your feedback listed point-to-point below : * * The weight interaction functions should be more explicitly defined rather than just described in text . * Thank you for this comment , indeed the BatchNorm and last-layer late-phase weights were missing explicit formulas , which made the presentation in Sect . 2.2 less clear . We have corrected this . * * I think there should be more discussion on the choice of $ T_0 $ . For example , in table 1 , why does SGD perform worse when $ T_0=0 $ ? It would be good to get a sense of robustness to this hyperparameter . * We have extended our $ T_0 $ sensitivity analysis to CIFAR-10 and searched using a finer step size ( Table 12 , see also Figure 5 ) ; our analysis reveals that this hyperparameter is robust , performance increases as long as it is set to a late-training value . Below , we present a slimmed-down version of Table 12 which shows that for $ T_0 $ > 80 epochs we improve on top of the baseline in both CIFAR10 ( 96.16+/-0.12 ) and CIFAR100 ( 81.31+/-0.16 ) in test set accuracy ( and out-of-distribution detection , see Figure 6 in the appendix ) . ++++ \\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; T_0 & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR10 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; CIFAR100 & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; \\ ++++\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 40 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.34 & nbsp ; +/- & nbsp ; 0.08 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 79.69 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; & nbsp ; 80 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; 96.50 & nbsp ; +/- & nbsp ; 0.11 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; 81.72 & nbsp ; +/- & nbsp ; 0.18 & nbsp ; & nbsp ; |\\ | & nbsp ; & nbsp ; & nbsp ; 100 & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp * * Good results on CIFAR . Late-phase weights are shown to boost performance over SGD and to be complementary with SWA . There are some benefits in the OOD setting as well . * Thank you for this appreciation of our results . We have strengthened our CIFAR experiments by considering additional models . On a WRN 28-14 ( CIFAR-10 ) , we increase accuracy from 96.75 % to 97.45 % , on top of SWA , a very high figure for residual networks on this dataset . We obtain an improvement in the order of ~1 % on CIFAR-100 with this model ."}, "3": {"review_id": "C0qJUx5dxFb-3", "review_text": "This work suggests a variant of ensembling that is more compute-efficient . Specifically , it involves forking an ensemble only in the late stage of training , and forming this ensemble via a `` low-dimentional '' family . That is , instead of maintaining independent networks , maintain only `` low-rank '' -style perturbations of the base network ( for various instanciations of `` low-rank '' ) . The experimental results are somewhat limited , but appear to be competitive with current efficient-ensembling approaches like SWA/SWAG . The absolute improvement of this method is not very large ( < 0.3 % on CIFAR , < 0.2 % on imagenet ) , and there is a large gap to Deep Ensembles . I weakly recommend acceptance , because the method appears promising for future work , and the experiments seem correct . There is also a theory section included , though I am generally unconvinced by results in such simple toy examples . ( such settings can usually be contrived to exhibit any desired behavior ) Weaknesses : - The experimental section would be greatly strengthened by additional experiments for different models and settings . There are only 2 architectures tested on CIFAR-10 , for example . It would also be informative to see the performance of these methods in `` harder '' settings -- for example , CIFAR-10 with fewer train samples . - The OOD uncertainty results could be expanded . Uncertainty estimation and robustness are some of the most relevant practical uses of ensemble methods , so it is especially important to evaluate ensembles in this context . Currently aggregate results are shown in Table 4 , but it would be good to explicitly see , for example : how the performance of this method degrades with increasing CIFAR-10C corruption severity , as opposed to Deep Ensembles . Also , reporting the Mean Corruption Error ( mCE ) for each dataset individually will allow standard comparison to prior methods . Comments which do not affect the score : It seems that starting the ensembling at a `` late phase '' in training is the main contribution of this work . This could be applied to any ensemble method , and you propose several explicit instantiations . It could help to focus the writing in terms of this contribution , and also to further investigate the role of T0 ( the time at which ensembling starts ) . Edit after rebuttal : Increased score from 6 to 7 .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thorough review and constructive criticism . We have followed your specific suggestions and expanded our OOD experiments , enlarged our coverage of models , and considered alternative use cases involving ensembling the solutions found with our method . We reply point-to-point below : * * The experimental results are somewhat limited , but appear to be competitive with current efficient-ensembling approaches like SWA/SWAG . The absolute improvement of this method is not very large ( < 0.3 % on CIFAR , < 0.2 % on imagenet ) , and there is a large gap to Deep Ensembles . [ ... ] The experimental section would be greatly strengthened by additional experiments for different models and settings . * We have now extended our experiments to include efficient-ensembling baselines ( dropout , MC-dropout , BatchEnsemble ) and new network architectures showing that our results are in fact strong compared to the performance increases achieved with other methods . Below , we highlight our current results for larger models ( test set accuracy shown in % ) : ++ -- +-+ -- +\\ | & nbsp ; Dataset & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; Model & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; & nbsp ; Base & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | & nbsp ; & nbsp ; Late-phase & nbsp ; |\\ ++ -- +-+ -- +\\ | ImageNet & nbsp ; | & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; ResNet-152 & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 78.37 +/- 0.01 | 78.77 +/- 0.01 |\\ | CIFAR-10 & nbsp ; | WRN28-14 ( SWA ) & nbsp ; | 96.75 +/- 0.05 | 97.45 +/- 0.10 |\\ | CIFAR-100 & nbsp ; | WRN28-14 ( SWA ) & nbsp ; | 84.01 +/- 0.29 | 85.00 +/- 0.25 |\\ ++ -- +-+ -- + These WRN 28-14 gains , obtained on top of SWA , place us among the best results for WRNs reported in PWC [ 1 , 2 ] . The improvement on ImageNet is significant for a fine-tuning method . Further , we would like to highlight that since our method yields a single model ( unlike MC-dropout or BatchEnsemble ) , it can be used to obtain a stronger DeepEnsemble in a straightforward manner . To showcase this we now present a proof-of-concept experiment showing that this is another possible use case , if one has the resources to build a DeepEnsemble ( CIFAR-10 in Table 1 & CIFAR-100 in Table 4 ) . * * I weakly recommend acceptance , because the method appears promising for future work , and the experiments seem correct . * Thank you for the encouraging feedback . In light of your and the other reviewers \u2019 comments we have performed new experiments and added new baselines to our work ( see general comment ) . This has significantly strengthened the paper and we hope that you now find it worthy of a \u2018 clear acceptance \u2019 . * * There is also a theory section included , though I am generally unconvinced by results in such simple toy examples . ( such settings can usually be contrived to exhibit any desired behavior ) . * Thank you for this comment -- we have rewritten Section 3.1 , to clarify and discuss the main contributions and implications of our new theory . We agree that going beyond the NQP would be desirable , but with the current analytical tools that is likely out-of-reach and beyond the scope of this work . * * There are only 2 architectures tested on CIFAR-10 , for example . It would also be informative to see the performance of these methods in `` harder '' settings -- for example , CIFAR-10 with fewer train samples . * We thank you for this comment and now include results when training with less ( a fifth ) CIFAR-10 examples ( Table 15 ) . These complement our new results obtained on a larger WRN ( the WRN 28-14 , Table 3 and table above ) on CIFAR-10 . In both cases , our implicit regularization leads to performance gains ( respectively +0.6 % , +0.7 % ) that are higher than on the WRN 28-10 ( +0.3 % ) . * * The OOD uncertainty results could be expanded . Uncertainty estimation and robustness are some of the most relevant practical uses of ensemble methods , so it is especially important to evaluate ensembles in this context . * We fully agree and have expanded our OOD section in response to your comment . We have improved our scores and considered yet another use case for our model : maintaining our late-phase ensemble at the end of training . Like SWAG , MC-dropout and BatchEnsemble , in this case , we integrate predictions across an efficiently-trained ensemble . This resulted in strongly improved OOD scores for our method ( Table 4 ) ."}}