{"year": "2021", "forum": "FZ1oTwcXchK", "title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks", "decision": "Accept (Poster)", "meta_review": "The work tackles the task to convert an artificial neural networks (ANN) to a spiking neural network (SNN). The topic is potentially important for energy-efficient hardware implementations of neural networks. There is already quite some literature available on this topic. \nCompared to these, the manuscript exhibits a number of strong contributions: It presents a theoretical analysis of the conversion error and consequently arrives at a principled way to reduce the conversion error. The authors test the performance of the conversion on a number of challenging data sets. Their method achieves excellent performances with reduced simulation time / latency (usually, in order to achieve comparable performance to ANNs, one needs to run the SNN for many simulated time steps- this simulation time is reduced by their model). \nOne reviewer criticized that the article was hard to read, but this opinion was not shared by other reviewers and the authors have improved the readability in a revision.\n\nIn summary, I believe that this manuscript presents a very good contribution to the field.", "reviews": [{"review_id": "FZ1oTwcXchK-0", "review_text": "# # Edit on second review I apologize again for the tone of my first review , I sincerely tried to understand the paper but I could not when I first read it . A re-read the paper and finally understood it during the review . I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations . With the new equation ( 1 ) the paper is hopefully more understandable now . I increase my grade from 3 to 5 . The findings are quite interesting but I still believe that the paper is not well written : the equations are interesting but the explanations between the equations are often unclear . One has to understand each equation and be quite imaginative to finally identify the contributions of the paper ( even for somebody only `` very slightly '' off from the research topic ) . # # Summary The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model . This relationship suggests a mapping between the two models which is imperfect , a loss seems to be derived to reduce this mismatch along the network training . The method is tested on CIFAR-10 and CIFAR-100 , and compared with some other methods for converting ANNs to SNNs . # # Critical review This topic is potentially important since spiking neural network are gaining popularity . But this paper is clearly badly written and it is extremely hard to understand , both in the math and in the text . I do n't think it would help the progress of the field to publish the article in the current form . I tried to read that carefully and got lost after equation ( 4 ) , the transition to equation ( 5 ) and ( 6 ) are not clear at all . I do not understand what is an approximation , what is a definition and what is a derivation . Also ( 5 ) seems wrong in itself , the authors are trying to approximate a rectified linear network but it suggest that the activity will be equivalent to a linear network ( at least when v ( T ) is small ) ? And magically this changes in ( 6 ) , and a clip non-linearity is introduced ? The Figure 1 seems very encouraging at first , because it suggests that there is a clear and easy mapping between accumulating the spikes and computing a relu . I did not understand where this is appearing in the math and I can not check whether the intuition conveyed by the figure is correct or not . I was therefore hoping to see an empirical study of the difference between the SNN and the ANN : do the activity of the spiking neural network match the activity of artificial network ? This is not shown . I do not even understand if it is necessary to re-train the network to go back from the SNN to ANN or vice versa . Since I had not understood the basics of the paper , it was impossible for me to understand the later section about the conversion error . My only take is that it seems wrong at first sight : how minimizing the error in the loss would minimize the mismatch between the network activity ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his/her effort in reviewing our paper . There are some definite misunderstanding of our paper that we want to clarify here . We guess that the main reason for R3 to feel lost in reading the context is that he/she may not be very familiar with the setup for SNN since the other two reviewers do not have the same problem in following the logic of our paper . Both of them are able to evaluate our paper with fair confidence and appreciate our contribution to both theory and practice . Based on R3 's comments , we provide further explanations below and hope it can help R3 and other general readers to understand our paper . Let 's first shortly describe the mechanism of the SNN . The SNN generates discrete spikes with respect to the series of events ( this is why we usually have the simulation length parameter for training the SNN ) while the ANN generates continuous values . Along the simulation process , the membrane potential accumulates on each SNN neuron and will release a post synaptic potential ( PSP ) to its linking neurons in the next layer when the accumulated membrane potential exceeds the threshold voltage . The activation values on SNNs are actually the spiking frequency multiplied by the threshold voltage . In terms of the conversion proposed in the current paper , from the perspective of statistical learning , a network , no matter it is SNN or ANN , is a parameterization of the sample space . Thus if two networks produce the same output for the same input , we would say the two networks are equivalent ignorant of the details in their infrastructures . And the goal of converting the ANN to SNN is to make the target SNN provide the most similar modeling with the source ANN on the sample distribution . This is an intuitive explanation of why we want to minimize the conversion error . For more rigorous discussion , one can refer to the information geometry theory that links the loss function and the metric on the space of distributions . In terms of the network activity , it is not our purpose to minimize the difference of layer-wise network activity between the source ANN and target SNN but the conclusion from the decomposition of conversion error that the overall conversion error can be minimized by approximating the activation values following a layer-wise strategy . This is why we do not perform a separate empirical study of the difference between the activity of the SNN and ANN but focus on the conversion error and the ultimate performance of the target SNN . For the transition from Eqn ( 4 ) to Eqn ( 5 ) ( 6 ) , there is no magic but the transform of notations to illustrate the threshold balancing procedure between the source ANN and target SNN . As we state at the beginning of the previous paragraph , the SNN actually generates discrete spikes . Thus , if we want to compare the activation values between the source ANN and target SNN , we need to quantify the frequency of spikes in SNN averaged through the whole simulation sequence . This is why we make the transition from Eqn ( 4 ) to Eqn ( 5 ) . We guess the reviewer 's confusion here is caused by the difference between $ \\mathbf { a } _ { l+1 } ' $ and $ \\mathbf { a } _l ' $ . As we state in the context , the word \u201c denote \u201d means that $ \\mathbf { a } _l ' $ is defined as the average input to the $ l $ -th layer . On the other hand , considering the network infrastructure of SNN , the $ ( l+1 ) $ -th layer 's input is also the PSP of the $ l $ -th layer . This relationship is described in Eqn ( 4 ) and transitioned to Eqn ( 5 ) in the average sense . For the transition from Eqn ( 5 ) to Eqn ( 6 ) , remember that these $ \\mathbf { a } _l ' $ s are discrete due to the features of SNN . In Eqn ( 5 ) , the difference of the linear form is accumulated and absorbed in $ \\mathbf { v } ^l ( T ) /T $ . In Eqn ( 6 ) , based on the discrete spiking mechanism , we calculate how much average PSP would be released as the input to the $ ( l+1 ) $ -th layer when it accumulates $ W_l\\cdot\\mathbf { a } _l ' $ voltage on the $ l $ -th layer . Or more explicitly , the clip function can be viewed as a format of SNN \u2019 s activation function while the ANN takes ReLU in the same position . To say the least , as the reviewer also points out , this section forms the basics of the paper . If it was incorrect , there would have been systematic errors in the following approximation thus it would be impossible for us to maintain the SOTA results compared to existing works on the same topic . Regarding the reviewer 's confusion in Fig 1 , the panel A gives the overall schematics of the conversion pipeline and panel B and C talk about the intuition of layer-wise approximation in Sec 5 . We will follow the reviewer \u2019 s suggestion and add a more explicit reference to avoid confusion here . Hope our clarification and explanation here can help solve the reviewer 's concerns and facilitate the understanding of our paper for potential readers in general background . Any further comments are welcome as well ."}, {"review_id": "FZ1oTwcXchK-1", "review_text": "Strength : ( 1 ) This paper proposes a layer-wise optimization method for ANN-SNN conversion . I appreciate the theoretical analysis of how to minimize the conversion error . ( 2 ) This work significantly reduces the simulation time since long simulation time is usually required for converted SNN to reduce error . ( 3 ) It 's interesting that conversion to SNN actually improves rather than damage the accuracy on ResNet . Weakness : ( 1 ) Although the proposed method is much more efficient , it does not show obvious performance improvement compared to existing methods . ( 2 ) In the experiment , the ResNet consistently show better performance . I hope the authors can provide more comments on this . ( 3 ) I 'm not sure if I missed anything . The threshold RELU is not defined in the paper which may cause confusion . ( 4 ) I hope the authors can summarize the whole steps by formula or algorithm to help readers understand the entire process .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his/her time in reviewing our work and appreciate the helpful suggestions to further improve the quality of our work . We \u2019 ve made the reversion accordingly in the manuscript and summarize the response below for ease of review . # # # # # Response to weakness 1 : The improvement reported in Table 1 is not very obvious due to the setup of the conversion from ANN to SNN . Within this setup , the source ANN is usually given . When the simulation step T is large enough , the conversion error would go zero , driving the converted SNN to approximate the performance of the source ANN . In the inference step of SNN , an image needs to be processed with T-step simulations . Thus the most critical part of converting ANN to SNN is actually the improvement of efficiency with a shorter simulation length , which is beneficial to not only the training but also the deployment of the SNN network . We agree with the reviewer that making SNN achieve a higher performance than the traditional ANN would fully release the potential power of SNN . But the direct training of the SNN with complex infrastructure is beyond the scope of the current work and still an open problem in the whole field . # # # # # Response to weakness 2 : The improvement on ResNet is relatively larger because the source ANN for the conversion is indeed trained without the batch-normalization thus its generalizability is a bit damaged compared to the ResNet with batch-normalization . The way we build up the converted SNN with threshold ReLU and shorter simulation length indeed make up for this loss of generalizability by limiting and discretizing the range of activation values . By Liang [ 1 ] , these approaches could decrease the Fisher-Rao norm that acts as an upper bound measuring the generalizability thus potentially increase the generalizability when we test the model performance on the testing dataset . The common batch-normalization is not suitable for the converted SNN as it will move the distribution of activation values to the negative range thus destabilize the convergence of the SNN as the spiking frequency can only be positive . So , recent conversion methods all use the ANN trained without batch-normalization . In future works , it will make sense to extend the threshold ReLU to the leaky version and build the SNN with inhibitory layers to generate negative activation values equivalent to the negative frequencies . # # # # # Response to weakness 3 : We appreciate the suggestion of adding an explicit definition of threshold RELU to avoid potential confusion . It is now defined in Section 2 PRELIMINARIES on page 2 . # # # # # Response to weakness 4 : We adopt this helpful suggestion to improve the readability and add the description of the conversion process as Algorithm 1 on page 5 . [ 1 ] Liang , Tengyuan , Tomaso Poggio , Alexander Rakhlin , and James Stokes . `` Fisher-rao metric , geometry , and complexity of neural networks . '' In The 22nd International Conference on Artificial Intelligence and Statistics , pp . 888-896.2019 ."}, {"review_id": "FZ1oTwcXchK-2", "review_text": "The authors seek a mechanism to train a spiking neural net to duplicate the function of a non-spiking one . This is desirable for energy-efficient inference , although the training process becomes challenging due to the discrete nature of the spiking process . To achieve their goal , they described the spiking neuron non-linearity by a `` staircase '' function of the input ( spiking output increases by 1 each time the input gets big enough to reach the next stair ) , and related that to the ReLu function used in the non-spiking neural net . They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions , and computed the minimum conversion error ( for converting ANN - > SNN ) . This scales with the square of the threshold voltage for spiking , divided by the simulation time . As one might expect , lower thresholds , and longer simulation times , both of which lead to potentially higher spike counts and thus lower discretization errors , lead to smaller conversion errors . Using this , they defined their procedure for training SNN to mimic ANN as follows : they trained the ANN with their modified ReLU ( which is closer to the SNN activation function but more readily differentiable ) , and then used the weights from that ANN in their SNN . Next , the authors evaluated their procedure on several different image categorization networks . Nice performance was obtained in all cases : better than using a normal ReLU , or other comparison activation functions , in the `` target '' ANN . Overall , this is a reasonably nice piece of work . I 'd like to see this applied to recurrent neural nets : there , the dynamics of the SNN could be used more naturally , and the results might be more meaningful .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his/her time in reviewing our work and appreciate the helpful suggestions to further improve the quality of our work . The conversion from RNN to SNN is a bit different from the current work as it requires a different set of terminology and the extension of SNN to negative spiking frequencies . As far as we know , there is NO reported result that systematically talks about the conversion from RNN to SNN . Instead of rigorously building the conversion theory which may be beyond the current scope of the paper , here we show an illustrative example to demonstrate the power of the proposed framework . We add the results in Appendix A.4 to keep the completeness of the current work . For ease of review , we copy the paragraphs below . ______________________________________________________________________________________________________________________________ # # # # # Performance of SNN converted from RNN Here we provide an illustrative example of how the proposed conversion pipeline can be extended to the case of converting RNN on the dataset for Sentiment Analysis on Movie Reviews \\citep { socher2013recursive } . The rules are slightly different from conversion in the main text considering the implicit definition of RNN 's hidden states . For the fairness of comparison , we set the same input and simulation length for the RNN and SNN and adjust the structure as follows . ( 1 ) The source RNN adopts the threshold-ReLU as its activation function with the remaining value as the hidden state value . The hidden value will be added to the pre-activation value at the next time point . ( 2 ) We add a non-negative attenuation $ \\tau $ to the hidden state values in order to enhance the nonlinearity . ( 3 ) The converted SNN keeps the same infrastructure , weight parameters , attenuation value $ \\tau $ as the source RNN . ( 4 ) On the output layer , we use the threshold balancing method and loop its input multiple times to fire enough spikes to obtain a good approximation to the fully-connected layer for the final prediction . We compare the performance of the source RNN , converted SNN and directly-trained SNN . On the validation set , the converted SNN achieves an accuracy of 0.5430 that is close to the source RNN ( acc = 0.5428 ) , while the directly-trained SNN with surrogate gradient only gets an 0.5106 accuracy . We also find that using the regular ReLU instead of threshold-ReLU on the source RNN produces a big accuracy drop for the converted SNN ( acc = 0.5100 ) . Since the surrogate gradient error of the directly-trained method will accumulate over the whole simulation process while complex infrastructures and tasks usually require a long simulation to achieve an effective spiking frequency distribution , the typical directly-training approaches for SNNs are often not optimal in complex network structures and tasks . Our results illustrate the potential efficiency of converting SNN from RNN . In future works , it would be promising to investigate how to design a better conversion strategy that can simultaneously combine the strength of RNN and SNN ."}], "0": {"review_id": "FZ1oTwcXchK-0", "review_text": "# # Edit on second review I apologize again for the tone of my first review , I sincerely tried to understand the paper but I could not when I first read it . A re-read the paper and finally understood it during the review . I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations . With the new equation ( 1 ) the paper is hopefully more understandable now . I increase my grade from 3 to 5 . The findings are quite interesting but I still believe that the paper is not well written : the equations are interesting but the explanations between the equations are often unclear . One has to understand each equation and be quite imaginative to finally identify the contributions of the paper ( even for somebody only `` very slightly '' off from the research topic ) . # # Summary The authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model . This relationship suggests a mapping between the two models which is imperfect , a loss seems to be derived to reduce this mismatch along the network training . The method is tested on CIFAR-10 and CIFAR-100 , and compared with some other methods for converting ANNs to SNNs . # # Critical review This topic is potentially important since spiking neural network are gaining popularity . But this paper is clearly badly written and it is extremely hard to understand , both in the math and in the text . I do n't think it would help the progress of the field to publish the article in the current form . I tried to read that carefully and got lost after equation ( 4 ) , the transition to equation ( 5 ) and ( 6 ) are not clear at all . I do not understand what is an approximation , what is a definition and what is a derivation . Also ( 5 ) seems wrong in itself , the authors are trying to approximate a rectified linear network but it suggest that the activity will be equivalent to a linear network ( at least when v ( T ) is small ) ? And magically this changes in ( 6 ) , and a clip non-linearity is introduced ? The Figure 1 seems very encouraging at first , because it suggests that there is a clear and easy mapping between accumulating the spikes and computing a relu . I did not understand where this is appearing in the math and I can not check whether the intuition conveyed by the figure is correct or not . I was therefore hoping to see an empirical study of the difference between the SNN and the ANN : do the activity of the spiking neural network match the activity of artificial network ? This is not shown . I do not even understand if it is necessary to re-train the network to go back from the SNN to ANN or vice versa . Since I had not understood the basics of the paper , it was impossible for me to understand the later section about the conversion error . My only take is that it seems wrong at first sight : how minimizing the error in the loss would minimize the mismatch between the network activity ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his/her effort in reviewing our paper . There are some definite misunderstanding of our paper that we want to clarify here . We guess that the main reason for R3 to feel lost in reading the context is that he/she may not be very familiar with the setup for SNN since the other two reviewers do not have the same problem in following the logic of our paper . Both of them are able to evaluate our paper with fair confidence and appreciate our contribution to both theory and practice . Based on R3 's comments , we provide further explanations below and hope it can help R3 and other general readers to understand our paper . Let 's first shortly describe the mechanism of the SNN . The SNN generates discrete spikes with respect to the series of events ( this is why we usually have the simulation length parameter for training the SNN ) while the ANN generates continuous values . Along the simulation process , the membrane potential accumulates on each SNN neuron and will release a post synaptic potential ( PSP ) to its linking neurons in the next layer when the accumulated membrane potential exceeds the threshold voltage . The activation values on SNNs are actually the spiking frequency multiplied by the threshold voltage . In terms of the conversion proposed in the current paper , from the perspective of statistical learning , a network , no matter it is SNN or ANN , is a parameterization of the sample space . Thus if two networks produce the same output for the same input , we would say the two networks are equivalent ignorant of the details in their infrastructures . And the goal of converting the ANN to SNN is to make the target SNN provide the most similar modeling with the source ANN on the sample distribution . This is an intuitive explanation of why we want to minimize the conversion error . For more rigorous discussion , one can refer to the information geometry theory that links the loss function and the metric on the space of distributions . In terms of the network activity , it is not our purpose to minimize the difference of layer-wise network activity between the source ANN and target SNN but the conclusion from the decomposition of conversion error that the overall conversion error can be minimized by approximating the activation values following a layer-wise strategy . This is why we do not perform a separate empirical study of the difference between the activity of the SNN and ANN but focus on the conversion error and the ultimate performance of the target SNN . For the transition from Eqn ( 4 ) to Eqn ( 5 ) ( 6 ) , there is no magic but the transform of notations to illustrate the threshold balancing procedure between the source ANN and target SNN . As we state at the beginning of the previous paragraph , the SNN actually generates discrete spikes . Thus , if we want to compare the activation values between the source ANN and target SNN , we need to quantify the frequency of spikes in SNN averaged through the whole simulation sequence . This is why we make the transition from Eqn ( 4 ) to Eqn ( 5 ) . We guess the reviewer 's confusion here is caused by the difference between $ \\mathbf { a } _ { l+1 } ' $ and $ \\mathbf { a } _l ' $ . As we state in the context , the word \u201c denote \u201d means that $ \\mathbf { a } _l ' $ is defined as the average input to the $ l $ -th layer . On the other hand , considering the network infrastructure of SNN , the $ ( l+1 ) $ -th layer 's input is also the PSP of the $ l $ -th layer . This relationship is described in Eqn ( 4 ) and transitioned to Eqn ( 5 ) in the average sense . For the transition from Eqn ( 5 ) to Eqn ( 6 ) , remember that these $ \\mathbf { a } _l ' $ s are discrete due to the features of SNN . In Eqn ( 5 ) , the difference of the linear form is accumulated and absorbed in $ \\mathbf { v } ^l ( T ) /T $ . In Eqn ( 6 ) , based on the discrete spiking mechanism , we calculate how much average PSP would be released as the input to the $ ( l+1 ) $ -th layer when it accumulates $ W_l\\cdot\\mathbf { a } _l ' $ voltage on the $ l $ -th layer . Or more explicitly , the clip function can be viewed as a format of SNN \u2019 s activation function while the ANN takes ReLU in the same position . To say the least , as the reviewer also points out , this section forms the basics of the paper . If it was incorrect , there would have been systematic errors in the following approximation thus it would be impossible for us to maintain the SOTA results compared to existing works on the same topic . Regarding the reviewer 's confusion in Fig 1 , the panel A gives the overall schematics of the conversion pipeline and panel B and C talk about the intuition of layer-wise approximation in Sec 5 . We will follow the reviewer \u2019 s suggestion and add a more explicit reference to avoid confusion here . Hope our clarification and explanation here can help solve the reviewer 's concerns and facilitate the understanding of our paper for potential readers in general background . Any further comments are welcome as well ."}, "1": {"review_id": "FZ1oTwcXchK-1", "review_text": "Strength : ( 1 ) This paper proposes a layer-wise optimization method for ANN-SNN conversion . I appreciate the theoretical analysis of how to minimize the conversion error . ( 2 ) This work significantly reduces the simulation time since long simulation time is usually required for converted SNN to reduce error . ( 3 ) It 's interesting that conversion to SNN actually improves rather than damage the accuracy on ResNet . Weakness : ( 1 ) Although the proposed method is much more efficient , it does not show obvious performance improvement compared to existing methods . ( 2 ) In the experiment , the ResNet consistently show better performance . I hope the authors can provide more comments on this . ( 3 ) I 'm not sure if I missed anything . The threshold RELU is not defined in the paper which may cause confusion . ( 4 ) I hope the authors can summarize the whole steps by formula or algorithm to help readers understand the entire process .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his/her time in reviewing our work and appreciate the helpful suggestions to further improve the quality of our work . We \u2019 ve made the reversion accordingly in the manuscript and summarize the response below for ease of review . # # # # # Response to weakness 1 : The improvement reported in Table 1 is not very obvious due to the setup of the conversion from ANN to SNN . Within this setup , the source ANN is usually given . When the simulation step T is large enough , the conversion error would go zero , driving the converted SNN to approximate the performance of the source ANN . In the inference step of SNN , an image needs to be processed with T-step simulations . Thus the most critical part of converting ANN to SNN is actually the improvement of efficiency with a shorter simulation length , which is beneficial to not only the training but also the deployment of the SNN network . We agree with the reviewer that making SNN achieve a higher performance than the traditional ANN would fully release the potential power of SNN . But the direct training of the SNN with complex infrastructure is beyond the scope of the current work and still an open problem in the whole field . # # # # # Response to weakness 2 : The improvement on ResNet is relatively larger because the source ANN for the conversion is indeed trained without the batch-normalization thus its generalizability is a bit damaged compared to the ResNet with batch-normalization . The way we build up the converted SNN with threshold ReLU and shorter simulation length indeed make up for this loss of generalizability by limiting and discretizing the range of activation values . By Liang [ 1 ] , these approaches could decrease the Fisher-Rao norm that acts as an upper bound measuring the generalizability thus potentially increase the generalizability when we test the model performance on the testing dataset . The common batch-normalization is not suitable for the converted SNN as it will move the distribution of activation values to the negative range thus destabilize the convergence of the SNN as the spiking frequency can only be positive . So , recent conversion methods all use the ANN trained without batch-normalization . In future works , it will make sense to extend the threshold ReLU to the leaky version and build the SNN with inhibitory layers to generate negative activation values equivalent to the negative frequencies . # # # # # Response to weakness 3 : We appreciate the suggestion of adding an explicit definition of threshold RELU to avoid potential confusion . It is now defined in Section 2 PRELIMINARIES on page 2 . # # # # # Response to weakness 4 : We adopt this helpful suggestion to improve the readability and add the description of the conversion process as Algorithm 1 on page 5 . [ 1 ] Liang , Tengyuan , Tomaso Poggio , Alexander Rakhlin , and James Stokes . `` Fisher-rao metric , geometry , and complexity of neural networks . '' In The 22nd International Conference on Artificial Intelligence and Statistics , pp . 888-896.2019 ."}, "2": {"review_id": "FZ1oTwcXchK-2", "review_text": "The authors seek a mechanism to train a spiking neural net to duplicate the function of a non-spiking one . This is desirable for energy-efficient inference , although the training process becomes challenging due to the discrete nature of the spiking process . To achieve their goal , they described the spiking neuron non-linearity by a `` staircase '' function of the input ( spiking output increases by 1 each time the input gets big enough to reach the next stair ) , and related that to the ReLu function used in the non-spiking neural net . They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions , and computed the minimum conversion error ( for converting ANN - > SNN ) . This scales with the square of the threshold voltage for spiking , divided by the simulation time . As one might expect , lower thresholds , and longer simulation times , both of which lead to potentially higher spike counts and thus lower discretization errors , lead to smaller conversion errors . Using this , they defined their procedure for training SNN to mimic ANN as follows : they trained the ANN with their modified ReLU ( which is closer to the SNN activation function but more readily differentiable ) , and then used the weights from that ANN in their SNN . Next , the authors evaluated their procedure on several different image categorization networks . Nice performance was obtained in all cases : better than using a normal ReLU , or other comparison activation functions , in the `` target '' ANN . Overall , this is a reasonably nice piece of work . I 'd like to see this applied to recurrent neural nets : there , the dynamics of the SNN could be used more naturally , and the results might be more meaningful .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his/her time in reviewing our work and appreciate the helpful suggestions to further improve the quality of our work . The conversion from RNN to SNN is a bit different from the current work as it requires a different set of terminology and the extension of SNN to negative spiking frequencies . As far as we know , there is NO reported result that systematically talks about the conversion from RNN to SNN . Instead of rigorously building the conversion theory which may be beyond the current scope of the paper , here we show an illustrative example to demonstrate the power of the proposed framework . We add the results in Appendix A.4 to keep the completeness of the current work . For ease of review , we copy the paragraphs below . ______________________________________________________________________________________________________________________________ # # # # # Performance of SNN converted from RNN Here we provide an illustrative example of how the proposed conversion pipeline can be extended to the case of converting RNN on the dataset for Sentiment Analysis on Movie Reviews \\citep { socher2013recursive } . The rules are slightly different from conversion in the main text considering the implicit definition of RNN 's hidden states . For the fairness of comparison , we set the same input and simulation length for the RNN and SNN and adjust the structure as follows . ( 1 ) The source RNN adopts the threshold-ReLU as its activation function with the remaining value as the hidden state value . The hidden value will be added to the pre-activation value at the next time point . ( 2 ) We add a non-negative attenuation $ \\tau $ to the hidden state values in order to enhance the nonlinearity . ( 3 ) The converted SNN keeps the same infrastructure , weight parameters , attenuation value $ \\tau $ as the source RNN . ( 4 ) On the output layer , we use the threshold balancing method and loop its input multiple times to fire enough spikes to obtain a good approximation to the fully-connected layer for the final prediction . We compare the performance of the source RNN , converted SNN and directly-trained SNN . On the validation set , the converted SNN achieves an accuracy of 0.5430 that is close to the source RNN ( acc = 0.5428 ) , while the directly-trained SNN with surrogate gradient only gets an 0.5106 accuracy . We also find that using the regular ReLU instead of threshold-ReLU on the source RNN produces a big accuracy drop for the converted SNN ( acc = 0.5100 ) . Since the surrogate gradient error of the directly-trained method will accumulate over the whole simulation process while complex infrastructures and tasks usually require a long simulation to achieve an effective spiking frequency distribution , the typical directly-training approaches for SNNs are often not optimal in complex network structures and tasks . Our results illustrate the potential efficiency of converting SNN from RNN . In future works , it would be promising to investigate how to design a better conversion strategy that can simultaneously combine the strength of RNN and SNN ."}}