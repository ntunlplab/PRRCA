{"year": "2019", "forum": "ryxLG2RcYX", "title": "Learning Abstract Models for Long-Horizon Exploration", "decision": "Reject", "meta_review": "The paper presents a novel approach to exploration in long-horizon / sparse reward RL settings. The approach is based on the notion of abstract states, a space that is lower-dimensional than the original state space, and in which transition dynamics can be learned and exploration is planned. A distributed algorithm is proposed for managing exploration in the abstract space (done by the manager), and learning to navigate between abstract states (workers). Empirical results show strong performance on hard exploration Atari games.\n\nThe paper addresses a key challenge in reinforcement learning - learning and planning in long horizon MDPs. It presents an original approach to this problem, and demonstrates that it can be leveraged to achieve strong empirical results. \n\nAt the same time, the reviewers and AC note several potential weaknesses, the focus here is on the subset that substantially affected the final acceptance decision. First, the paper deviates from the majority of current state of the art deep RL approaches by leveraging prior knowledge in the form of the RAM state. The cause for concern is not so much the use of the RAM information, but the comparison to other prior approaches using \"comparable amounts of prior knowledge\" - an argument that was considered misleading by the reviewers and AC. The reviewers make detailed suggestions on how to address these concerns in a future revision. Despite initially diverging assessments, the final consensus between the reviewers and AC was that the stated concerns would require a thorough revision of the paper and that it should not be accepted in its current stage.\n\nOn a separate note, a lot of the discussion between R1 and the authors centered on whether more comparisons / a larger number of seeds should be run. The authors argued that the requested comparisons would be too costly. A suggestion for a future revision of the paper would be to only run a large number (e.g., 10) of seeds for the first 150M steps of each experiment, and presenting these results separately from the long-running experiments. This should be a cost efficient way to shed light on a particularly important range, and would help validate claims about sample efficiency.", "reviews": [{"review_id": "ryxLG2RcYX-0", "review_text": "This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards. In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards. To address this problem, the authors propose an abstract MDP algorithm. The algorithm consists of three parts: manager, worker, and discoverer. The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states. Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation. The main strong point of this paper is the experiment section. The proposed algorithm outperforms all previous state of the art algorithms for Montezuma\u2019s revenge, Pitfall!, and Private eye over a factor of 2. It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state. In some RL tasks, it is not allowed to access the RAM state. ================================ I've read all other reviewers' comments and the response from authors, and decreased the score. Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm. I agree to other reviewers. The algorithm assumptions are strong. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 3 for their comments . Reviewer 3 points out the strong state-of-the-art performance of our approach as a strength and mentions prior knowledge ( our use of RAM state information ) as a minor weakness . To clarify , in our experiments , we outperform previous non-demonstration state-of-the-art approaches that use a comparable amount of prior knowledge . We discuss our usage of prior knowledge in greater detail in the section titled \u201c Prior Knowledge \u201d in our response to Reviewer 2 ."}, {"review_id": "ryxLG2RcYX-1", "review_text": "This paper considers how to effectively perform exploration in the setting where a difficult, high-dimensional MDP can be mapped to a simpler, lower-dimensional MDP. They propose a hierarchical approach where a model of the abstract MDP is incrementally learned, and then used to train sub-policies to transition between abstract states. These sub-policies are trained using intrinsic rewards for transitioning to the correct state, and the transition probabilities in the abstract MDP reflect how well a sub-policy can perform the transition. The approach is evaluated on three difficult Atari games, which all require difficult exploration: Montezuma's Revenge, Pitfall and Private Eye, and is shown to achieve good performance in all of them. Furthermore, the model can be used to generalize to new tasks by changing the rewards associated with different transitions. The main downside with this paper is that the mapping from original state (i.e. pixels) to the abstract state is assumed to be known beforehand, which requires prior knowledge. The authors hardcode this mapping for each of the games by fetching the relevant bits of information from RAM. This prevents fair comparison to many other methods which only use pixels, and makes this paper borderline rather than strong accept. Quality: the method is evaluated on difficult problems and shown to perform well. The experiments are thorough and explore a variety of dimensions such as robustness to stochasticity, granularity of the abstract state and generalization to new tasks. The approach does strike me as rather complicated though - it requires 19 (!) different hyperparameters as shown in table 2. The authors do mention that many of these did not require much tuning and they intend on making their code public. Still, this suggests that re-implementation or extensions by others may be challenging. Are all of these moving parts necessary? Clarity: the paper is well-written, for the most part clear, and the details are thoroughly described in the appendix. Originality: this approach in the context of modern deep learning is to my knowledge novel. Significance: This paper provides a general approach for hierarchical model-based planning when the mapping from the hard MDP to the easy one is known, and in this sense is significant. It is limited by the assumption that the mapping to abstract states is known. I suspect the complexity of the approach may also be a limiting factor. Pros: + good results on 3 challenging problems + effective demonstration of hierachical model-based planning Cons: - requires significant prior knowledge for state encoding - complicated method Minor: - in the intro, last paragraph: \"Our approach significantly outperforms previous non-demonstration SOTA approaches in all 3 domains\". Please specify that you use extra knowledge extracted from RAM, otherwise this is misleading. - Algorithm 1: nagivate -> navigate - Section 4, last sentence: broken appendix link. - Bottom of page 6: \"Recent work on contextual MDPs...as we do here\" is not a sentence. - In related work, it would be nice to mention some relevant early work by Schmidhuber on subgoal generation: http://people.idsia.ch/~juergen/subgoals.html *** Updated *** After reading the updated paper, responses, other reviews, and looking at related works more closely, I have changed my score to a 5. This is due to several factors. Although the paper's core idea is definitely interesting, the fact that they use hardcoded features, rather the standard setup which uses pixels, makes comparison to other methods much more complicated. In particular, I think that the comparison to DQN-PixelCNN is unfair, as this other method makes very few assumptions about the inputs (only that they are pixels). The authors sort of point this out in the main text, but this is somewhat misleading. They say \"PixelCNN uses less prior knowledge than our approach\". In fact, it uses as much prior knowledge as any RL method which operates on pixels. Granted, this is nonzero, but it's vastly less than what this paper's method assumes. The other comparison is to SOORL (which uses a different state encoding altogether). The comparison to SmartHash is fairer, although the variant of SmartHash they compare against is not the main method the paper proposes (a generic autoencoder-based state encoding which makes minimal assumptions about the input). It would have been better if the authors included experiments for their method using such a learned state encoding. Reporting SOTA results on very hard tasks using extra hardcoded features or other domain knowledge is potentially misleading to the community as to how far along we are in solving these tasks, and extra care should be taken to put these results in context. Otherwise, for those not familiar with the subtleties, this makes it seem like these tasks are being solved when in fact they are not. My concern is that other works may then be asked to be compared against these artificially high results. Having many different task setups also makes comparison between different published works confusing in general. Other works (such as Ostrovski et al) have been able to make progress on these tasks while staying within the standard pixel-based framework. These concerns would have been partially mitigated had the authors made it *very* clear that they were assuming substantial prior knowledge, which makes their method non-comparable to others which do not make this assumption. This could have been done in the introduction (which was one of my comments, but this was not included in the updated draft). I.e., something to the effect of \"We emphasize that our approach assumes substantially more prior knowledge than other approaches which operate only on pixels, and as such is not directly comparable with these approaches\". In addition, I would have liked if the authors had followed the suggestion of Reviewer 1 to include results in pixel space, even if negative, but this was not done either (using a simple autoencoder-based representation, like the one in the SmartHash paper, would have also been fine). As it is, statements such as \"Our approach achieves more than 2x the reward of prior non-demonstration SOTA approaches\" and \"our approach relies on some prior knowledge in the state abstraction function, although we compare against SOTA methods using a similar amount of prior knowledge in our experiments\" are quite misleading and unfair to other methods which do not assume access to prior knowledge (the second statement is untrue for the case of DQN-PixelCNN). Another point which I had not noticed previously is the very high sample complexity (2 billion). One of the motivations behind model-based approaches is that they are supposed to be more sample efficient, but that does not seem to be the case here. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank Reviewer 2 for their detailed and thoughtful feedback ! Reviewer 2 raises two main concerns : 1 ) that our approach requires prior knowledge and 2 ) that our approach is complicated , which we address in the two sections below : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Prior Knowledge In this work , we assume access to prior knowledge ( i.e. , RAM state information ) in the form of the state abstraction function . However , in our experiments , we compare with state-of-the-art approaches that use a comparable amount of prior knowledge ( these approaches use more prior knowledge in 1 game , the same prior knowledge in 1 game , and less prior knowledge in 1 game ) . In each game , we compare with the highest scoring non-demonstration approach and we achieve new state-of-the-art results in each game , by over 2x : - In Montezuma \u2019 s Revenge , we compare with SmartHash , which requires RAM state information equivalent to the prior knowledge used by our approach . Our approach achieves over 2x as much reward as SmartHash on average . - In Pitfall ! , we compare with SOORL , which requires parsing out all the relevant objects on the screen , prior knowledge much stronger than that used by our approach . Our approach achieves over 10x as much reward on average . In addition , we also compare with Apex DQfD , which uses expert demonstrations , even stronger prior knowledge . Our approach achieves about 2.5x the reward of Apex DQfD on average . We note that no prior approach has ever achieved > 0 reward on Pitfall ! with only RAM state information ( our approach achieves ~10K reward ) . - In Private Eye , we compare with DQN-CTS , which encodes the prior knowledge that semantically different states tend to have very different pixels . DQN-CTS uses weaker prior knowledge than our approach , but we compare with DQN-CTS because it achieves the best performance out of all non-demonstration prior approaches . Our approach achieves over 2x as much reward as DQN-CTS on average . To further understand what portion of the performance of our method is due to just prior knowledge , we \u2019 ve run additional experiments with AbstractStateHash , an approach ( described in greater detail in the paper ) which uses the same prior knowledge as our approach and uses this prior knowledge to do count-based exploration ( count-based exploration methods have achieved the prior state-of-the-art results in the hardest exploration games ) . In the initial submission , we already reported results of AbstractStateHash on Montezuma \u2019 s Revenge , which achieves results competitive with the prior state-of-the-art ; our approach achieves > 2x the reward of AbstractStateHash . We will soon submit an updated draft with results of AbstractStateHash on Pitfall ! and Private Eye and we provide a summary below . - On Pitfall ! , AbstractStateHash achieves 0 reward ( comparable with many strong approaches , e.g. , DQN-PixelCNN and Rainbow ) , whereas our approach achieves ~10K reward . - On Private Eye , our approach achieves > 100x the reward of AbstractStateHash . These results suggest that while the RAM state prior knowledge does provide our approach valuable signal , prior state-of-the-art methods do not effectively leverage this prior knowledge . In addition , in Section 7.5 , we analyze the effect of varying the state abstraction function to answer the question of : how hard is it to find a state abstraction function that works well with our method ? We find that our approach significantly outperforms the prior state-of-the-art under many abstract state representations . This alleviates the burden of selecting the perfect state abstraction function for new tasks ( in our case , for each game , we selected an abstraction function and never changed or tuned it ) , and suggests that future work could find different state abstraction functions requiring less prior knowledge . In other domains , it may also be possible to easily extract abstract states from the state . For example , many robotics tasks have fully observable states ( e.g.consisting of joint angles of a robotic arm and positions of objects ) . In these tasks , a good state abstraction function might just extract the dimensions corresponding to the position of the gripper and the positions of the objects ."}, {"review_id": "ryxLG2RcYX-2", "review_text": "This paper deal with learning abstract MDPs for planning in tasks that require long-horizon due to sparse rewards. This is an extremely important and timely topic in the RL community. The paper is generally clear and well written. The proposed algorithm seems reasonable and it is conceptually simple to understand. In the current experimental results presented it also seems to outperform the alternative baselines. Nonetheless, the paper has few flaws that significantly impact the stated contributions and reduced my rating. 1) a stated contribution are theoretical guarantees about the performance of the algorithm. this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying. Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?). Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic. Overall, if you want to claim theoretical guarantees you will have to significantly improve the manuscript. 2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature. Listing related work is no the same as describing similarities and differences compared to previous methods. For example, a paper that obviously comes to mind is \"FeUdal Networks for Hierarchical Reinforcement Learning\". What are the differences to your approach? Also, please place the related work earlier on in the paper. Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature. 3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used. This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines. feudal RL should be one, Roderick et al 2017 should be another one (especially considering your discussion in Sec 8) Additional feedback: - The paper is currently oriented towards discrete states. What can you say about continuous spaces? - The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance? - Using only 4 seeds seems too little to provide accurate standard deviations. Please run at least 10 experiments. - The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative. Otherwise, this choice is incomprehensible. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank Reviewer 1 for their detailed comments and feedback . Reviewer 1 \u2019 s main concerns are 1 ) that the related works section does not sufficiently frame our work with previous literature , 2 ) that the proofs of theoretical guarantees are not sufficiently rigorous , and 3 ) that the experiments section is not comprehensive enough . We have posted a significantly updated new draft to address these concerns . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Experiments Reviewer 1 claims that we do not sufficiently compare with enough other methods , and specifically asks for comparisons with Feudal Networks ( FuN ) and Roderick et al. , 2017 . We already comprehensively compare with the prior non-demonstration state-of-the-art , which use a comparable amount of prior knowledge , in each game . Since we already compare with the prior state-of-the-art approaches , and other approaches perform significantly worse than the prior state-of-the-art approaches , we do not compare with the many other deep RL approaches . In particular , FuN and Roderick et al. , 2017 both report results on Montezuma \u2019 s Revenge . The prior state-of-the-art approach we compare against , SmartHash , outperforms these approaches by 1.75x and 4x respectively , at the number of frames they report ( 200M and 50M respectively ) . Our approach further outperforms SmartHash by over 2x . Reviewer 1 further asks for evaluation on more games . We believe that we have already demonstrated a significant improvement over the prior state-of-the-art , and additional experiments could be prohibitively expensive . In particular , we follow Aytar et al. , 2018 , and evaluate on 3 of the hardest exploration games from the Arcade Learning Environment . We do not evaluate on many of the simpler other games ( e.g. , Breakout ) , because they do not require sophisticated exploration and can already be solved with current state-of-the-art methods . We use the same set of minimally tuned hyperparameters ( tuned only on Montezuma \u2019 s Revenge ) and obtain new state-of-the-art results by over 2x , suggesting that our approach can generalize to new tasks . Our results are not cherry-picked as R1 suggests : following many recent deep RL works , e.g. , Ostrovski et al. , 2017 , Tang et al. , 2017 , we run 4 seeds on each task , and obtain statistically significant results . Even our * worst seed * outperforms or is competitive with the prior state-of-the-art * best seed * . We note that running 10 seeds would approximately cost $ 30,000 per additional game in compute . Renting the appropriate equipment ( e.g. , via Google Cloud ) to run a single seed to completion costs ~ $ 1,500 . To run 20 seeds ( 10 for our approach , 10 for the prior state-of-the-art ) would cost 20 x $ 1,500 = $ 30,000 or roughly the median US annual salary . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Related Works We \u2019 ve updated the related works section in our recently posted draft to more carefully compare Please see Sections 1 and 7 for updated related work . The main critical difference between our work and other HRL works is that we build an abstract MDP , which enables us to plan for targeted exploration ; other works also learn skills and operate in latent abstract state spaces , but not necessarily in a way that satisfies the property of an MDP , which can make effectively using the learned skills difficult . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Theory In the updated draft of our paper , we have updated the rigor of the theory section : please see Section 5 and Appendix C for updated theory . To summarize : we \u2019 re interested in the sample complexity of RL algorithms , i.e. , the number of samples required for the learned policy to become near-optimal ( achieve reward at most epsilon less than the optimal policy ) . Standard results ( e.g. , MBIE-EB , R-MAX ) can guarantee a near-optimal policy , but they require so many samples ( polynomial in the size of the state space ) in deep RL settings , that the guarantees are effectively vacuous . In contrast , for a subclass of MDPs , our approach provably learns a near-optimal policy in a number of samples polynomial in the size of the * abstract * MDP ."}], "0": {"review_id": "ryxLG2RcYX-0", "review_text": "This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards. In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards. To address this problem, the authors propose an abstract MDP algorithm. The algorithm consists of three parts: manager, worker, and discoverer. The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states. Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation. The main strong point of this paper is the experiment section. The proposed algorithm outperforms all previous state of the art algorithms for Montezuma\u2019s revenge, Pitfall!, and Private eye over a factor of 2. It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state. In some RL tasks, it is not allowed to access the RAM state. ================================ I've read all other reviewers' comments and the response from authors, and decreased the score. Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm. I agree to other reviewers. The algorithm assumptions are strong. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 3 for their comments . Reviewer 3 points out the strong state-of-the-art performance of our approach as a strength and mentions prior knowledge ( our use of RAM state information ) as a minor weakness . To clarify , in our experiments , we outperform previous non-demonstration state-of-the-art approaches that use a comparable amount of prior knowledge . We discuss our usage of prior knowledge in greater detail in the section titled \u201c Prior Knowledge \u201d in our response to Reviewer 2 ."}, "1": {"review_id": "ryxLG2RcYX-1", "review_text": "This paper considers how to effectively perform exploration in the setting where a difficult, high-dimensional MDP can be mapped to a simpler, lower-dimensional MDP. They propose a hierarchical approach where a model of the abstract MDP is incrementally learned, and then used to train sub-policies to transition between abstract states. These sub-policies are trained using intrinsic rewards for transitioning to the correct state, and the transition probabilities in the abstract MDP reflect how well a sub-policy can perform the transition. The approach is evaluated on three difficult Atari games, which all require difficult exploration: Montezuma's Revenge, Pitfall and Private Eye, and is shown to achieve good performance in all of them. Furthermore, the model can be used to generalize to new tasks by changing the rewards associated with different transitions. The main downside with this paper is that the mapping from original state (i.e. pixels) to the abstract state is assumed to be known beforehand, which requires prior knowledge. The authors hardcode this mapping for each of the games by fetching the relevant bits of information from RAM. This prevents fair comparison to many other methods which only use pixels, and makes this paper borderline rather than strong accept. Quality: the method is evaluated on difficult problems and shown to perform well. The experiments are thorough and explore a variety of dimensions such as robustness to stochasticity, granularity of the abstract state and generalization to new tasks. The approach does strike me as rather complicated though - it requires 19 (!) different hyperparameters as shown in table 2. The authors do mention that many of these did not require much tuning and they intend on making their code public. Still, this suggests that re-implementation or extensions by others may be challenging. Are all of these moving parts necessary? Clarity: the paper is well-written, for the most part clear, and the details are thoroughly described in the appendix. Originality: this approach in the context of modern deep learning is to my knowledge novel. Significance: This paper provides a general approach for hierarchical model-based planning when the mapping from the hard MDP to the easy one is known, and in this sense is significant. It is limited by the assumption that the mapping to abstract states is known. I suspect the complexity of the approach may also be a limiting factor. Pros: + good results on 3 challenging problems + effective demonstration of hierachical model-based planning Cons: - requires significant prior knowledge for state encoding - complicated method Minor: - in the intro, last paragraph: \"Our approach significantly outperforms previous non-demonstration SOTA approaches in all 3 domains\". Please specify that you use extra knowledge extracted from RAM, otherwise this is misleading. - Algorithm 1: nagivate -> navigate - Section 4, last sentence: broken appendix link. - Bottom of page 6: \"Recent work on contextual MDPs...as we do here\" is not a sentence. - In related work, it would be nice to mention some relevant early work by Schmidhuber on subgoal generation: http://people.idsia.ch/~juergen/subgoals.html *** Updated *** After reading the updated paper, responses, other reviews, and looking at related works more closely, I have changed my score to a 5. This is due to several factors. Although the paper's core idea is definitely interesting, the fact that they use hardcoded features, rather the standard setup which uses pixels, makes comparison to other methods much more complicated. In particular, I think that the comparison to DQN-PixelCNN is unfair, as this other method makes very few assumptions about the inputs (only that they are pixels). The authors sort of point this out in the main text, but this is somewhat misleading. They say \"PixelCNN uses less prior knowledge than our approach\". In fact, it uses as much prior knowledge as any RL method which operates on pixels. Granted, this is nonzero, but it's vastly less than what this paper's method assumes. The other comparison is to SOORL (which uses a different state encoding altogether). The comparison to SmartHash is fairer, although the variant of SmartHash they compare against is not the main method the paper proposes (a generic autoencoder-based state encoding which makes minimal assumptions about the input). It would have been better if the authors included experiments for their method using such a learned state encoding. Reporting SOTA results on very hard tasks using extra hardcoded features or other domain knowledge is potentially misleading to the community as to how far along we are in solving these tasks, and extra care should be taken to put these results in context. Otherwise, for those not familiar with the subtleties, this makes it seem like these tasks are being solved when in fact they are not. My concern is that other works may then be asked to be compared against these artificially high results. Having many different task setups also makes comparison between different published works confusing in general. Other works (such as Ostrovski et al) have been able to make progress on these tasks while staying within the standard pixel-based framework. These concerns would have been partially mitigated had the authors made it *very* clear that they were assuming substantial prior knowledge, which makes their method non-comparable to others which do not make this assumption. This could have been done in the introduction (which was one of my comments, but this was not included in the updated draft). I.e., something to the effect of \"We emphasize that our approach assumes substantially more prior knowledge than other approaches which operate only on pixels, and as such is not directly comparable with these approaches\". In addition, I would have liked if the authors had followed the suggestion of Reviewer 1 to include results in pixel space, even if negative, but this was not done either (using a simple autoencoder-based representation, like the one in the SmartHash paper, would have also been fine). As it is, statements such as \"Our approach achieves more than 2x the reward of prior non-demonstration SOTA approaches\" and \"our approach relies on some prior knowledge in the state abstraction function, although we compare against SOTA methods using a similar amount of prior knowledge in our experiments\" are quite misleading and unfair to other methods which do not assume access to prior knowledge (the second statement is untrue for the case of DQN-PixelCNN). Another point which I had not noticed previously is the very high sample complexity (2 billion). One of the motivations behind model-based approaches is that they are supposed to be more sample efficient, but that does not seem to be the case here. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank Reviewer 2 for their detailed and thoughtful feedback ! Reviewer 2 raises two main concerns : 1 ) that our approach requires prior knowledge and 2 ) that our approach is complicated , which we address in the two sections below : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Prior Knowledge In this work , we assume access to prior knowledge ( i.e. , RAM state information ) in the form of the state abstraction function . However , in our experiments , we compare with state-of-the-art approaches that use a comparable amount of prior knowledge ( these approaches use more prior knowledge in 1 game , the same prior knowledge in 1 game , and less prior knowledge in 1 game ) . In each game , we compare with the highest scoring non-demonstration approach and we achieve new state-of-the-art results in each game , by over 2x : - In Montezuma \u2019 s Revenge , we compare with SmartHash , which requires RAM state information equivalent to the prior knowledge used by our approach . Our approach achieves over 2x as much reward as SmartHash on average . - In Pitfall ! , we compare with SOORL , which requires parsing out all the relevant objects on the screen , prior knowledge much stronger than that used by our approach . Our approach achieves over 10x as much reward on average . In addition , we also compare with Apex DQfD , which uses expert demonstrations , even stronger prior knowledge . Our approach achieves about 2.5x the reward of Apex DQfD on average . We note that no prior approach has ever achieved > 0 reward on Pitfall ! with only RAM state information ( our approach achieves ~10K reward ) . - In Private Eye , we compare with DQN-CTS , which encodes the prior knowledge that semantically different states tend to have very different pixels . DQN-CTS uses weaker prior knowledge than our approach , but we compare with DQN-CTS because it achieves the best performance out of all non-demonstration prior approaches . Our approach achieves over 2x as much reward as DQN-CTS on average . To further understand what portion of the performance of our method is due to just prior knowledge , we \u2019 ve run additional experiments with AbstractStateHash , an approach ( described in greater detail in the paper ) which uses the same prior knowledge as our approach and uses this prior knowledge to do count-based exploration ( count-based exploration methods have achieved the prior state-of-the-art results in the hardest exploration games ) . In the initial submission , we already reported results of AbstractStateHash on Montezuma \u2019 s Revenge , which achieves results competitive with the prior state-of-the-art ; our approach achieves > 2x the reward of AbstractStateHash . We will soon submit an updated draft with results of AbstractStateHash on Pitfall ! and Private Eye and we provide a summary below . - On Pitfall ! , AbstractStateHash achieves 0 reward ( comparable with many strong approaches , e.g. , DQN-PixelCNN and Rainbow ) , whereas our approach achieves ~10K reward . - On Private Eye , our approach achieves > 100x the reward of AbstractStateHash . These results suggest that while the RAM state prior knowledge does provide our approach valuable signal , prior state-of-the-art methods do not effectively leverage this prior knowledge . In addition , in Section 7.5 , we analyze the effect of varying the state abstraction function to answer the question of : how hard is it to find a state abstraction function that works well with our method ? We find that our approach significantly outperforms the prior state-of-the-art under many abstract state representations . This alleviates the burden of selecting the perfect state abstraction function for new tasks ( in our case , for each game , we selected an abstraction function and never changed or tuned it ) , and suggests that future work could find different state abstraction functions requiring less prior knowledge . In other domains , it may also be possible to easily extract abstract states from the state . For example , many robotics tasks have fully observable states ( e.g.consisting of joint angles of a robotic arm and positions of objects ) . In these tasks , a good state abstraction function might just extract the dimensions corresponding to the position of the gripper and the positions of the objects ."}, "2": {"review_id": "ryxLG2RcYX-2", "review_text": "This paper deal with learning abstract MDPs for planning in tasks that require long-horizon due to sparse rewards. This is an extremely important and timely topic in the RL community. The paper is generally clear and well written. The proposed algorithm seems reasonable and it is conceptually simple to understand. In the current experimental results presented it also seems to outperform the alternative baselines. Nonetheless, the paper has few flaws that significantly impact the stated contributions and reduced my rating. 1) a stated contribution are theoretical guarantees about the performance of the algorithm. this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying. Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?). Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic. Overall, if you want to claim theoretical guarantees you will have to significantly improve the manuscript. 2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature. Listing related work is no the same as describing similarities and differences compared to previous methods. For example, a paper that obviously comes to mind is \"FeUdal Networks for Hierarchical Reinforcement Learning\". What are the differences to your approach? Also, please place the related work earlier on in the paper. Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature. 3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used. This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines. feudal RL should be one, Roderick et al 2017 should be another one (especially considering your discussion in Sec 8) Additional feedback: - The paper is currently oriented towards discrete states. What can you say about continuous spaces? - The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance? - Using only 4 seeds seems too little to provide accurate standard deviations. Please run at least 10 experiments. - The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative. Otherwise, this choice is incomprehensible. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank Reviewer 1 for their detailed comments and feedback . Reviewer 1 \u2019 s main concerns are 1 ) that the related works section does not sufficiently frame our work with previous literature , 2 ) that the proofs of theoretical guarantees are not sufficiently rigorous , and 3 ) that the experiments section is not comprehensive enough . We have posted a significantly updated new draft to address these concerns . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Experiments Reviewer 1 claims that we do not sufficiently compare with enough other methods , and specifically asks for comparisons with Feudal Networks ( FuN ) and Roderick et al. , 2017 . We already comprehensively compare with the prior non-demonstration state-of-the-art , which use a comparable amount of prior knowledge , in each game . Since we already compare with the prior state-of-the-art approaches , and other approaches perform significantly worse than the prior state-of-the-art approaches , we do not compare with the many other deep RL approaches . In particular , FuN and Roderick et al. , 2017 both report results on Montezuma \u2019 s Revenge . The prior state-of-the-art approach we compare against , SmartHash , outperforms these approaches by 1.75x and 4x respectively , at the number of frames they report ( 200M and 50M respectively ) . Our approach further outperforms SmartHash by over 2x . Reviewer 1 further asks for evaluation on more games . We believe that we have already demonstrated a significant improvement over the prior state-of-the-art , and additional experiments could be prohibitively expensive . In particular , we follow Aytar et al. , 2018 , and evaluate on 3 of the hardest exploration games from the Arcade Learning Environment . We do not evaluate on many of the simpler other games ( e.g. , Breakout ) , because they do not require sophisticated exploration and can already be solved with current state-of-the-art methods . We use the same set of minimally tuned hyperparameters ( tuned only on Montezuma \u2019 s Revenge ) and obtain new state-of-the-art results by over 2x , suggesting that our approach can generalize to new tasks . Our results are not cherry-picked as R1 suggests : following many recent deep RL works , e.g. , Ostrovski et al. , 2017 , Tang et al. , 2017 , we run 4 seeds on each task , and obtain statistically significant results . Even our * worst seed * outperforms or is competitive with the prior state-of-the-art * best seed * . We note that running 10 seeds would approximately cost $ 30,000 per additional game in compute . Renting the appropriate equipment ( e.g. , via Google Cloud ) to run a single seed to completion costs ~ $ 1,500 . To run 20 seeds ( 10 for our approach , 10 for the prior state-of-the-art ) would cost 20 x $ 1,500 = $ 30,000 or roughly the median US annual salary . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Related Works We \u2019 ve updated the related works section in our recently posted draft to more carefully compare Please see Sections 1 and 7 for updated related work . The main critical difference between our work and other HRL works is that we build an abstract MDP , which enables us to plan for targeted exploration ; other works also learn skills and operate in latent abstract state spaces , but not necessarily in a way that satisfies the property of an MDP , which can make effectively using the learned skills difficult . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Theory In the updated draft of our paper , we have updated the rigor of the theory section : please see Section 5 and Appendix C for updated theory . To summarize : we \u2019 re interested in the sample complexity of RL algorithms , i.e. , the number of samples required for the learned policy to become near-optimal ( achieve reward at most epsilon less than the optimal policy ) . Standard results ( e.g. , MBIE-EB , R-MAX ) can guarantee a near-optimal policy , but they require so many samples ( polynomial in the size of the state space ) in deep RL settings , that the guarantees are effectively vacuous . In contrast , for a subclass of MDPs , our approach provably learns a near-optimal policy in a number of samples polynomial in the size of the * abstract * MDP ."}}