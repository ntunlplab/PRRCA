{"year": "2018", "forum": "HJIoJWZCZ", "title": "Adversarial Dropout Regularization", "decision": "Accept (Poster)", "meta_review": "The general consensus is that this method provides a practical and interesting approach to unsupervised domain adaptation. One reviewer had concerns with comparing to state of the art baselines, but those have been addressed in the revision.\n\nThere were also issues concerning correctness due to a typo. Based on the responses, and on the pseudocode, it seems like there wasn't an issue with the results, just in the way the entropy objective was reported.\n\nYou may want to consider reporting the example given by reviewer 2 as a negative example where you expect the method to fail. This will be helpful for researchers using and building on your paper.", "reviews": [{"review_id": "HJIoJWZCZ-0", "review_text": "(Summary) This paper is about learning discriminative features for the target domain in unsupervised DA problem. The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators. (Pros) The approach proposed in section 3.2 uses dropout logits and the sensitivity criterion between two softmax probability distributions which seems novel. (Cons) 1. By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims \"achieved state of the art results on three datasets.\" in sec5. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Does the proposed method outperform these state of the art methods using the same network architectures? 2. I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G,C. In the current draft, it's not immediately clear how the loss functions depend on the optimization variables. For example, in eqns 2,3,5, the minimization is over G,C but G,C do not appear anywhere in the equation. 3. For the digits experiments, appendix B states \"we used exactly the same network architecture\". Well, which architecture was it? 4. It's not clear what exactly the \"ENT\" baseline is. The text says \"(ENT) obtained by modifying (Springenberg 2015)\". I'd encourage the authors to make this part more explicit and self-explanatory. (Assessment) Borderline. The method section is not very well written and the authors avoid comparing the method against the state of the art methods in unsupervised DA.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We uploaded an updated version of the paper with changes highlighted in blue . To Reviewer 1 1 . By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims `` achieved state of the art results on three datasets . '' in sec5.1 ) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks , Bousmalis et al.CVPR17 , and 2 ) Learning Transferrable Representations for Unsupervised Domain Adaptation , Sener et al.NIPS16.Does the proposed method outperform these state of the art methods using the same network architectures ? In the updated version of our paper , we added new experimental results following the same setting as Bousmalis did ( Table 1 ) . Ours is slightly better on MNIST- > USPS , but Bousmalis et al.don \u2019 t report on more difficult shifts where we achieve state of the art , as such SVHN- > MNIST . In addition , we compared our method with Sener et al.NIPS16 in Table 1 . Changes in the Paper In Table 1 , We added Sener NIPS16 , for SVHN to MNIST . We also added results on MNIST to USPS to compare with Bousmalis CVPR 2016 . Results of our method changed in the adaptation using USPS because we found a bug in preprocessing of USPS . According to the change , we replaced the graph of Fig4 ( a ) ( b ) and we changed the relevant sentences . 2.I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G , C . In the current draft , it 's not immediately clear how the loss functions depend on the optimization variables . For example , in eqns 2,3,5 , the minimization is over G , C but G , C do not appear anywhere in the equation . We clarified notation of Eqns 2,3,5 . Change of paper Change notation of Eqns 2,3,5 . 3.For the digits experiments , appendix B states `` we used exactly the same network architecture '' . Well , which architecture was it ? We wanted to say that , for our baseline method , we used the same network architecture as our proposed method . We added this explanation . Change of paper . Add sentence in the last of our appendix section ( Digits Classification Training Detail ) . 4.It 's not clear what exactly the `` ENT '' baseline is . The text says `` ( ENT ) obtained by modifying ( Springenberg 2015 ) '' . I 'd encourage the authors to make this part more explicit and self-explanatory . We did explain it in the appendix , but we added sentences to make the method clearer . Change of paper Add sentence in Section 2 , Section 4.2 ."}, {"review_id": "HJIoJWZCZ-1", "review_text": " Unsupervised Domain adaptation is the problem of training a classifier without labels in some target domain if we have labeled data from a (hopefully) similar dataset with labels. For example, training a classifier using simulated rendered images with labels, to work on real images. Learning discriminative features for the target domain is a fundamental problem for unsupervised domain adaptation. The problem is challenging (and potentially ill-posed) when no labeled examples are given in the target domain. This paper proposes a new training technique called ADR, which tries to learn discriminative features for the target domain. The key idea of this technique is to move the target-domain features away from the source-domain decision boundary. ADR achieves this goal by encouraging the learned features to be robust to the dropout noise applied to the classifier. My main concern about this paper is that the idea of \"placing the target-domain features far away from the source-domain decision boundary\" does not necessarily lead to *discriminative features* for the target domain. In fact, it is easy to come up with a counter-example: the target-domain features are far from the *source-domain* decision boundary, but they are all (both the positive and negative examples) on the same side of the boundary, which leads to poor target classification accuracy. The loss function (Equations 2-5) proposed in the paper does not prevent the occurrence of this counter-example. Another concern comes from using the proposed idea in training a GAN (Section 4.3). Generating fake images that are far away from the boundary (as forced by the first term of Equation 9) is somewhat opposite to the objective of GAN training, which aims at aligning distributions of real and fake images. Although the second term of Equation 9 tries to make the generated and the real images similar, the paper does not explain how to properly balance the two terms of Equation 9. As a result, I am worried that the proposed method may lead to more mode-collapsing for GAN. The experimental evaluation seems solid for domain adaptation. The semi-supervised GANs part seemed significantly less developed and might be weakening rather than strengthening the paper. Overall the performance of the proposed method is quite well done and the results are encouraging, despite the lack of theoretical foundations for this method. ", "rating": "7: Good paper, accept", "reply_text": "We uploaded an updated version of the paper with changes highlighted in blue . To Reviewer 2 1. , My main concern about this paper is that the idea of `` placing the target-domain features far away from the source-domain decision boundary '' does not necessarily lead to * discriminative features * for the target domain . In fact , it is easy to come up with a counter-example : the target-domain features are far from the * source-domain * decision boundary , but they are all ( both the positive and negative examples ) on the same side of the boundary , which leads to poor target classification accuracy . The loss function ( Equations 2-5 ) proposed in the paper does not prevent the occurrence of this counter-example . Yes , we understand that there can be such a counter-example with our method . Note that we add a term that discourages target examples from being placed on one side of the boundary . However it is possible in theory that positive and negative examples switch labels , but we find that this does not occur in practice , and our method works well based on our experimental results . 2. , Another concern comes from using the proposed idea in training a GAN ( Section 4.3 ) . Generating fake images that are far away from the boundary ( as forced by the first term of Equation 9 ) is somewhat opposite to the objective of GAN training , which aims at aligning distributions of real and fake images . Although the second term of Equation 9 tries to make the generated and the real images similar , the paper does not explain how to properly balance the two terms of Equation 9 . As a result , I am worried that the proposed method may lead to more mode-collapsing for GAN . The experimental evaluation seems solid for domain adaptation . The semi-supervised GANs part seemed significantly less developed and might be weakening rather than strengthening the paper . If the goal is to train a GAN to mimic a distribution only , then our additional objective may not help , but if the goal is to learn features for semi-supervised learning , then our objective helps by forcing the GAN to not generate fake images near the boundary ( ambiguous features ) ."}, {"review_id": "HJIoJWZCZ-2", "review_text": "I think the paper was mostly well-written, the idea was simple and great. I'm still wrapping my head around it and it took me a while to feel convinced that this idea helps with domain adaptation. A better explanation of the intuition would help other readers. The experiments were extensive and show that this is a solid new method for trying out for any adaptation problem. This also shows how to better utilize task models associated with GANs and domain adversarial training, as used eg. by Bousmalis et al., CVPR 2017, or Ganin et al, ICML 2015, Ghifary et al, ECCV 2016, etc. I think important work was missing in related work for domain adaptation. I think it's particularly important to talk about pixel/image-level adaptations eg CycleGAN/DiscoGAN etc and specifically as those were used for domain adaptation such as Domain Transfer Networks, PixelDA, etc. Other works like Ghifary et al, 2016, Bousmalis et al. 2016 could also be cited in the list of matching distributions in hidden layers of a CNN. Some specific comments: Sect. 3 paragraph 2 should be much clearer, it was hard to understand. In Sect. 3.1 you mention that each node of the network is removed with some probability; this is not true. it's each node within a layer associated with dropout (unless you have dropout on every layer in the network). It also wasn't clear to me whether C_1 and C_2 are always different. If so, is the symmetric KL divergence still valid if it's minimizing the divergence of distributions that are different in every iteration? (Nit: capitalize Kullback Leibler) Eq.3 I think the minus should be a plus? Fig.3 should be improved, it wasn't well presented and a few labels as to what everything is could help the reader significantly. It also seems that neuron 3 does all the work here, which was a bit confusing to me. Could you explain that? On p.6 you discuss that you don't use a target validation set as in Saito et al. Is one really better than the other and why? In other words, how do you obtain these fixed hyperparameters that you use? On p. 9 you claim that the unlabeled images should be distributed uniformly among the classes. Why is that? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We uploaded an updated version of the paper with changes highlighted in blue . To Reviewer 3 1 , I think important work was missing in related work for domain adaptation . I think it 's particularly important to talk about pixel/image-level adaptations eg CycleGAN/DiscoGAN etc and specifically as those were used for domain adaptation such as Domain Transfer Networks , PixelDA , etc . Other works like Ghifary et al , 2016 , Bousmalis et al.2016 could also be cited in the list of matching distributions in hidden layers of a CNN . We will refer to such methods and compare with PixelDA as possible as we can . ( Same question as Reviewer1 , 1 ) 2 . Sect.3 paragraph 2 should be much clearer , it was hard to understand . We changed paragraph 2 of section 3 . 3.In Sect.3.1 you mention that each node of the network is removed with some probability ; this is not true . it 's each node within a layer associated with dropout ( unless you have dropout on every layer in the network ) . It also was n't clear to me whether C_1 and C_2 are always different . If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? ( Nit : capitalize Kullback Leibler ) \u201d It also was n't clear to me whether C_1 and C_2 are always different \u201d \u2192C_1 and C_2 are not necessarily always different . C_1 and C_2 can be the same classifier . However , it rarely happens . \u201c If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? \u201d \u2192Yes , we think it is valid . The generator tries to minimize the divergence . The divergence means the sensitivity to noise caused by dropout . The goal of minimizing it is to generate features that are insensitive to the dropout noise . We minimize the divergence of distributions that are different in almost every iteration . 4.Eq.3 I think the minus should be a plus ? No.In Eq.3 , we aim to maximize the sensitivity for classifiers . In this phase , the classifiers should be trained to be sensitive to the noise caused by dropout . Thus , the minus should be a minus . 5.Fig.3 should be improved , it was n't well presented and a few labels as to what everything is could help the reader significantly . It also seems that neuron 3 does all the work here , which was a bit confusing to me . Could you explain that ? We improved the presentation . Neuron 3 seems to be dominant in bottom row ( our method.However , when comparing Neuron 3 and Column 6 , the shape of boundary looks a little different because of the effect of other neurons . What we wanted to show here is that each neurons will learn different features by our method . We will improve our presentation . Change of paper Add notation in Figure 3 , add caption . 6. , On p.6 you discuss that you do n't use a target validation set as in Saito et al.Is one really better than the other and why ? In other words , how do you obtain these fixed hyperparameters that you use ? The main hyperparameter in our method is n , which indicates how many times to repeat Step 3 in our method . We set 4 in our experiments . Although we did not show in our experimental results , we tried other number such as 1,2,3 . Through the experiment , we found that 4 works well in most settings . With regard to other hyperparameters , such as batch-size , learning rate , we used the ones that are common in other papers on domain adaptation . If one uses a target val set ( as in Saito et al . ) , then one assumes access to training labels on target , which we don \u2019 t want to assume in our setting . 7.On p. 9 you claim that the unlabeled images should be distributed uniformly among the classes . Why is that ? We assumed that it is not desirable if unlabeled images are aligned with one class . We add this term following \u201c Unsupervised and semi-supervised learning with categorical generative adversarial networks \u201d ."}], "0": {"review_id": "HJIoJWZCZ-0", "review_text": "(Summary) This paper is about learning discriminative features for the target domain in unsupervised DA problem. The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators. (Pros) The approach proposed in section 3.2 uses dropout logits and the sensitivity criterion between two softmax probability distributions which seems novel. (Cons) 1. By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims \"achieved state of the art results on three datasets.\" in sec5. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Does the proposed method outperform these state of the art methods using the same network architectures? 2. I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G,C. In the current draft, it's not immediately clear how the loss functions depend on the optimization variables. For example, in eqns 2,3,5, the minimization is over G,C but G,C do not appear anywhere in the equation. 3. For the digits experiments, appendix B states \"we used exactly the same network architecture\". Well, which architecture was it? 4. It's not clear what exactly the \"ENT\" baseline is. The text says \"(ENT) obtained by modifying (Springenberg 2015)\". I'd encourage the authors to make this part more explicit and self-explanatory. (Assessment) Borderline. The method section is not very well written and the authors avoid comparing the method against the state of the art methods in unsupervised DA.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We uploaded an updated version of the paper with changes highlighted in blue . To Reviewer 1 1 . By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims `` achieved state of the art results on three datasets . '' in sec5.1 ) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks , Bousmalis et al.CVPR17 , and 2 ) Learning Transferrable Representations for Unsupervised Domain Adaptation , Sener et al.NIPS16.Does the proposed method outperform these state of the art methods using the same network architectures ? In the updated version of our paper , we added new experimental results following the same setting as Bousmalis did ( Table 1 ) . Ours is slightly better on MNIST- > USPS , but Bousmalis et al.don \u2019 t report on more difficult shifts where we achieve state of the art , as such SVHN- > MNIST . In addition , we compared our method with Sener et al.NIPS16 in Table 1 . Changes in the Paper In Table 1 , We added Sener NIPS16 , for SVHN to MNIST . We also added results on MNIST to USPS to compare with Bousmalis CVPR 2016 . Results of our method changed in the adaptation using USPS because we found a bug in preprocessing of USPS . According to the change , we replaced the graph of Fig4 ( a ) ( b ) and we changed the relevant sentences . 2.I suggest the authors to rewrite the method section 3.2 so that the loss function depends on the optimization variables G , C . In the current draft , it 's not immediately clear how the loss functions depend on the optimization variables . For example , in eqns 2,3,5 , the minimization is over G , C but G , C do not appear anywhere in the equation . We clarified notation of Eqns 2,3,5 . Change of paper Change notation of Eqns 2,3,5 . 3.For the digits experiments , appendix B states `` we used exactly the same network architecture '' . Well , which architecture was it ? We wanted to say that , for our baseline method , we used the same network architecture as our proposed method . We added this explanation . Change of paper . Add sentence in the last of our appendix section ( Digits Classification Training Detail ) . 4.It 's not clear what exactly the `` ENT '' baseline is . The text says `` ( ENT ) obtained by modifying ( Springenberg 2015 ) '' . I 'd encourage the authors to make this part more explicit and self-explanatory . We did explain it in the appendix , but we added sentences to make the method clearer . Change of paper Add sentence in Section 2 , Section 4.2 ."}, "1": {"review_id": "HJIoJWZCZ-1", "review_text": " Unsupervised Domain adaptation is the problem of training a classifier without labels in some target domain if we have labeled data from a (hopefully) similar dataset with labels. For example, training a classifier using simulated rendered images with labels, to work on real images. Learning discriminative features for the target domain is a fundamental problem for unsupervised domain adaptation. The problem is challenging (and potentially ill-posed) when no labeled examples are given in the target domain. This paper proposes a new training technique called ADR, which tries to learn discriminative features for the target domain. The key idea of this technique is to move the target-domain features away from the source-domain decision boundary. ADR achieves this goal by encouraging the learned features to be robust to the dropout noise applied to the classifier. My main concern about this paper is that the idea of \"placing the target-domain features far away from the source-domain decision boundary\" does not necessarily lead to *discriminative features* for the target domain. In fact, it is easy to come up with a counter-example: the target-domain features are far from the *source-domain* decision boundary, but they are all (both the positive and negative examples) on the same side of the boundary, which leads to poor target classification accuracy. The loss function (Equations 2-5) proposed in the paper does not prevent the occurrence of this counter-example. Another concern comes from using the proposed idea in training a GAN (Section 4.3). Generating fake images that are far away from the boundary (as forced by the first term of Equation 9) is somewhat opposite to the objective of GAN training, which aims at aligning distributions of real and fake images. Although the second term of Equation 9 tries to make the generated and the real images similar, the paper does not explain how to properly balance the two terms of Equation 9. As a result, I am worried that the proposed method may lead to more mode-collapsing for GAN. The experimental evaluation seems solid for domain adaptation. The semi-supervised GANs part seemed significantly less developed and might be weakening rather than strengthening the paper. Overall the performance of the proposed method is quite well done and the results are encouraging, despite the lack of theoretical foundations for this method. ", "rating": "7: Good paper, accept", "reply_text": "We uploaded an updated version of the paper with changes highlighted in blue . To Reviewer 2 1. , My main concern about this paper is that the idea of `` placing the target-domain features far away from the source-domain decision boundary '' does not necessarily lead to * discriminative features * for the target domain . In fact , it is easy to come up with a counter-example : the target-domain features are far from the * source-domain * decision boundary , but they are all ( both the positive and negative examples ) on the same side of the boundary , which leads to poor target classification accuracy . The loss function ( Equations 2-5 ) proposed in the paper does not prevent the occurrence of this counter-example . Yes , we understand that there can be such a counter-example with our method . Note that we add a term that discourages target examples from being placed on one side of the boundary . However it is possible in theory that positive and negative examples switch labels , but we find that this does not occur in practice , and our method works well based on our experimental results . 2. , Another concern comes from using the proposed idea in training a GAN ( Section 4.3 ) . Generating fake images that are far away from the boundary ( as forced by the first term of Equation 9 ) is somewhat opposite to the objective of GAN training , which aims at aligning distributions of real and fake images . Although the second term of Equation 9 tries to make the generated and the real images similar , the paper does not explain how to properly balance the two terms of Equation 9 . As a result , I am worried that the proposed method may lead to more mode-collapsing for GAN . The experimental evaluation seems solid for domain adaptation . The semi-supervised GANs part seemed significantly less developed and might be weakening rather than strengthening the paper . If the goal is to train a GAN to mimic a distribution only , then our additional objective may not help , but if the goal is to learn features for semi-supervised learning , then our objective helps by forcing the GAN to not generate fake images near the boundary ( ambiguous features ) ."}, "2": {"review_id": "HJIoJWZCZ-2", "review_text": "I think the paper was mostly well-written, the idea was simple and great. I'm still wrapping my head around it and it took me a while to feel convinced that this idea helps with domain adaptation. A better explanation of the intuition would help other readers. The experiments were extensive and show that this is a solid new method for trying out for any adaptation problem. This also shows how to better utilize task models associated with GANs and domain adversarial training, as used eg. by Bousmalis et al., CVPR 2017, or Ganin et al, ICML 2015, Ghifary et al, ECCV 2016, etc. I think important work was missing in related work for domain adaptation. I think it's particularly important to talk about pixel/image-level adaptations eg CycleGAN/DiscoGAN etc and specifically as those were used for domain adaptation such as Domain Transfer Networks, PixelDA, etc. Other works like Ghifary et al, 2016, Bousmalis et al. 2016 could also be cited in the list of matching distributions in hidden layers of a CNN. Some specific comments: Sect. 3 paragraph 2 should be much clearer, it was hard to understand. In Sect. 3.1 you mention that each node of the network is removed with some probability; this is not true. it's each node within a layer associated with dropout (unless you have dropout on every layer in the network). It also wasn't clear to me whether C_1 and C_2 are always different. If so, is the symmetric KL divergence still valid if it's minimizing the divergence of distributions that are different in every iteration? (Nit: capitalize Kullback Leibler) Eq.3 I think the minus should be a plus? Fig.3 should be improved, it wasn't well presented and a few labels as to what everything is could help the reader significantly. It also seems that neuron 3 does all the work here, which was a bit confusing to me. Could you explain that? On p.6 you discuss that you don't use a target validation set as in Saito et al. Is one really better than the other and why? In other words, how do you obtain these fixed hyperparameters that you use? On p. 9 you claim that the unlabeled images should be distributed uniformly among the classes. Why is that? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We uploaded an updated version of the paper with changes highlighted in blue . To Reviewer 3 1 , I think important work was missing in related work for domain adaptation . I think it 's particularly important to talk about pixel/image-level adaptations eg CycleGAN/DiscoGAN etc and specifically as those were used for domain adaptation such as Domain Transfer Networks , PixelDA , etc . Other works like Ghifary et al , 2016 , Bousmalis et al.2016 could also be cited in the list of matching distributions in hidden layers of a CNN . We will refer to such methods and compare with PixelDA as possible as we can . ( Same question as Reviewer1 , 1 ) 2 . Sect.3 paragraph 2 should be much clearer , it was hard to understand . We changed paragraph 2 of section 3 . 3.In Sect.3.1 you mention that each node of the network is removed with some probability ; this is not true . it 's each node within a layer associated with dropout ( unless you have dropout on every layer in the network ) . It also was n't clear to me whether C_1 and C_2 are always different . If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? ( Nit : capitalize Kullback Leibler ) \u201d It also was n't clear to me whether C_1 and C_2 are always different \u201d \u2192C_1 and C_2 are not necessarily always different . C_1 and C_2 can be the same classifier . However , it rarely happens . \u201c If so , is the symmetric KL divergence still valid if it 's minimizing the divergence of distributions that are different in every iteration ? \u201d \u2192Yes , we think it is valid . The generator tries to minimize the divergence . The divergence means the sensitivity to noise caused by dropout . The goal of minimizing it is to generate features that are insensitive to the dropout noise . We minimize the divergence of distributions that are different in almost every iteration . 4.Eq.3 I think the minus should be a plus ? No.In Eq.3 , we aim to maximize the sensitivity for classifiers . In this phase , the classifiers should be trained to be sensitive to the noise caused by dropout . Thus , the minus should be a minus . 5.Fig.3 should be improved , it was n't well presented and a few labels as to what everything is could help the reader significantly . It also seems that neuron 3 does all the work here , which was a bit confusing to me . Could you explain that ? We improved the presentation . Neuron 3 seems to be dominant in bottom row ( our method.However , when comparing Neuron 3 and Column 6 , the shape of boundary looks a little different because of the effect of other neurons . What we wanted to show here is that each neurons will learn different features by our method . We will improve our presentation . Change of paper Add notation in Figure 3 , add caption . 6. , On p.6 you discuss that you do n't use a target validation set as in Saito et al.Is one really better than the other and why ? In other words , how do you obtain these fixed hyperparameters that you use ? The main hyperparameter in our method is n , which indicates how many times to repeat Step 3 in our method . We set 4 in our experiments . Although we did not show in our experimental results , we tried other number such as 1,2,3 . Through the experiment , we found that 4 works well in most settings . With regard to other hyperparameters , such as batch-size , learning rate , we used the ones that are common in other papers on domain adaptation . If one uses a target val set ( as in Saito et al . ) , then one assumes access to training labels on target , which we don \u2019 t want to assume in our setting . 7.On p. 9 you claim that the unlabeled images should be distributed uniformly among the classes . Why is that ? We assumed that it is not desirable if unlabeled images are aligned with one class . We add this term following \u201c Unsupervised and semi-supervised learning with categorical generative adversarial networks \u201d ."}}