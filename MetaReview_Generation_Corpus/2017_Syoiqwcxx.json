{"year": "2017", "forum": "Syoiqwcxx", "title": "Local minima in training of deep networks", "decision": "Reject", "meta_review": "This paper analyzes under which circumstances bad local optima prevent effective training of deep neural networks. The contribution is real, but the gap between the proposed scenarios and real training scenarios diminishes its importance.", "reviews": [{"review_id": "Syoiqwcxx-0", "review_text": "The main merit of this paper is to draw again attention to how crucial initialization of deep network *can* be; and to counter the popular impression that modern architectures and improved gradient descent techniques make optimization local minima and saddle points no longer a problem. While the paper provides interesting counter-examples that showcase how bad initialization mixed with particular data can lead the optimization to get stuck at a poor solution, these feel like contrived artificial constructs. More importantly the paper does not consider popular heuristics that likely help to avoid getting stuck, such as: non-saturating activation functions (e.g. leaky RELU), batch-norm, skip connections (resnet), that can all be thought of as contributing to keep the gradients flowing. The paper puts up a big warning sign about potential initialization problems (with standard RELU nets), but without proposing new solutions or workarounds, nor carrying out a systematic analysis of how this picture is affected by most commonly used current heuristic techniques (in architecture, initialization and training). Such a broader scope analysis, especially if it did lead to insights of practical relevance, could much increase the value of the paper for the reader. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "When constructing examples one often faces a tradeoff between simplicity and universality . What we decided to strive for in this paper was to present elegant examples with a nice , human-readable structure . Believing in the \u201c one picture worth a thousand words \u201d philosophy we decided to go with examples which actually could be drawn . We agree that examples aimed at the common successful heuristics like batch norm or skip connections would be very interesting . However , due to the very nature of those heuristics - they would need to be much more technically complicated . Thus - in the authors minds - the paper benefits from simplicity , and sends a positive message : we really do need those techniques , things can go very wrong very quickly if we do not use them . The authors believed , that there was a value in demonstrating theoretical reasons for us needing the modern deep learning toolkit . Of course , we have ResNets and batch normalization for good reasons , they were invented to deal with problems encountered in practice . However , what the paper adds in the authors \u2019 opinion is the observation , that the difficulties that gave birth to ResNets , skip connections , momentum or dropouts are fundamental in nature . Much of the theoretical research is aimed at proving good behaviour of learning under general weak assumptions not addressing the specifics of the architecture and the details of learning . This might give an impression that the heuristics the deep learning developed in practice are more of a gimmick , and that their purpose is is mostly to speed up the algorithms . What one of the goals of our paper was - was to bring to the attention the fact , that we need those heuristics for deep reasons and that they are often fundamental in order to learn . The authors agree with the suggestion that proposing new solutions or workarounds would provide a lot of extra value . However , that is a very challenging task . One of the practices of the scientific method is , when facing a big and important challenge that can not be dealt with in \u201c one blow \u201d , is to attack it one step at a time . The authors believe that a good starting point is to first identify the \u201c canonical \u201d list of phenomena that present obstacles to the training , to then devise good strategies to make training work despite them . Coming up with better strategies is the authors \u2019 ultimate endgoal . This paper is meant as \u201c recognize what you are fighting against \u201d part of the strategy . Final remark - the authors know examples that \u201c deadlock \u201d networks based on non-saturating activation functions as well . Being limited by the volume we decided to present ReLU based examples , as they are the most commonly used activations . But the \u201c deadlocking \u201d is by no means caused only by the inactive regions . The material in the paper has been reorganized , to improve the presentation , reflecting the valuable feedback the authors received from the reviewers and readers ."}, {"review_id": "Syoiqwcxx-1", "review_text": "This paper studies the error surface of deep rectifier networks, giving specific examples for which the error surface has local minima. Several experimental results show that learning can be trapped at apparent local minima by a variety of factors ranging from the nature of the dataset to the nature of the initializations. This paper develops a lot of good intuitions and useful examples of ways that training can go awry. Even though the examples constructed in this paper are contrived, this does not necessarily remove their theoretical importance. It is very useful to have simple examples where things go wrong. However the broader theoretical framing of the paper appears to be going after a strawman. \u201cThe underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.\u201d I believe this perspective is already contained in several of the works cited as not belonging to this perspective. Choromanska et al., for instance, analyze Gaussian inputs, and so clearly make claims based on characteristics of the data the models are run on. More broadly, the loss function is determined jointly by the dataset and the model parameters, and so no account of the error surface can be separated from dataset properties. It is not clear to me what \u2018emerging structures due to high dimensional spaces\u2019 are, or what they could be, that would make them independent of the dataset and initial model parameters. The emerging structure of the error surface is necessarily related to the dataset and model parameters. Again, a key worry with this paper is that it is aiming at a strawman: replica methods characterize average behavior for infinite systems, so it is not surprising that specific finite sized systems might yield poor optimization landscapes. The paper seems surprised that training can be broken with a bad initialization, but initialization is known to be critical, even for linear networks: saddle points are not innocuous, with bad initializations dramatically slowing learning (e.g. Saxe et al. 2014). It seems like the proof of proposition 5 may have an error. Suppose cdf_b(0) = 0 and cdf_W(0)=1/2. We have P(learning fails) >= 1 - 1/2^{h^2(k-1)}, meaning that the probability of failure _increases_ as the number of hidden units increases. It seems like it should rather be (ignoring the bias) p(fails) >= 1 - [ 1 - p(w<0)^h^2]^{k-1}. In this case the limit as k-> infinity depends on how h scales with k, so it is no longer necessarily true that \u201cone does not have a globally good behaviour of learning regardless of the model size.\u201d The paper also appears to insufficiently distinguish between local minima and saddle points. Section 3.1 states it shows training being stuck in a local minimum, but this is based on training with a fixed budget of epochs. It is not possible to tell whether this result reflects a genuine local minimum or a saddle point based on simulation results. It may also be the case that, while rectifiers suffer from genuine blind spots, sigmoid or soft rectifier nonlinearities may not. On the XOR problem with two hidden nodes, for instance, it was thought that were local minima but in fact there are none (e.g. L. Hamey, \u201cAnalysis of the error surface of the XOR network with two hidden nodes,\u201d 1995). If the desire is simply to show that training does not converge for particular finite problems, much simpler counterexamples can be constructed and would suffice: set all hidden unit weights to zero, for instance. In the response to prereview questions, the authors write \u2018If the \u201ccomplete characterization\u201d [of the error surface] was indeed universally valid, we would not be able to break the learning with the initialization\u2019 but, as mentioned previously, the basic results for even deep linear networks show that a bad initialization (at or near a saddle point) will break learning. Again, it seems this paper is attacking a straw man along the lines of \u201cnothing can possibly go wrong with neural network training.\u201d No prior theoretical result claims this. The Figure 2 explanation seems counterintuitive to me. Simply scaling the input, if the weight matrices are initialized with zero biases, will not change the regions over which each ReLU activates. That is, this manipulation does not achieve the goal of concentrating \u201cmost of the data points in very few linear regions.\u201d A far more likely explanation is that the much weaker scaling has not been compensated by the learning algorithm, but the algorithm would converge if run longer. The response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling on the data is not one but five orders of magnitude\u2014and indeed the training does converge without issue for scaling up to four orders of magnitude. The response notes that Adam should compensate for the scaling factor, but this depends on the details of the Adam implementation\u2014the epsilon factor used to protect against division by zero, for example. This paper contains many interesting results, but a variety of small technical concerns remain. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , the authors would like to thank the reviewer for spotting the technical flaws that slipped through their quality control . Fortunately , they do not invalidate any of the results and they were easy to repair . Namely : - There was indeed an error in the proof Proposition 5 which now has been corrected . Fortunately , that had no influence on the formulation of the Proposition itself . In the analysis provided , h is treated as a constant , only k ( depth ) is a variable , thus leading to an increasing probability of training failure . One could strengthen this result by providing the upper bound for the growth of h with respect to k for this results to hold ( which roughly speaking would be logarithmic ) , which we omitted for simplicity . - The claim in 3.1 about local minima got there due to an unfortunate error during the final organization of the material . 3.1 initially referred to the ReLU example , in which case we do have a local minimum indeed - which we prove . Thank you for spotting that . We could not agree more that just seeing empirically that the training slows down is not sufficient to make any claims about being stuck in a local minimum . We have made the necessary correction to the text . Especially in this paper that strives to improve our understanding of the basics of the landscape of the error surface the distinction needs to be clear - again , thank you for catching that . The argument \u201c If the desire is simply to show that training does not converge for particular finite problems , much simpler counterexamples can be constructed and would suffice : set all hidden unit weights to zero , for instance. \u201d was a crucial object in a heated debate among the authors when they were planning this research . The authors were facing the dilemma : 1 . Construct an example using ReLUs . Pros : These are the most commonly used activation functions . Cons : ... but it is trivial to deadlock them . Just put all the datapoints in the inactive region for example . 2.Use Sigmoids . Pros : the examples are highly nontrivial and have a fascinating geometry . Cons : The paper would get criticized for studying sigmoids that are so passe . The final decision was to construct interesting ReLU based examples . As much as the authors fully agree that it is extremely easy to construct a trivial example of a situation with no learning occurring , especially when using ReLUs , they believe that there is a value in constructing nontrivial ones , and that the examples presented fall into that category . In particular \u201c all weights set to zero \u201d example is in the authors \u2019 opinion a less interesting than the ones constructed in the paper , because it is a zero measure ( and even a high-codimension isolated set ) in a parameter space , and in most cases an unstable singularity of a training process . Regarding Figure 2 , and the number of linear regions on which the data lies by scaling down each examples : The code used to produce those results used an initialization where biases were non zero ( which is the default initialization provided by torch ) , but sampled from the same Gaussian as the weights . Hence scaling down the images has the desired effect . Also , since the Gaussian from which we sample the biases has mean 0 , the region on which all the data points end up is not the one where all hidden units are inactive ( i.e.the biases are not large negative numbers ) . The material in the paper has been reorganized , to improve the presentation , reflecting the valuable feedback the authors received from the reviewers and readers ."}, {"review_id": "Syoiqwcxx-2", "review_text": "The paper studies some special cases of neural networks and datasets where optimization fails. Most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics. This reduces the practical relevance of the analysis. The experiment \"bad initialization on MNIST\" shows that for very negative biases or weights drawn from a non-centered distribution, all ReLU activations are \"off\" for all data points, and thus, optimization is prevented. This never occurs in practice, because using proper initialization heuristics avoid these cases. The \"jellyfish\" dataset constructed by the authors is demonstrated to be difficult to fit by a small model. However, the size/depth of the considered model is unsuitable for this problem. Proposition 4 assumes that we can choose the mean from which the weight parameters are initialized. This is typically not the case in practice as most initialization heuristics draw weight parameters from a distribution with mean 0. Proposition 5 considers infinitely deep ReLU networks. Very deep networks would however preferably be of type ResNet.", "rating": "3: Clear rejection", "reply_text": "The goal of this paper is to bridge the gap between the theoretical research and practical experience . On one hand a tremendous effort was made to come up with successful recipes for hyperparameter selection , initialization etc . The point made by the reviewer is 100 % valid - yes , they work ! They are the very founding stone of the development and the staggering success of deep learning . Yet the very same time - they are to a big extent absent in the universe of theoretical research . We do not see them in the assumptions of the results trying to prove something about why deep learning works as well as it does . We are not by any means trying to show that deep learning is not working . On the contrary . What we are trying to say is : if one strays from the path laid out by the good practices and experience of thousands of successful experiments , things break badly on really simple examples . Thus , it is important to narrow the gap between the theory and practice , in particular , for the theoretical research to embrace more closely what is known empirically to the practitioners . We thought that ICLR , a forum bringing together both more practically-oriented and more theoretically focused communities was a good place to make this observation . The material in the paper has been reorganized , to improve the presentation , reflecting the valuable feedback the authors received from the reviewers and readers ."}], "0": {"review_id": "Syoiqwcxx-0", "review_text": "The main merit of this paper is to draw again attention to how crucial initialization of deep network *can* be; and to counter the popular impression that modern architectures and improved gradient descent techniques make optimization local minima and saddle points no longer a problem. While the paper provides interesting counter-examples that showcase how bad initialization mixed with particular data can lead the optimization to get stuck at a poor solution, these feel like contrived artificial constructs. More importantly the paper does not consider popular heuristics that likely help to avoid getting stuck, such as: non-saturating activation functions (e.g. leaky RELU), batch-norm, skip connections (resnet), that can all be thought of as contributing to keep the gradients flowing. The paper puts up a big warning sign about potential initialization problems (with standard RELU nets), but without proposing new solutions or workarounds, nor carrying out a systematic analysis of how this picture is affected by most commonly used current heuristic techniques (in architecture, initialization and training). Such a broader scope analysis, especially if it did lead to insights of practical relevance, could much increase the value of the paper for the reader. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "When constructing examples one often faces a tradeoff between simplicity and universality . What we decided to strive for in this paper was to present elegant examples with a nice , human-readable structure . Believing in the \u201c one picture worth a thousand words \u201d philosophy we decided to go with examples which actually could be drawn . We agree that examples aimed at the common successful heuristics like batch norm or skip connections would be very interesting . However , due to the very nature of those heuristics - they would need to be much more technically complicated . Thus - in the authors minds - the paper benefits from simplicity , and sends a positive message : we really do need those techniques , things can go very wrong very quickly if we do not use them . The authors believed , that there was a value in demonstrating theoretical reasons for us needing the modern deep learning toolkit . Of course , we have ResNets and batch normalization for good reasons , they were invented to deal with problems encountered in practice . However , what the paper adds in the authors \u2019 opinion is the observation , that the difficulties that gave birth to ResNets , skip connections , momentum or dropouts are fundamental in nature . Much of the theoretical research is aimed at proving good behaviour of learning under general weak assumptions not addressing the specifics of the architecture and the details of learning . This might give an impression that the heuristics the deep learning developed in practice are more of a gimmick , and that their purpose is is mostly to speed up the algorithms . What one of the goals of our paper was - was to bring to the attention the fact , that we need those heuristics for deep reasons and that they are often fundamental in order to learn . The authors agree with the suggestion that proposing new solutions or workarounds would provide a lot of extra value . However , that is a very challenging task . One of the practices of the scientific method is , when facing a big and important challenge that can not be dealt with in \u201c one blow \u201d , is to attack it one step at a time . The authors believe that a good starting point is to first identify the \u201c canonical \u201d list of phenomena that present obstacles to the training , to then devise good strategies to make training work despite them . Coming up with better strategies is the authors \u2019 ultimate endgoal . This paper is meant as \u201c recognize what you are fighting against \u201d part of the strategy . Final remark - the authors know examples that \u201c deadlock \u201d networks based on non-saturating activation functions as well . Being limited by the volume we decided to present ReLU based examples , as they are the most commonly used activations . But the \u201c deadlocking \u201d is by no means caused only by the inactive regions . The material in the paper has been reorganized , to improve the presentation , reflecting the valuable feedback the authors received from the reviewers and readers ."}, "1": {"review_id": "Syoiqwcxx-1", "review_text": "This paper studies the error surface of deep rectifier networks, giving specific examples for which the error surface has local minima. Several experimental results show that learning can be trapped at apparent local minima by a variety of factors ranging from the nature of the dataset to the nature of the initializations. This paper develops a lot of good intuitions and useful examples of ways that training can go awry. Even though the examples constructed in this paper are contrived, this does not necessarily remove their theoretical importance. It is very useful to have simple examples where things go wrong. However the broader theoretical framing of the paper appears to be going after a strawman. \u201cThe underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.\u201d I believe this perspective is already contained in several of the works cited as not belonging to this perspective. Choromanska et al., for instance, analyze Gaussian inputs, and so clearly make claims based on characteristics of the data the models are run on. More broadly, the loss function is determined jointly by the dataset and the model parameters, and so no account of the error surface can be separated from dataset properties. It is not clear to me what \u2018emerging structures due to high dimensional spaces\u2019 are, or what they could be, that would make them independent of the dataset and initial model parameters. The emerging structure of the error surface is necessarily related to the dataset and model parameters. Again, a key worry with this paper is that it is aiming at a strawman: replica methods characterize average behavior for infinite systems, so it is not surprising that specific finite sized systems might yield poor optimization landscapes. The paper seems surprised that training can be broken with a bad initialization, but initialization is known to be critical, even for linear networks: saddle points are not innocuous, with bad initializations dramatically slowing learning (e.g. Saxe et al. 2014). It seems like the proof of proposition 5 may have an error. Suppose cdf_b(0) = 0 and cdf_W(0)=1/2. We have P(learning fails) >= 1 - 1/2^{h^2(k-1)}, meaning that the probability of failure _increases_ as the number of hidden units increases. It seems like it should rather be (ignoring the bias) p(fails) >= 1 - [ 1 - p(w<0)^h^2]^{k-1}. In this case the limit as k-> infinity depends on how h scales with k, so it is no longer necessarily true that \u201cone does not have a globally good behaviour of learning regardless of the model size.\u201d The paper also appears to insufficiently distinguish between local minima and saddle points. Section 3.1 states it shows training being stuck in a local minimum, but this is based on training with a fixed budget of epochs. It is not possible to tell whether this result reflects a genuine local minimum or a saddle point based on simulation results. It may also be the case that, while rectifiers suffer from genuine blind spots, sigmoid or soft rectifier nonlinearities may not. On the XOR problem with two hidden nodes, for instance, it was thought that were local minima but in fact there are none (e.g. L. Hamey, \u201cAnalysis of the error surface of the XOR network with two hidden nodes,\u201d 1995). If the desire is simply to show that training does not converge for particular finite problems, much simpler counterexamples can be constructed and would suffice: set all hidden unit weights to zero, for instance. In the response to prereview questions, the authors write \u2018If the \u201ccomplete characterization\u201d [of the error surface] was indeed universally valid, we would not be able to break the learning with the initialization\u2019 but, as mentioned previously, the basic results for even deep linear networks show that a bad initialization (at or near a saddle point) will break learning. Again, it seems this paper is attacking a straw man along the lines of \u201cnothing can possibly go wrong with neural network training.\u201d No prior theoretical result claims this. The Figure 2 explanation seems counterintuitive to me. Simply scaling the input, if the weight matrices are initialized with zero biases, will not change the regions over which each ReLU activates. That is, this manipulation does not achieve the goal of concentrating \u201cmost of the data points in very few linear regions.\u201d A far more likely explanation is that the much weaker scaling has not been compensated by the learning algorithm, but the algorithm would converge if run longer. The response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling on the data is not one but five orders of magnitude\u2014and indeed the training does converge without issue for scaling up to four orders of magnitude. The response notes that Adam should compensate for the scaling factor, but this depends on the details of the Adam implementation\u2014the epsilon factor used to protect against division by zero, for example. This paper contains many interesting results, but a variety of small technical concerns remain. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , the authors would like to thank the reviewer for spotting the technical flaws that slipped through their quality control . Fortunately , they do not invalidate any of the results and they were easy to repair . Namely : - There was indeed an error in the proof Proposition 5 which now has been corrected . Fortunately , that had no influence on the formulation of the Proposition itself . In the analysis provided , h is treated as a constant , only k ( depth ) is a variable , thus leading to an increasing probability of training failure . One could strengthen this result by providing the upper bound for the growth of h with respect to k for this results to hold ( which roughly speaking would be logarithmic ) , which we omitted for simplicity . - The claim in 3.1 about local minima got there due to an unfortunate error during the final organization of the material . 3.1 initially referred to the ReLU example , in which case we do have a local minimum indeed - which we prove . Thank you for spotting that . We could not agree more that just seeing empirically that the training slows down is not sufficient to make any claims about being stuck in a local minimum . We have made the necessary correction to the text . Especially in this paper that strives to improve our understanding of the basics of the landscape of the error surface the distinction needs to be clear - again , thank you for catching that . The argument \u201c If the desire is simply to show that training does not converge for particular finite problems , much simpler counterexamples can be constructed and would suffice : set all hidden unit weights to zero , for instance. \u201d was a crucial object in a heated debate among the authors when they were planning this research . The authors were facing the dilemma : 1 . Construct an example using ReLUs . Pros : These are the most commonly used activation functions . Cons : ... but it is trivial to deadlock them . Just put all the datapoints in the inactive region for example . 2.Use Sigmoids . Pros : the examples are highly nontrivial and have a fascinating geometry . Cons : The paper would get criticized for studying sigmoids that are so passe . The final decision was to construct interesting ReLU based examples . As much as the authors fully agree that it is extremely easy to construct a trivial example of a situation with no learning occurring , especially when using ReLUs , they believe that there is a value in constructing nontrivial ones , and that the examples presented fall into that category . In particular \u201c all weights set to zero \u201d example is in the authors \u2019 opinion a less interesting than the ones constructed in the paper , because it is a zero measure ( and even a high-codimension isolated set ) in a parameter space , and in most cases an unstable singularity of a training process . Regarding Figure 2 , and the number of linear regions on which the data lies by scaling down each examples : The code used to produce those results used an initialization where biases were non zero ( which is the default initialization provided by torch ) , but sampled from the same Gaussian as the weights . Hence scaling down the images has the desired effect . Also , since the Gaussian from which we sample the biases has mean 0 , the region on which all the data points end up is not the one where all hidden units are inactive ( i.e.the biases are not large negative numbers ) . The material in the paper has been reorganized , to improve the presentation , reflecting the valuable feedback the authors received from the reviewers and readers ."}, "2": {"review_id": "Syoiqwcxx-2", "review_text": "The paper studies some special cases of neural networks and datasets where optimization fails. Most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics. This reduces the practical relevance of the analysis. The experiment \"bad initialization on MNIST\" shows that for very negative biases or weights drawn from a non-centered distribution, all ReLU activations are \"off\" for all data points, and thus, optimization is prevented. This never occurs in practice, because using proper initialization heuristics avoid these cases. The \"jellyfish\" dataset constructed by the authors is demonstrated to be difficult to fit by a small model. However, the size/depth of the considered model is unsuitable for this problem. Proposition 4 assumes that we can choose the mean from which the weight parameters are initialized. This is typically not the case in practice as most initialization heuristics draw weight parameters from a distribution with mean 0. Proposition 5 considers infinitely deep ReLU networks. Very deep networks would however preferably be of type ResNet.", "rating": "3: Clear rejection", "reply_text": "The goal of this paper is to bridge the gap between the theoretical research and practical experience . On one hand a tremendous effort was made to come up with successful recipes for hyperparameter selection , initialization etc . The point made by the reviewer is 100 % valid - yes , they work ! They are the very founding stone of the development and the staggering success of deep learning . Yet the very same time - they are to a big extent absent in the universe of theoretical research . We do not see them in the assumptions of the results trying to prove something about why deep learning works as well as it does . We are not by any means trying to show that deep learning is not working . On the contrary . What we are trying to say is : if one strays from the path laid out by the good practices and experience of thousands of successful experiments , things break badly on really simple examples . Thus , it is important to narrow the gap between the theory and practice , in particular , for the theoretical research to embrace more closely what is known empirically to the practitioners . We thought that ICLR , a forum bringing together both more practically-oriented and more theoretically focused communities was a good place to make this observation . The material in the paper has been reorganized , to improve the presentation , reflecting the valuable feedback the authors received from the reviewers and readers ."}}