{"year": "2019", "forum": "Syfz6sC9tQ", "title": "Generative Feature Matching Networks", "decision": "Reject", "meta_review": "The paper proposes a method of training implicit generative models based on moment matching in the feature spaces of pre-trained feature extractors, derived from autoencoders or classifiers. The authors also propose a trick for tracking the moving averages by appealing to the Adam optimizer and deriving updates based on the implied loss function of a moving average update. \n\nIt was generally agreed that the paper was well written and easy to follow, that empirical results were good, but that the novelty is relatively low. Generative models have been built out of pre-trained classifiers before (e.g. generative plug & play networks), feature matching losses for generator networks have been proposed before (e.g. Salimans et al, 2016).  The contribution here is mainly the  extensive empirical analysis plus the AMA trick.\n\nAfter receiving exclusively confidence score 3 reviews, I sought the opinion of a 4th reviewer, an expert on GANs and GAN-like generative models. Their remaining sticking points, after a rapid rebuttal, are with possible degeneracies in the loss function and class-level information leakage from pre-trained classifiers, making these results are not properly \"unconditional\". The authors rebutted this by suggesting that unlike Salimans et al (2016), there is no signal backpropagated from the label layer, but I find this particularly unconvincing: the objective in that work maximizes a \"none-of-the-above\" class (and thus minimizes *all* classes). The gradient backpropagated to the generator is uninformative about which particular class a sample should imitate, but the features learned by the discriminator needing to discriminate between classes shape those gradients in a particular way all the same, and the result is samples that look like distinct CIFAR classes. In the same way, the gradients used to train GFMN are \"shaped\" by particular class-discriminative features when trained against a classifier feature extractor.\n\nFrom my own perspective, while there is no theory presented to support why this method is a good idea (why matching arbitrary features unconnected with the generative objective should lead to good results), the idea of optimizing a moment matching objective in classifier feature space is rather obvious, and it is unsurprising that with enough \"elbow grease\" it can be made to work. The Adam moving average trick is interesting but a deeper analysis and ablation of why this works would have helped convince the reader that it is principled. \n\nThis paper was very much on the borderline. Aside from quibbles over the fairness of comparisons above, I was forced to ask myself whether I could imagine that this would be a widely read, influential, and frequently cited piece of work. I believe that the carefully done empirical investigation has its merits, but that the core ideas are rather obvious and the added novelty of a poorly understood stabilized moving average is not enough to warrant acceptance.", "reviews": [{"review_id": "Syfz6sC9tQ-0", "review_text": "This paper consists of two contributions: (1) using a fixed pre-trained network as a discriminator in feature matching loss ((Salimans et al., 2016). Since it's fixed there is no GAN-like training procedure. (2) Using \"ADAM\"-moving average to improve the convergency for the feature matching loss. The paper is well written and easy to follow but it lack of some intuition for the proposed approach. There also some typo, e.g. \"quiet\" -> quite. Overall, it's a combination of several published method so I would expect a strong performance/analysis on the experimental session. Detailed Comment: For contribution (1): The proposed method is very similar to (Li et al. (2015)) as the author pointed out in related work besides this work map to data space directly. Is there any intuition why this is better? The proposed loss (the same as (Salimans et al., 2016)) only try to matching first-order momentum. So I assume it is insensitive to higher-order statistics. Does it less successful at producing samples with high visual fidelity? For contribution (2): \"one would need big mini-batches which would result in slowing down the training.\" why larger batch slowing down the training? Is there any qualitative results? Based recent paper e.g. big gan, it seem the model can benefit a lot from larger batch. In the meanwhile, even larger batch make it slower to converge, it can improve throughput. Again, can the author provide some intuition for these modification? It's also unclear to me what is ADAM(). Better to link some equation to the original paper or simply write down the formulation and give some explanation on it. For experiments: I'm not an expert to interpret experimental results for image generation. But overall, the results seems not very impressive. Given the best results is using ImageNet as a classifier, I think it should compare with some semi-supervised image generation paper. For example, for CIFAR results, it seems worse than (Warde-Farley & Bengio, 2017), Table 1, semi-supervised case. If we compare unsupervised case (autoencoder), it also seems a lot worse. Appendix A.8 is very interesting / important to apply pre-trained network in GAN framework. However, it only say failed to train without any explanation. I think even it just comparable with GAN, it is interesting if there is no mode collapsing and easy to train. However, it has no proper imagenet results (it has a subset, but only some generated image shows here). In summary, this paper provide some interesting perspectives. However, the main algorithms are very similar to some existing methods, more discussion could be used to compare with the existing literature and clarify the novelty of the current paper. The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach. I tend to give a weak reject or reject for this paper. --- Update post in AC discussion session. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Regarding experiments : Rev : \u201c I think even it just comparable with GAN , it is interesting if there is no mode collapsing and easy to train . However , it has no proper imagenet results ( it has a subset , but only some generated image shows here ) . \u201d \u201c The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach. \u201d We compare our results with Miyato ( ICLR 2018 ) , Bikowski ( ICLR 2018 ) and Ravuri et al . ( ICML 2018 ) which are very strong up-to-date baselines ( all published in 2018 ) and were the state-of-the-art results by the time we submitted the paper . Regarding ImageNet results , note that in this paper we do not propose to perform conditional generation . All ( very recent ) papers reporting IS/FID for ImageNet perform conditional generation . Our ImageNet results should be compared with ( Ravuri et al. , 2018 ) for the Daisy portion , and ( Salimans et.al , 2016 ; Zhao et al. , 2017 ) for the ImageNet dogs portion . Again , it is not fair to compare our results with the ones from large-scale experiments or with systems that explicitly model conditional generation . Note that the focus of this paper is to demonstrate that we can train effective generative models without adversarial training by employing frozen pretrained neural networks . We selected benchmarks that have been used for the last three/four years by the community on gen. models : CIFAR10 , CelebA , MNIST , STL10 . We performed an extensive number of experiments , reported quantitative results using two metrics ( IS and FID ) and systematically assessed multiple aspects of the proposed approach : ( 1 ) We checked the advantage of AMA vs MA ; ( 2 ) demonstrated the correlation of loss vs. image quality ; ( 3 ) evaluated different methods for pretraining the feature extractor ( autoencoding , classification ) ; ( 4 ) checked different architectures for the feat . extractor ( DCGAN , VGG19 , Resnet18 ) ; ( 5 ) assessed the impact of the number of features/layers used ; ( 6 ) evaluated in-domain and cross domain feature extractors ; ( 7 ) tested the benefit of initializing the generator ; ( 8 ) evaluated the joint use of multiple feature extractors ( VGG19 + Resnet18 ) for training the same generator ; ( 9 ) performed experiments with WGAN-GP initialized with VGG19/Resnet18 ; ( 10 ) presented results for different portions of ImageNet ; ( 11 ) presented visual comparison of images generated between GFMN and GMMN ; ( 12 ) compared our results with state-of-the-art methods . Moreover , the improved Sec.4.2.3 now contains even more experiments and discussions regarding the advantages of using AMA . Finally , we would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions . Moreover , as we present a method that provides evidence for the power of pretrained DCNN representations for learning generative models , we believe our work is a perfect fit for ICLR and is of great interest for its community . Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating ."}, {"review_id": "Syfz6sC9tQ-1", "review_text": "This paper proposes to learn implicit generative models by a feature matching objective which forces the generator to produce samples that match the means of the data distribution in some fixed feature space, focusing on image generation and feature spaces given by pre-trained image classifiers. On the positive side, the paper is well-written and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets. The method is nice in that, unlike GANs and the stability issues that come with them, it minimizes a single loss and requires only a single module, the generator. On the other hand, the general applicability of the method is unclear, the novelty is somewhat limited, and the evaluation is missing a few important baselines. In detail: 1) The proposed objective was used as a GAN auxiliary objective in [Salimans et al., 2016] and further explored in [Warde-Farley & Bengio, 2017]. The novel bit here is that the proposed objective doesn\u2019t include the standard GAN term (so no need for an adversarially-optimized discriminator), and the feature extractor is a fixed pre-trained classifier or encoder from an auto-encoder (rather than a discriminator). 2) The method only forces the generator\u2019s sample distribution to match the first moment (the mean) of the data distribution. While the paper shows that this can result in a generator that produces reasonably good samples in practice, it seems like this may have happened due to a \u201clucky\u201d artifact of the chosen pre-trained feature extractors. For example, a degenerate generator that produces a single image whose features exactly match the mean would be a global optimum under this objective, equally good as a generator that exactly matches the data distribution. Perhaps no such image exists for the chosen pre-trained classifiers, but it\u2019s nonetheless concerning that the objective does nothing to prevent this type of behavior in the general case. (This is similar to the mode collapse problem that often occurs with GAN training in practice, but at least a GAN generator is required to exactly match the full data distribution to achieve the global optimum of that objective.) 3) It\u2019s unclear why the proposed ADAM-based Moving Average (AMA) updates are appropriate for estimate the mean features of the data distribution. Namely, unlike EMA updates, it\u2019s not clear that this is an unbiased estimator (I suspect it\u2019s not); i.e. that the expectation of the resulting estimates is actually the true mean of the dataset features. It\u2019s therefore not clear whether the stated objective is actually what\u2019s being optimized when these AMA updates are used. 4) Related to (3), an important baseline which is not discussed is the true fixed mean of the dataset distribution. In Sec. 2.4 (on AMA) it\u2019s claimed that \u201cone would need large mini-batches for generating a good estimate of the mean features...this can easily result in memory issues\u201d, but this is not true: one could trivially compute the full exact dataset mean of these fixed features by accumulating a sum over the dataset (e.g., one image a time, with minibatch size 1) and then dividing the result by the number of images in the dataset. Without this baseline, I can\u2019t rule out that the method only works due to its reliance on the stochasticity of the dataset mean estimates to avoid the behavior described in (2), or even the fact that the estimates are biased due to the use of ADAM as described in (3). 5) The best results in Table 3 rely on initializing G with the weights of a decoder pretrained for autoencoding. However, the performance of the decoder itself with no additional training from the GFMN objective is not reported, so it\u2019s possible that most of the result relies on *autoencoder* training rather than feature matching to get a good generator. This explanation seems especially plausible due to the fact that the learning rate is set to a miniscule value (5*10^-6 for ADAM, 1-2 orders of magnitude smaller than typical values). Without the generator pretraining, the next best CIFAR result is an Inception Score of 7.67, lower than the unsupervised result from [Warde-Farley & Bengio, 2017] of 7.72. 6) It is misleading to call the results based on ImageNet-pretrained models \u201cunconditional\u201d -- there is plenty of overlap in the supervision provided by the labeled images of the much larger ImageNet to CIFAR and other datasets explored here. This is especially true given that the reported metrics (Inception Score and FID) are themselves based on ImageNet-pretrained classifiers. If the results were instead compared to prior work on conditional generation (e.g. ProGAN (Karras et al., 2017), which reports CIFAR IS of 8.56), there would be a clear gap between these results and the state of the art. Overall, the current version of the paper needs additional experiments and clarifying discussion to address these issues. ======================================= REVISION Based on the authors' responses, I withdraw points 3-5 from my original review. Thanks to the authors for the additional experiments. On (3), I indeed misunderstood where the moving average was being applied; thanks for the correction. On (4), the additional experiment using the global mean features for real data convinces me that the method does not rely on the stochasticity of the estimates. (Though, given that the global mean works just as well, it seems like it would be more efficient and arguably cleaner to simply have that be the main method. But this isn't a major issue.) On (5), I misread the learning rate specified for \"using the autoencoder features\" as being the learning rate for autoencoder *pretraining*; thanks for the correction. The added results in Appendix 11 do show that the pretrained decoder on its own does not produce good samples. My biggest remaining concerns are with points (2) and (6) from my original review. On (2), I did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset. That being said, the paper shows that this doesn't tend to happen in practice with existing classifiers, which is an interesting empirical contribution. (It would be nice to also see ablation studies on this point, showing the results of training against features from single layers across the network.) On (6), I'm still unconvinced that making use of ImageNet classifiers isn't providing something like a conditional training signal, and that using such classifiers isn't a bit of an \"unfair advantage\" vs. other methods when the metrics themselves are based on an ImageNet classifier. I realize that ImageNet and CIFAR have different label sets, but most if not all of the CIFAR classes are nonetheless represented -- in a finer-grained way -- in ImageNet. If ImageNet and CIFAR were really completely unrelated, an ImageNet classifier could not be used as an evaluation metric for CIFAR generators. (And yes, I saw the CelebA results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old DCGAN results, I can't tell.) On the other hand, given that the approach relies on these classifiers, I don't have a good suggestion for how to control for this and make the comparison with prior work completely fair. Still, it would be nice to see acknowledgment and discussion of this caveat in a future revision of the paper. Overall, given that most of my concerns have been addressed with additional experiments and clarification, and that the paper is well-written and has some interesting results from its relatively simple approach, I've raised my rating to above acceptance threshold.", "rating": "6: Marginally above acceptance threshold", "reply_text": "6 ) On ImageNet vs. CIFAR10 ; conditional generation ; IS/FID metrics and experimental results : AUTHORS : The reviewer 's concerns are based on misconceptions . Below we list some facts that will help to rule out the reviewer 's concerns : ( a ) the label overlap between ImageNet and CIFAR10 is not direct at all . ImageNet uses fine grained labels , while CIFAR10 uses 10 labels only . For instance , while ImageNet has dozens of classes for dogs ( one for each breed ) , CIFAR10 groups all dog images in one single class ( see [ 3 ] , Appendix F ) . Labeling the data in different ways produce quite different classification problems . Moreover , the images in CIFAR10 are not a subset of ImageNet since they have different resolutions . It is completely safe to say that CIFAR10 is an out-of-domain dataset with regard to ImageNet ; ( b ) the reviewer is ignoring the fact that we also have good results for an extreme case of out-of-domain dataset : CelebA . There is no overlap between ImageNet and CelebA and the images from these two datasets are quite different . Nevertheless , the VGG19 classifier produces much better results than the autoencoder pretrained in CelebA . In the new Appendix 13 , Fig 14 , we show additional results that support this fact . These results are clear evidence that VGG19 classifiers is just a much better feature extractor than autoencoders , which is also the real reason for our boost in performance for CIFAR10 ( and not `` label overlapping '' ) . There is a long literature ( some cited in our paper ) that show the effectiveness of VGG19 ImageNet classifier as feature extractor ; ( c ) ImageNet classifiers , and in special VGG classifier , is the default choice for feature extraction in computer vision tasks . We are sure that all the reviewers would have complained about our work if we have not used ImageNet-based feature extractors ; ( d ) our use of VGG19/Resnet18 ImageNet classifiers have no impact in the used metrics for different reasons : ( d.1 ) IS and FID are computed using the default tensorflow Inception model trained with images of size 299x299 , while the classifier ( the feature extractor ) that we use in our experiments was trained using images of size 32x32 , as informed in Appendix A.3 . Our ImageNet VGG19 classifier has top-1 accuracy of 29.14 % while the Inception net has about top-1 accuracy of 79 % . In summary , our classifier is completely different in many crucial aspects when compared to the Inception classifier used to compute IS and IFD ; ( d.2 ) we use the classifier as a feature extractor only , no log likelihoods from the classifier is used in our objective function ; ( e ) we do not perform conditional generation . GAN-based methods that perform conditional generation use direct feedback from the labels in the form of log likelihoods from the discriminator ( using the k+1 trick from Salimans et.al 2016 ) or from an auxiliary classifier . In the contrary , our generator is trained with a loss function that performs feature matching only , there is no feedback in form of a log likelihood from the labeled data . Our generator is agnostic to the labels ( there is no one-hots concatenated to the noise ) and it is only distilling the knowledge of the multi-layer feature space , without explicitly taking advantage of any labeling that went to the training to this feature space ; ( f ) ProGAN ( Karras et al. , 2017 ) uses a generator architecture that contains residual connections and is deeper and more complex than the DCGAN-like architecture that we use in our CIFAR10 experiments . Their better performance is due mainly to the generator 's architecture and the tricks used to train such an architecture . It is unfair to compare our results with the ones of a method that uses a bigger and more complex generator . We were careful and fair in trying to put in Table 3 the results for very recent and relevant work that use generator architecture that are similar to ours . By the way , the trick of progressively growing the generator can also be applied in GFMN and would likely increase our performance . But this experiment is completely out of the scope of the current paper ; [ 3 ] Oliver et al.Realistic Evaluation of Deep Semi-Supervised Learning Algorithms . ArXiv 2018. https : //arxiv.org/pdf/1804.09170.pdf"}, {"review_id": "Syfz6sC9tQ-2", "review_text": "The paper proposes a non-adversarial feature matching generative model (GFMN). In feature matching GANs, the discriminator extract features that are employed by the generator to match the real data distribution. Through the experiments, the paper shows that the loss function is correlated with the generated image quality, and the same pretrained feature extractor (pre-trained on imagenet) can be employed across a variety of datasets. The paper also discusses the choice of pretrained network or autoencoder as the feature extractor. The paper also introduces an ADAM-based moving average. The paper compares the results with on CIFAR10 and STL10 with a variety of recent State-of-the-art approaches in terms on IS and FID. + The paper is well written and easy to follow. - However, there are some typos that should be addressed. Such as: \u201cThe decoder part of an AE consists exactly in an image generator \u201d \u201cOur proposed approach consists in training G by minimizing\u201d \u201cDifferent past work have shown\u201d -> has \u201cin Equation equation 1 by\u201d \u201chave also used\u201d better to use the present tense. + It suggests a non-adversarial approach to generate images using pre-trained networks. So the training is easier and the quality of the generated images, as well as the FID and IS, are still comparable to the state-of-the-art approaches. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive feedback . We have done a careful proof reading on the paper and addressed the typos pointed by the reviewer . Additionally , we have greatly improved and expanded sections 2.4 and 4.2.3 and added the new section 4.3 as well as two new appendices ( A.9 and A.10 ) . In the post destined to all the reviewers , we give more details about the main changes in the new version of the paper . We would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions . Moreover , as we present a method that evidence the power of pretrained DCNN representations for training generative models , we believe that our work is a perfect fit for ICLR and of big interest for its community . Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating ."}, {"review_id": "Syfz6sC9tQ-3", "review_text": "The paper introduces Generative Feature Matching Networks (GFMNs) which is a non-adversarial approach to train generative models based on feature matching. GFMN uses pretrained neural networks such as Autoencoders (AE) and Deep Convolutional Neural Networks (DCNN) to extract features. Equation (1) is the proposed loss function for the generator network. In order to avoid big mini-batches, the GFMN performs feature matching with ADAM moving average.The paper validates its proposed approach with several experiments applied on benchmark datasets such as CIFAR10 and ImageNet. The paper is well-written and straight-forward to follow. The problem is well-motivated by fully discussing the literature and the proposed method is clearly introduced. The method is then validated using several different experiments. Typos: ** Page 1 -- Paragraph 3 -- Line 8: \"(2) mode collapsing in not an issue\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive feedback . We have done a careful proof reading on the paper and addressed the typos pointed by the reviewer . Additionally , we have greatly improved and expanded sections 2.4 and 4.2.3 and added the new section 4.3 as well as two new appendices ( A.9 and A.10 ) . In the post destined to all the reviewers , we give more details about the main changes in the new version of the paper . We would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions . Moreover , as we present a method that evidence the power of pretrained DCNN representations for training generative models , we believe that our work is a perfect fit for ICLR and of big interest for its community . Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating ."}], "0": {"review_id": "Syfz6sC9tQ-0", "review_text": "This paper consists of two contributions: (1) using a fixed pre-trained network as a discriminator in feature matching loss ((Salimans et al., 2016). Since it's fixed there is no GAN-like training procedure. (2) Using \"ADAM\"-moving average to improve the convergency for the feature matching loss. The paper is well written and easy to follow but it lack of some intuition for the proposed approach. There also some typo, e.g. \"quiet\" -> quite. Overall, it's a combination of several published method so I would expect a strong performance/analysis on the experimental session. Detailed Comment: For contribution (1): The proposed method is very similar to (Li et al. (2015)) as the author pointed out in related work besides this work map to data space directly. Is there any intuition why this is better? The proposed loss (the same as (Salimans et al., 2016)) only try to matching first-order momentum. So I assume it is insensitive to higher-order statistics. Does it less successful at producing samples with high visual fidelity? For contribution (2): \"one would need big mini-batches which would result in slowing down the training.\" why larger batch slowing down the training? Is there any qualitative results? Based recent paper e.g. big gan, it seem the model can benefit a lot from larger batch. In the meanwhile, even larger batch make it slower to converge, it can improve throughput. Again, can the author provide some intuition for these modification? It's also unclear to me what is ADAM(). Better to link some equation to the original paper or simply write down the formulation and give some explanation on it. For experiments: I'm not an expert to interpret experimental results for image generation. But overall, the results seems not very impressive. Given the best results is using ImageNet as a classifier, I think it should compare with some semi-supervised image generation paper. For example, for CIFAR results, it seems worse than (Warde-Farley & Bengio, 2017), Table 1, semi-supervised case. If we compare unsupervised case (autoencoder), it also seems a lot worse. Appendix A.8 is very interesting / important to apply pre-trained network in GAN framework. However, it only say failed to train without any explanation. I think even it just comparable with GAN, it is interesting if there is no mode collapsing and easy to train. However, it has no proper imagenet results (it has a subset, but only some generated image shows here). In summary, this paper provide some interesting perspectives. However, the main algorithms are very similar to some existing methods, more discussion could be used to compare with the existing literature and clarify the novelty of the current paper. The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach. I tend to give a weak reject or reject for this paper. --- Update post in AC discussion session. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Regarding experiments : Rev : \u201c I think even it just comparable with GAN , it is interesting if there is no mode collapsing and easy to train . However , it has no proper imagenet results ( it has a subset , but only some generated image shows here ) . \u201d \u201c The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach. \u201d We compare our results with Miyato ( ICLR 2018 ) , Bikowski ( ICLR 2018 ) and Ravuri et al . ( ICML 2018 ) which are very strong up-to-date baselines ( all published in 2018 ) and were the state-of-the-art results by the time we submitted the paper . Regarding ImageNet results , note that in this paper we do not propose to perform conditional generation . All ( very recent ) papers reporting IS/FID for ImageNet perform conditional generation . Our ImageNet results should be compared with ( Ravuri et al. , 2018 ) for the Daisy portion , and ( Salimans et.al , 2016 ; Zhao et al. , 2017 ) for the ImageNet dogs portion . Again , it is not fair to compare our results with the ones from large-scale experiments or with systems that explicitly model conditional generation . Note that the focus of this paper is to demonstrate that we can train effective generative models without adversarial training by employing frozen pretrained neural networks . We selected benchmarks that have been used for the last three/four years by the community on gen. models : CIFAR10 , CelebA , MNIST , STL10 . We performed an extensive number of experiments , reported quantitative results using two metrics ( IS and FID ) and systematically assessed multiple aspects of the proposed approach : ( 1 ) We checked the advantage of AMA vs MA ; ( 2 ) demonstrated the correlation of loss vs. image quality ; ( 3 ) evaluated different methods for pretraining the feature extractor ( autoencoding , classification ) ; ( 4 ) checked different architectures for the feat . extractor ( DCGAN , VGG19 , Resnet18 ) ; ( 5 ) assessed the impact of the number of features/layers used ; ( 6 ) evaluated in-domain and cross domain feature extractors ; ( 7 ) tested the benefit of initializing the generator ; ( 8 ) evaluated the joint use of multiple feature extractors ( VGG19 + Resnet18 ) for training the same generator ; ( 9 ) performed experiments with WGAN-GP initialized with VGG19/Resnet18 ; ( 10 ) presented results for different portions of ImageNet ; ( 11 ) presented visual comparison of images generated between GFMN and GMMN ; ( 12 ) compared our results with state-of-the-art methods . Moreover , the improved Sec.4.2.3 now contains even more experiments and discussions regarding the advantages of using AMA . Finally , we would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions . Moreover , as we present a method that provides evidence for the power of pretrained DCNN representations for learning generative models , we believe our work is a perfect fit for ICLR and is of great interest for its community . Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating ."}, "1": {"review_id": "Syfz6sC9tQ-1", "review_text": "This paper proposes to learn implicit generative models by a feature matching objective which forces the generator to produce samples that match the means of the data distribution in some fixed feature space, focusing on image generation and feature spaces given by pre-trained image classifiers. On the positive side, the paper is well-written and easy to follow, the experiments are clearly described, and the evaluation shows the method can achieve good results on a few datasets. The method is nice in that, unlike GANs and the stability issues that come with them, it minimizes a single loss and requires only a single module, the generator. On the other hand, the general applicability of the method is unclear, the novelty is somewhat limited, and the evaluation is missing a few important baselines. In detail: 1) The proposed objective was used as a GAN auxiliary objective in [Salimans et al., 2016] and further explored in [Warde-Farley & Bengio, 2017]. The novel bit here is that the proposed objective doesn\u2019t include the standard GAN term (so no need for an adversarially-optimized discriminator), and the feature extractor is a fixed pre-trained classifier or encoder from an auto-encoder (rather than a discriminator). 2) The method only forces the generator\u2019s sample distribution to match the first moment (the mean) of the data distribution. While the paper shows that this can result in a generator that produces reasonably good samples in practice, it seems like this may have happened due to a \u201clucky\u201d artifact of the chosen pre-trained feature extractors. For example, a degenerate generator that produces a single image whose features exactly match the mean would be a global optimum under this objective, equally good as a generator that exactly matches the data distribution. Perhaps no such image exists for the chosen pre-trained classifiers, but it\u2019s nonetheless concerning that the objective does nothing to prevent this type of behavior in the general case. (This is similar to the mode collapse problem that often occurs with GAN training in practice, but at least a GAN generator is required to exactly match the full data distribution to achieve the global optimum of that objective.) 3) It\u2019s unclear why the proposed ADAM-based Moving Average (AMA) updates are appropriate for estimate the mean features of the data distribution. Namely, unlike EMA updates, it\u2019s not clear that this is an unbiased estimator (I suspect it\u2019s not); i.e. that the expectation of the resulting estimates is actually the true mean of the dataset features. It\u2019s therefore not clear whether the stated objective is actually what\u2019s being optimized when these AMA updates are used. 4) Related to (3), an important baseline which is not discussed is the true fixed mean of the dataset distribution. In Sec. 2.4 (on AMA) it\u2019s claimed that \u201cone would need large mini-batches for generating a good estimate of the mean features...this can easily result in memory issues\u201d, but this is not true: one could trivially compute the full exact dataset mean of these fixed features by accumulating a sum over the dataset (e.g., one image a time, with minibatch size 1) and then dividing the result by the number of images in the dataset. Without this baseline, I can\u2019t rule out that the method only works due to its reliance on the stochasticity of the dataset mean estimates to avoid the behavior described in (2), or even the fact that the estimates are biased due to the use of ADAM as described in (3). 5) The best results in Table 3 rely on initializing G with the weights of a decoder pretrained for autoencoding. However, the performance of the decoder itself with no additional training from the GFMN objective is not reported, so it\u2019s possible that most of the result relies on *autoencoder* training rather than feature matching to get a good generator. This explanation seems especially plausible due to the fact that the learning rate is set to a miniscule value (5*10^-6 for ADAM, 1-2 orders of magnitude smaller than typical values). Without the generator pretraining, the next best CIFAR result is an Inception Score of 7.67, lower than the unsupervised result from [Warde-Farley & Bengio, 2017] of 7.72. 6) It is misleading to call the results based on ImageNet-pretrained models \u201cunconditional\u201d -- there is plenty of overlap in the supervision provided by the labeled images of the much larger ImageNet to CIFAR and other datasets explored here. This is especially true given that the reported metrics (Inception Score and FID) are themselves based on ImageNet-pretrained classifiers. If the results were instead compared to prior work on conditional generation (e.g. ProGAN (Karras et al., 2017), which reports CIFAR IS of 8.56), there would be a clear gap between these results and the state of the art. Overall, the current version of the paper needs additional experiments and clarifying discussion to address these issues. ======================================= REVISION Based on the authors' responses, I withdraw points 3-5 from my original review. Thanks to the authors for the additional experiments. On (3), I indeed misunderstood where the moving average was being applied; thanks for the correction. On (4), the additional experiment using the global mean features for real data convinces me that the method does not rely on the stochasticity of the estimates. (Though, given that the global mean works just as well, it seems like it would be more efficient and arguably cleaner to simply have that be the main method. But this isn't a major issue.) On (5), I misread the learning rate specified for \"using the autoencoder features\" as being the learning rate for autoencoder *pretraining*; thanks for the correction. The added results in Appendix 11 do show that the pretrained decoder on its own does not produce good samples. My biggest remaining concerns are with points (2) and (6) from my original review. On (2), I did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset. That being said, the paper shows that this doesn't tend to happen in practice with existing classifiers, which is an interesting empirical contribution. (It would be nice to also see ablation studies on this point, showing the results of training against features from single layers across the network.) On (6), I'm still unconvinced that making use of ImageNet classifiers isn't providing something like a conditional training signal, and that using such classifiers isn't a bit of an \"unfair advantage\" vs. other methods when the metrics themselves are based on an ImageNet classifier. I realize that ImageNet and CIFAR have different label sets, but most if not all of the CIFAR classes are nonetheless represented -- in a finer-grained way -- in ImageNet. If ImageNet and CIFAR were really completely unrelated, an ImageNet classifier could not be used as an evaluation metric for CIFAR generators. (And yes, I saw the CelebA results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old DCGAN results, I can't tell.) On the other hand, given that the approach relies on these classifiers, I don't have a good suggestion for how to control for this and make the comparison with prior work completely fair. Still, it would be nice to see acknowledgment and discussion of this caveat in a future revision of the paper. Overall, given that most of my concerns have been addressed with additional experiments and clarification, and that the paper is well-written and has some interesting results from its relatively simple approach, I've raised my rating to above acceptance threshold.", "rating": "6: Marginally above acceptance threshold", "reply_text": "6 ) On ImageNet vs. CIFAR10 ; conditional generation ; IS/FID metrics and experimental results : AUTHORS : The reviewer 's concerns are based on misconceptions . Below we list some facts that will help to rule out the reviewer 's concerns : ( a ) the label overlap between ImageNet and CIFAR10 is not direct at all . ImageNet uses fine grained labels , while CIFAR10 uses 10 labels only . For instance , while ImageNet has dozens of classes for dogs ( one for each breed ) , CIFAR10 groups all dog images in one single class ( see [ 3 ] , Appendix F ) . Labeling the data in different ways produce quite different classification problems . Moreover , the images in CIFAR10 are not a subset of ImageNet since they have different resolutions . It is completely safe to say that CIFAR10 is an out-of-domain dataset with regard to ImageNet ; ( b ) the reviewer is ignoring the fact that we also have good results for an extreme case of out-of-domain dataset : CelebA . There is no overlap between ImageNet and CelebA and the images from these two datasets are quite different . Nevertheless , the VGG19 classifier produces much better results than the autoencoder pretrained in CelebA . In the new Appendix 13 , Fig 14 , we show additional results that support this fact . These results are clear evidence that VGG19 classifiers is just a much better feature extractor than autoencoders , which is also the real reason for our boost in performance for CIFAR10 ( and not `` label overlapping '' ) . There is a long literature ( some cited in our paper ) that show the effectiveness of VGG19 ImageNet classifier as feature extractor ; ( c ) ImageNet classifiers , and in special VGG classifier , is the default choice for feature extraction in computer vision tasks . We are sure that all the reviewers would have complained about our work if we have not used ImageNet-based feature extractors ; ( d ) our use of VGG19/Resnet18 ImageNet classifiers have no impact in the used metrics for different reasons : ( d.1 ) IS and FID are computed using the default tensorflow Inception model trained with images of size 299x299 , while the classifier ( the feature extractor ) that we use in our experiments was trained using images of size 32x32 , as informed in Appendix A.3 . Our ImageNet VGG19 classifier has top-1 accuracy of 29.14 % while the Inception net has about top-1 accuracy of 79 % . In summary , our classifier is completely different in many crucial aspects when compared to the Inception classifier used to compute IS and IFD ; ( d.2 ) we use the classifier as a feature extractor only , no log likelihoods from the classifier is used in our objective function ; ( e ) we do not perform conditional generation . GAN-based methods that perform conditional generation use direct feedback from the labels in the form of log likelihoods from the discriminator ( using the k+1 trick from Salimans et.al 2016 ) or from an auxiliary classifier . In the contrary , our generator is trained with a loss function that performs feature matching only , there is no feedback in form of a log likelihood from the labeled data . Our generator is agnostic to the labels ( there is no one-hots concatenated to the noise ) and it is only distilling the knowledge of the multi-layer feature space , without explicitly taking advantage of any labeling that went to the training to this feature space ; ( f ) ProGAN ( Karras et al. , 2017 ) uses a generator architecture that contains residual connections and is deeper and more complex than the DCGAN-like architecture that we use in our CIFAR10 experiments . Their better performance is due mainly to the generator 's architecture and the tricks used to train such an architecture . It is unfair to compare our results with the ones of a method that uses a bigger and more complex generator . We were careful and fair in trying to put in Table 3 the results for very recent and relevant work that use generator architecture that are similar to ours . By the way , the trick of progressively growing the generator can also be applied in GFMN and would likely increase our performance . But this experiment is completely out of the scope of the current paper ; [ 3 ] Oliver et al.Realistic Evaluation of Deep Semi-Supervised Learning Algorithms . ArXiv 2018. https : //arxiv.org/pdf/1804.09170.pdf"}, "2": {"review_id": "Syfz6sC9tQ-2", "review_text": "The paper proposes a non-adversarial feature matching generative model (GFMN). In feature matching GANs, the discriminator extract features that are employed by the generator to match the real data distribution. Through the experiments, the paper shows that the loss function is correlated with the generated image quality, and the same pretrained feature extractor (pre-trained on imagenet) can be employed across a variety of datasets. The paper also discusses the choice of pretrained network or autoencoder as the feature extractor. The paper also introduces an ADAM-based moving average. The paper compares the results with on CIFAR10 and STL10 with a variety of recent State-of-the-art approaches in terms on IS and FID. + The paper is well written and easy to follow. - However, there are some typos that should be addressed. Such as: \u201cThe decoder part of an AE consists exactly in an image generator \u201d \u201cOur proposed approach consists in training G by minimizing\u201d \u201cDifferent past work have shown\u201d -> has \u201cin Equation equation 1 by\u201d \u201chave also used\u201d better to use the present tense. + It suggests a non-adversarial approach to generate images using pre-trained networks. So the training is easier and the quality of the generated images, as well as the FID and IS, are still comparable to the state-of-the-art approaches. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive feedback . We have done a careful proof reading on the paper and addressed the typos pointed by the reviewer . Additionally , we have greatly improved and expanded sections 2.4 and 4.2.3 and added the new section 4.3 as well as two new appendices ( A.9 and A.10 ) . In the post destined to all the reviewers , we give more details about the main changes in the new version of the paper . We would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions . Moreover , as we present a method that evidence the power of pretrained DCNN representations for training generative models , we believe that our work is a perfect fit for ICLR and of big interest for its community . Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating ."}, "3": {"review_id": "Syfz6sC9tQ-3", "review_text": "The paper introduces Generative Feature Matching Networks (GFMNs) which is a non-adversarial approach to train generative models based on feature matching. GFMN uses pretrained neural networks such as Autoencoders (AE) and Deep Convolutional Neural Networks (DCNN) to extract features. Equation (1) is the proposed loss function for the generator network. In order to avoid big mini-batches, the GFMN performs feature matching with ADAM moving average.The paper validates its proposed approach with several experiments applied on benchmark datasets such as CIFAR10 and ImageNet. The paper is well-written and straight-forward to follow. The problem is well-motivated by fully discussing the literature and the proposed method is clearly introduced. The method is then validated using several different experiments. Typos: ** Page 1 -- Paragraph 3 -- Line 8: \"(2) mode collapsing in not an issue\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive feedback . We have done a careful proof reading on the paper and addressed the typos pointed by the reviewer . Additionally , we have greatly improved and expanded sections 2.4 and 4.2.3 and added the new section 4.3 as well as two new appendices ( A.9 and A.10 ) . In the post destined to all the reviewers , we give more details about the main changes in the new version of the paper . We would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions . Moreover , as we present a method that evidence the power of pretrained DCNN representations for training generative models , we believe that our work is a perfect fit for ICLR and of big interest for its community . Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating ."}}