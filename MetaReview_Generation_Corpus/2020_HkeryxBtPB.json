{"year": "2020", "forum": "HkeryxBtPB", "title": "MMA Training: Direct Input Space Margin Maximization through Adversarial Training", "decision": "Accept (Poster)", "meta_review": "This work presents a new loss function that combines the usual cross-entropy term with a margin maximization term applied to the correctly classified examples. There have been a lot of recent ideas on how to incorporate margin into the training process for deep learning. The paper differs from those in the way that it computes margin. The paper shows that training with the proposed max margin loss results in robustness against some adversarial attacks.\nThere were initially some concerns about baseline comparisons; one of the reviewers requesting comparison against TRADES, and the other making comments on CW-L2. In response, authors ran additional experiments and listed those in their rebuttal and in the revised draft. This led some reviewers to raise their initial scores. At the end, majority of reviewers recommended accept. Alongside with them, I find extensions of classic large margin ideas to deep learning settings (when margin is not necessarily defined at the output layer) an important research direction for constructing deep models that are robust and can generalize. ", "reviews": [{"review_id": "HkeryxBtPB-0", "review_text": "Summary: This paper proposes an adaptive margin-based adversarial training (eg. MMA) approach to train robust DNNs by maximizing the shortest margin of inputs to the decision boundary. Theoretical analyses have been provided to understand the connection between robust optimization and margin maximization. The main difference between the proposed approach to standard adversarial training is the adaptive selection of the perturbation bound \\epsilon. This makes adversarial training with large perturbation possible, which was previously unachievable by standard adversarial training (Madry et al.) Empirical results match the theoretical analysis. Pros: 1. The margin maximization idea has been well-explained, both intuitively and theoretically. 2. Interesting theoretical analyses and understandings of robust optimization from the margin perspective. 3. Clear advantage of MMA over standard adversarial training under large perturbations. 8. The proposed PGDLS is very interesting, actually quite good and much simpler, without extra computational cost. A similar idea was discussed in paper [3], where they gradually increase the convergence quality of training adversarial examples, and show the convergence guarantee of \"dynamic training\". 9. The gradient-free SPSA helps confirm the improvements of MMA under large perturbations are not a side effect of gradient masking. Cons: 1. The idea of \"shortest successful perturbation\" appears like a type of weak training attack, looking for minimum perturbations to just cross the classification boundary, like deepfool [1] or confidence 0 CW-L2 attack [2]. 2. The margin d_\\theta in Equation (1)/(2)/... defined on which norm? L_\\infty or L2 norm? I assume it's the infinity norm. In Theorem 2.1, the \\delta^{*} = argmin ||\\delta||, is a norm? Looks like a mistake. 3. The minimum margin \\delta^{*} is a bit confusing, is it used in maximization or just in the outer minimization? The last paragraph of page 3, L(\\theta, \\delta) or L(\\delta, \\theta), consistency check? 4. Why do we need the \"gradients of margins to model parameters\" analysis from Proposition 2.1 to remark 2.2? Given the \\delta^{*} found in the inner maximization (eg. attacking) process (step 1), minimizing the loss over this \\delta^{*} seems quite a straightforward step 2. Why don't go directly from Theorem 2.1 to Proposition 2.4, since the extensions from LM loss to SLM and CE loss via Proposition 2.3 -> Proposition 2.4., just proves that the standard classification loss CE can already maximize the margin given \\delta^{*}? 5. Section 4 Experiments. The experimental settings are not clear, and are not standard. What CIFAR10-\\ell_{\\infty} means: is it the CW-L2 attack, used for training, or for testing? How the test attacks were generated, the m and N, are confusing: for each test image, you have 260 samples for CIFAR10 (which means 260*10K in total), or just 260 in total (this is far less than a typical setting causing inaccurate results)? How are the d_max determined, and what are their relationship to standard \\epsilon? How the m models were trained? 6. Fairness of the comparison. Since MMA changes \\epsilon, how to fairly compare the robustness to standard epsilon bounded adversarial training is not discussed. Is it fair to compare MMA-3.0 vs PGD-2.5, since they have different epsilon? Why robustness was not tested against strong, unrestricted attacks like CW-L2 [2], and report the average L2 perturbations required to completely break the robustly trained model (and show MMA-trained models enforce large perturbations to succeed)? 7. Significance of the results. Normally, \\epsilon_{infty} > 16/255 will cause perceptual difference. Under 16/255, PGD-8/16, PGDLS-8/16 are still the best. At this level, it is quite a surprise that MMA does not improve robustness, although it does increase clean accuracy. This means the theoretical analysis only stand under certain circumstances. I don't think the optimal \\epsilon < margin can explain this, as it does not make sense to me the margin can be larger than 16/255. On the other hand, I thought the theoretical parts were discussing the ROBUSTNESS, not the CLEAN ACCURACY? But it turns out the MMA benefits a lot the clean accuracy? Why do we need robustness against large \\infty perturbations, this definitely deserves more discussion, as when perturbation goes large, the L2 attack (eg. CW-L2) makes more sense than PGD-\\infty. [1] Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. \"Deepfool: a simple and accurate method to fool deep neural networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [2] Carlini, Nicholas, and David Wagner. \"Towards evaluating the robustness of neural networks.\" 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017. [3] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" International Conference on Machine Learning. 2019. ============ My rating stays the same after reading through all the responses. I appreciate the authors' clarification on the notations and experimental settings. My 8/9 are positive points. My major concern is still the effectiveness of the proposed approach, and fairness of the comparison. It seems that MMA only works when the perturbation is large, which often larger than the \\epsilon used to train baseline adversarial training methods such as Trades. The authors seem have misunderstood my request for CWL2 results, I was just suggesting that the average L2 perturbation of CWL2 attack can be used as a fair test measure for robustness, instead of the AvgRobAcc used in the paper, and the susceptible comparison between MMA-12 vs PGD-8, or MMA-32 vs Trades. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your detailed reviews and also valuing our contributions . We would like to carify with R3 ( and other readers ) that many items under `` Cons '' are not strictly `` disadvantages '' of our method . Most of them seem to be clarification questions , especially that item 8 and 9 seem to be comments with positive sentiment . Please let us know if any of our answers does not resolve the confusion , and we are happy to further elaborate . > > 1 . `` shortest successful perturbation '' appears like a type of weak training attack , and similar to deepfool [ 1 ] or confidence 0 CW-L2 attack [ 2 ] . You are right that the `` shortest successful perturbation '' is similar to deepfool and CW-L2 . More precisely , deepfool , CW-L2 and our proposed AN-PGD are all algorithms for approximating the `` shortest successful perturbation '' , as we mentioned in Section 2.3 `` Other attacks that can serve a similar purpose can also fit into our MMA training framework '' . Specifically , the deepfool attack is not strong enough ( e.g.as shown in Rony et al.2019 ) , probably due to it only uses first order approximation to find $ \\delta^ * $ , and therefore is not suitable for MMA training . On the other hand , CW-L2 is likely strong enough , but too expensive to compute during training . Moreover , while the `` shortest successful perturbation '' , $ \\delta^ * $ , is a `` weak '' training attack if measured in the adversarial loss , it is the * * right * * attacks fo training . Attacks with magnitude larger than the margin $ \\|\\delta^ * \\| $ could be stronger than $ \\delta^ * $ , our margin maximization theory suggests that training on `` longer '' ( and thus stronger ) perturbations does not necessarily increase the margin ( Section 3 and Figure 2 ) . Reference : Rony et al.Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses , CVPR 2019 Please let us know if we do not directly address your concern out of this comment . > > 6.Fairness of comparison Please see response to all reviewers . Also please let us know if we do not fully address your concern , and we are happy to further elaborate on this . > > 6.Results on strong , unrestricted attacks like CW-L2 We reported AvgRobAcc , the average robust accuracy over different perturbation magnitudes , including those with very large magnitudes . Therefore , AvgRobAcc serves similar purpose to average norm of the strong unrestricted attacks . We are working on experiments using CW-L2 to test our models trained with $ \\ell_2 $ attacks , and will report the results when ready . > > 7.Why MMA does not improve robustness , but improves clean accuracy ? This means the theoretical analysis only stand under certain circumstances . This result does not contradict our theory . In contrast , it is very well aligned with our theory . For wrongly classified examples , MMA training focuses on getting them classified correctly . For correctly classified examples , our theory suggests that MMA training tries to enlarge the margins of all of them , based on their intrinsic robustness ( i.e.how difficult for a model to achieve a large margin on different points may be different ) . On the other hand , PGD training fails to adapt to the intrinsic robustness of different points , and thus significantly sacrifices its clean accuracy in order to achieve the slight additional robustness for large perturbation . This observation also echoes the sensitivity of PGD to its fixed ( and arbitrary ) perturbation magnitude . We will make this argument more clear in the paper . Please let us know if we need to further clarify . > > 7.Why do we need robustness against large $ \\ell_\\infty $ perturbations ? when perturbation goes large , the L2 attack ( eg.CW-L2 ) makes more sense than PGD- $ \\ell_\\infty $ . We agree with R3 that perturbations that cause perceptual differences shall not be included to test the robustness of the model . However , it is hard to determine the boundary of `` perceptual differences '' in terms of the perturbation magnitude . Compared to PGD training , MMA provides a natural way in dealing with this dilemma : user can set $ d_\\max $ represents the magnitude that is `` too large '' . Below $ d_\\max $ , MMA training enlarges the margin of each individual example based on its robustness under the current model , to the maximium capacity of the model . In contrast , the fixed $ \\epsilon $ in PGD training need to be `` large enough but not too large '' , which is much harder or even impossible to set , since each example could have different intrinsic robustness . As a result , MMA training is fairly insensitive to $ d_\\max $ , but PGD training is very sensitive to $ \\epsilon $ . In terms of what norm to measure the perturbation magnitude , we believe that it is more reasonable to evaluate the model using the norm that the model is trained on , namely `` train on $ \\ell_2 $ test on $ \\ell_2 $ '' , and `` train on $ \\ell_\\infty $ test on $ \\ell_\\infty $ '' . We will add CW-L2 results to models trained with $ \\ell_2 $ attacks when they are ready ."}, {"review_id": "HkeryxBtPB-1", "review_text": "Summary: The paper propose to use maximal margin optimization for correctly classified examples while keeping the optimization on misclassified examples unchanged. Specifically, for correctly classified examples, MMA adopts cross-entropy loss on adversarial examples, which are generated with example-dependent perturbation limit. For misclassified examples, MMA directly applies cross-entropy loss on natural examples. Problems: 1. For the performance measurement, why use the AvgRobAcc? does it make any sense to combine black-box results and white-box results? 2. For the epsilon, since it is different from the standard adversarial settings, how to guarantee the fair comparison? For example, how to evaluate the performance of MMA-12 to PGD-8 under the same test attack PGD-8? 3. For the baseline, the authors lack some necessary baselines, like the following [1] and [2] [1] Theoretically Principled Trade-off between Robustness and Accuracy. ICML 2019 [2] On the Convergence and Robustness of Adversarial Training. ICML2019", "rating": "6: Weak Accept", "reply_text": "Thank you for your efforts in reviewing our paper and the questions . Please let us know if we do not directly address your concern in this response . We are happy to hear further feedbacks from you . Besides the contributions that you 've summarized , we would also like to point out that 1 ) Our MMA training algorithm is not just a heuristic algorithm . The seemingly intuitive formulation of `` minimizing cross-entropy loss on shortest successful perturbation '' is backed up by our theories on * * direct * * margin maximization , and non-trivial construction of the margin 's lower bound ( Section 2 ) . 2 ) Section 3 analyzes the how does the fixed $ \\epsilon $ in standard adversarial training influence training , from a margin maximization perspective . Our theoretical predictions are supported by results in Sections 4 . > > 1. why use the AvgRobAcc ? We believe AvgRobAcc , the average robust accuracy , is a more comprehensive measure than the robust accuracy under a fixed ( and arbitrary ) perturbation magnitude . In practice , it is difficult to argue to what attack magnitude , the robust accuracy is more important . When there is a tradeoff , it is difficult to decide if a model with higher robust accuracy to $ 8/255 $ attacks but lower accuracy to $ 16/255 $ attacks is more robust or less robust . Another example would be the tradeoff between robustness and clean accuracy . It seems more rasonable to measure the `` area under the curve '' , which is approximated by AvgRobAcc . > > 1. does it make any sense to combine black-box results and white-box results ? Our intention is to have the strongest attack on each model to approximate the `` true '' robustness of the model . Therefore we report robust accuracy against the strongest attack among both white-box and black-box attacks . > > 2.Fairness of evaluation Please see response to all reviewers . > > 3.For the baseline , the authors lack some necessary baselines , like the following [ 1 ] and [ 2 ] We are working on evaluating [ 1 ] and [ 2 ] under our test settings . We will report the results when ready . We would also like to make a comment that , as concurrent work , our idea on directly maximizing input space margin is orthogonal to [ 1 ] 's idea on optimizing a regularized surrogate loss , and [ 2 ] 's idea on dynamically adjusting the convergence of inner maximization ."}, {"review_id": "HkeryxBtPB-2", "review_text": "This paper proposes a method, Max-Margin Adversarial (MMA) training, for robust learning against adversarial attacks. In the MMA, the margin in the input space is directly maximized. In order to alleviate an instability of the learning, a softmax variant of the max-margin is introduced. Moreover, the margin-maximization and the minimization of the worst-case loss are studied. Some numerical experiments show that the proposed MMA training is efficient against several adversarial attacks. * review: Overall, this paper is clearly written, and the readability is high. Though the idea in this paper is rather simple and straightforward, some theoretical supports are presented. A minor drawback is the length of the paper. The authors could shorten the paper within eight pages that is the standard length of ICLR paper. - In proposition 2.4: the loss L^{CE} should be clearly defined. - In equation (8), how is the weight of L^CE and L^MMA determined?", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . We are glad that you find the paper is clearly written and also value our theoretical results . > > A minor drawback is the length of the paper ... We will try our best to further shorten the main body of the paper . > > In proposition 2.4 : the loss $ L^ { CE } $ should be clearly defined . $ L^ { CE } _\\theta = \\log\\sum_j \\exp ( f_\\theta^j ( x ) ) - f_\\theta^y ( x ) $ , and we will make it clear in the paper . > > In equation ( 8 ) , how is the weight of $ L^ { CE } $ and $ L^ { MMA } $ determined ? We tested 3 pairs of weights , ( 1/3 , 2/3 ) , ( 1/2 , 1/2 ) and ( 2/3 , 1/3 ) , in our initial CIFAR10 Linf experiments . We observed that ( 1/3 , 2/3 ) , namely ( 1/3 for $ L^ { CE } $ and 2/3 for $ L^ { MMA } $ ) gives better performance . We then fixed it and use the same value for all the other experiments in the paper , including the MNIST experiments and L2 attack experiment s. We will make it clear in the appendix ."}], "0": {"review_id": "HkeryxBtPB-0", "review_text": "Summary: This paper proposes an adaptive margin-based adversarial training (eg. MMA) approach to train robust DNNs by maximizing the shortest margin of inputs to the decision boundary. Theoretical analyses have been provided to understand the connection between robust optimization and margin maximization. The main difference between the proposed approach to standard adversarial training is the adaptive selection of the perturbation bound \\epsilon. This makes adversarial training with large perturbation possible, which was previously unachievable by standard adversarial training (Madry et al.) Empirical results match the theoretical analysis. Pros: 1. The margin maximization idea has been well-explained, both intuitively and theoretically. 2. Interesting theoretical analyses and understandings of robust optimization from the margin perspective. 3. Clear advantage of MMA over standard adversarial training under large perturbations. 8. The proposed PGDLS is very interesting, actually quite good and much simpler, without extra computational cost. A similar idea was discussed in paper [3], where they gradually increase the convergence quality of training adversarial examples, and show the convergence guarantee of \"dynamic training\". 9. The gradient-free SPSA helps confirm the improvements of MMA under large perturbations are not a side effect of gradient masking. Cons: 1. The idea of \"shortest successful perturbation\" appears like a type of weak training attack, looking for minimum perturbations to just cross the classification boundary, like deepfool [1] or confidence 0 CW-L2 attack [2]. 2. The margin d_\\theta in Equation (1)/(2)/... defined on which norm? L_\\infty or L2 norm? I assume it's the infinity norm. In Theorem 2.1, the \\delta^{*} = argmin ||\\delta||, is a norm? Looks like a mistake. 3. The minimum margin \\delta^{*} is a bit confusing, is it used in maximization or just in the outer minimization? The last paragraph of page 3, L(\\theta, \\delta) or L(\\delta, \\theta), consistency check? 4. Why do we need the \"gradients of margins to model parameters\" analysis from Proposition 2.1 to remark 2.2? Given the \\delta^{*} found in the inner maximization (eg. attacking) process (step 1), minimizing the loss over this \\delta^{*} seems quite a straightforward step 2. Why don't go directly from Theorem 2.1 to Proposition 2.4, since the extensions from LM loss to SLM and CE loss via Proposition 2.3 -> Proposition 2.4., just proves that the standard classification loss CE can already maximize the margin given \\delta^{*}? 5. Section 4 Experiments. The experimental settings are not clear, and are not standard. What CIFAR10-\\ell_{\\infty} means: is it the CW-L2 attack, used for training, or for testing? How the test attacks were generated, the m and N, are confusing: for each test image, you have 260 samples for CIFAR10 (which means 260*10K in total), or just 260 in total (this is far less than a typical setting causing inaccurate results)? How are the d_max determined, and what are their relationship to standard \\epsilon? How the m models were trained? 6. Fairness of the comparison. Since MMA changes \\epsilon, how to fairly compare the robustness to standard epsilon bounded adversarial training is not discussed. Is it fair to compare MMA-3.0 vs PGD-2.5, since they have different epsilon? Why robustness was not tested against strong, unrestricted attacks like CW-L2 [2], and report the average L2 perturbations required to completely break the robustly trained model (and show MMA-trained models enforce large perturbations to succeed)? 7. Significance of the results. Normally, \\epsilon_{infty} > 16/255 will cause perceptual difference. Under 16/255, PGD-8/16, PGDLS-8/16 are still the best. At this level, it is quite a surprise that MMA does not improve robustness, although it does increase clean accuracy. This means the theoretical analysis only stand under certain circumstances. I don't think the optimal \\epsilon < margin can explain this, as it does not make sense to me the margin can be larger than 16/255. On the other hand, I thought the theoretical parts were discussing the ROBUSTNESS, not the CLEAN ACCURACY? But it turns out the MMA benefits a lot the clean accuracy? Why do we need robustness against large \\infty perturbations, this definitely deserves more discussion, as when perturbation goes large, the L2 attack (eg. CW-L2) makes more sense than PGD-\\infty. [1] Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. \"Deepfool: a simple and accurate method to fool deep neural networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [2] Carlini, Nicholas, and David Wagner. \"Towards evaluating the robustness of neural networks.\" 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017. [3] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" International Conference on Machine Learning. 2019. ============ My rating stays the same after reading through all the responses. I appreciate the authors' clarification on the notations and experimental settings. My 8/9 are positive points. My major concern is still the effectiveness of the proposed approach, and fairness of the comparison. It seems that MMA only works when the perturbation is large, which often larger than the \\epsilon used to train baseline adversarial training methods such as Trades. The authors seem have misunderstood my request for CWL2 results, I was just suggesting that the average L2 perturbation of CWL2 attack can be used as a fair test measure for robustness, instead of the AvgRobAcc used in the paper, and the susceptible comparison between MMA-12 vs PGD-8, or MMA-32 vs Trades. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your detailed reviews and also valuing our contributions . We would like to carify with R3 ( and other readers ) that many items under `` Cons '' are not strictly `` disadvantages '' of our method . Most of them seem to be clarification questions , especially that item 8 and 9 seem to be comments with positive sentiment . Please let us know if any of our answers does not resolve the confusion , and we are happy to further elaborate . > > 1 . `` shortest successful perturbation '' appears like a type of weak training attack , and similar to deepfool [ 1 ] or confidence 0 CW-L2 attack [ 2 ] . You are right that the `` shortest successful perturbation '' is similar to deepfool and CW-L2 . More precisely , deepfool , CW-L2 and our proposed AN-PGD are all algorithms for approximating the `` shortest successful perturbation '' , as we mentioned in Section 2.3 `` Other attacks that can serve a similar purpose can also fit into our MMA training framework '' . Specifically , the deepfool attack is not strong enough ( e.g.as shown in Rony et al.2019 ) , probably due to it only uses first order approximation to find $ \\delta^ * $ , and therefore is not suitable for MMA training . On the other hand , CW-L2 is likely strong enough , but too expensive to compute during training . Moreover , while the `` shortest successful perturbation '' , $ \\delta^ * $ , is a `` weak '' training attack if measured in the adversarial loss , it is the * * right * * attacks fo training . Attacks with magnitude larger than the margin $ \\|\\delta^ * \\| $ could be stronger than $ \\delta^ * $ , our margin maximization theory suggests that training on `` longer '' ( and thus stronger ) perturbations does not necessarily increase the margin ( Section 3 and Figure 2 ) . Reference : Rony et al.Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses , CVPR 2019 Please let us know if we do not directly address your concern out of this comment . > > 6.Fairness of comparison Please see response to all reviewers . Also please let us know if we do not fully address your concern , and we are happy to further elaborate on this . > > 6.Results on strong , unrestricted attacks like CW-L2 We reported AvgRobAcc , the average robust accuracy over different perturbation magnitudes , including those with very large magnitudes . Therefore , AvgRobAcc serves similar purpose to average norm of the strong unrestricted attacks . We are working on experiments using CW-L2 to test our models trained with $ \\ell_2 $ attacks , and will report the results when ready . > > 7.Why MMA does not improve robustness , but improves clean accuracy ? This means the theoretical analysis only stand under certain circumstances . This result does not contradict our theory . In contrast , it is very well aligned with our theory . For wrongly classified examples , MMA training focuses on getting them classified correctly . For correctly classified examples , our theory suggests that MMA training tries to enlarge the margins of all of them , based on their intrinsic robustness ( i.e.how difficult for a model to achieve a large margin on different points may be different ) . On the other hand , PGD training fails to adapt to the intrinsic robustness of different points , and thus significantly sacrifices its clean accuracy in order to achieve the slight additional robustness for large perturbation . This observation also echoes the sensitivity of PGD to its fixed ( and arbitrary ) perturbation magnitude . We will make this argument more clear in the paper . Please let us know if we need to further clarify . > > 7.Why do we need robustness against large $ \\ell_\\infty $ perturbations ? when perturbation goes large , the L2 attack ( eg.CW-L2 ) makes more sense than PGD- $ \\ell_\\infty $ . We agree with R3 that perturbations that cause perceptual differences shall not be included to test the robustness of the model . However , it is hard to determine the boundary of `` perceptual differences '' in terms of the perturbation magnitude . Compared to PGD training , MMA provides a natural way in dealing with this dilemma : user can set $ d_\\max $ represents the magnitude that is `` too large '' . Below $ d_\\max $ , MMA training enlarges the margin of each individual example based on its robustness under the current model , to the maximium capacity of the model . In contrast , the fixed $ \\epsilon $ in PGD training need to be `` large enough but not too large '' , which is much harder or even impossible to set , since each example could have different intrinsic robustness . As a result , MMA training is fairly insensitive to $ d_\\max $ , but PGD training is very sensitive to $ \\epsilon $ . In terms of what norm to measure the perturbation magnitude , we believe that it is more reasonable to evaluate the model using the norm that the model is trained on , namely `` train on $ \\ell_2 $ test on $ \\ell_2 $ '' , and `` train on $ \\ell_\\infty $ test on $ \\ell_\\infty $ '' . We will add CW-L2 results to models trained with $ \\ell_2 $ attacks when they are ready ."}, "1": {"review_id": "HkeryxBtPB-1", "review_text": "Summary: The paper propose to use maximal margin optimization for correctly classified examples while keeping the optimization on misclassified examples unchanged. Specifically, for correctly classified examples, MMA adopts cross-entropy loss on adversarial examples, which are generated with example-dependent perturbation limit. For misclassified examples, MMA directly applies cross-entropy loss on natural examples. Problems: 1. For the performance measurement, why use the AvgRobAcc? does it make any sense to combine black-box results and white-box results? 2. For the epsilon, since it is different from the standard adversarial settings, how to guarantee the fair comparison? For example, how to evaluate the performance of MMA-12 to PGD-8 under the same test attack PGD-8? 3. For the baseline, the authors lack some necessary baselines, like the following [1] and [2] [1] Theoretically Principled Trade-off between Robustness and Accuracy. ICML 2019 [2] On the Convergence and Robustness of Adversarial Training. ICML2019", "rating": "6: Weak Accept", "reply_text": "Thank you for your efforts in reviewing our paper and the questions . Please let us know if we do not directly address your concern in this response . We are happy to hear further feedbacks from you . Besides the contributions that you 've summarized , we would also like to point out that 1 ) Our MMA training algorithm is not just a heuristic algorithm . The seemingly intuitive formulation of `` minimizing cross-entropy loss on shortest successful perturbation '' is backed up by our theories on * * direct * * margin maximization , and non-trivial construction of the margin 's lower bound ( Section 2 ) . 2 ) Section 3 analyzes the how does the fixed $ \\epsilon $ in standard adversarial training influence training , from a margin maximization perspective . Our theoretical predictions are supported by results in Sections 4 . > > 1. why use the AvgRobAcc ? We believe AvgRobAcc , the average robust accuracy , is a more comprehensive measure than the robust accuracy under a fixed ( and arbitrary ) perturbation magnitude . In practice , it is difficult to argue to what attack magnitude , the robust accuracy is more important . When there is a tradeoff , it is difficult to decide if a model with higher robust accuracy to $ 8/255 $ attacks but lower accuracy to $ 16/255 $ attacks is more robust or less robust . Another example would be the tradeoff between robustness and clean accuracy . It seems more rasonable to measure the `` area under the curve '' , which is approximated by AvgRobAcc . > > 1. does it make any sense to combine black-box results and white-box results ? Our intention is to have the strongest attack on each model to approximate the `` true '' robustness of the model . Therefore we report robust accuracy against the strongest attack among both white-box and black-box attacks . > > 2.Fairness of evaluation Please see response to all reviewers . > > 3.For the baseline , the authors lack some necessary baselines , like the following [ 1 ] and [ 2 ] We are working on evaluating [ 1 ] and [ 2 ] under our test settings . We will report the results when ready . We would also like to make a comment that , as concurrent work , our idea on directly maximizing input space margin is orthogonal to [ 1 ] 's idea on optimizing a regularized surrogate loss , and [ 2 ] 's idea on dynamically adjusting the convergence of inner maximization ."}, "2": {"review_id": "HkeryxBtPB-2", "review_text": "This paper proposes a method, Max-Margin Adversarial (MMA) training, for robust learning against adversarial attacks. In the MMA, the margin in the input space is directly maximized. In order to alleviate an instability of the learning, a softmax variant of the max-margin is introduced. Moreover, the margin-maximization and the minimization of the worst-case loss are studied. Some numerical experiments show that the proposed MMA training is efficient against several adversarial attacks. * review: Overall, this paper is clearly written, and the readability is high. Though the idea in this paper is rather simple and straightforward, some theoretical supports are presented. A minor drawback is the length of the paper. The authors could shorten the paper within eight pages that is the standard length of ICLR paper. - In proposition 2.4: the loss L^{CE} should be clearly defined. - In equation (8), how is the weight of L^CE and L^MMA determined?", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . We are glad that you find the paper is clearly written and also value our theoretical results . > > A minor drawback is the length of the paper ... We will try our best to further shorten the main body of the paper . > > In proposition 2.4 : the loss $ L^ { CE } $ should be clearly defined . $ L^ { CE } _\\theta = \\log\\sum_j \\exp ( f_\\theta^j ( x ) ) - f_\\theta^y ( x ) $ , and we will make it clear in the paper . > > In equation ( 8 ) , how is the weight of $ L^ { CE } $ and $ L^ { MMA } $ determined ? We tested 3 pairs of weights , ( 1/3 , 2/3 ) , ( 1/2 , 1/2 ) and ( 2/3 , 1/3 ) , in our initial CIFAR10 Linf experiments . We observed that ( 1/3 , 2/3 ) , namely ( 1/3 for $ L^ { CE } $ and 2/3 for $ L^ { MMA } $ ) gives better performance . We then fixed it and use the same value for all the other experiments in the paper , including the MNIST experiments and L2 attack experiment s. We will make it clear in the appendix ."}}