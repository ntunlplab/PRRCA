{"year": "2020", "forum": "rJg3zxBYwH", "title": "Learning Likelihoods with Conditional Normalizing Flows ", "decision": "Reject", "meta_review": "The authors propose a conditional normalizing flow approach to learning likelihoods. While reviewers appreciated the paper, in its present form it lacked a clear champion, and there were still some remaining concerns about novelty and clarity of presentation. The authors are encouraged to continue with this work and to account for reviewer comments in future revisions. Following up on the author response, a reviewer adds:\n\"Thanks for your clarification. I still disagree that the conditional flow architecture proposed should be considered as a novel contribution. The reason why I mentioned [1] or [2] was not because they follow the exact setting (coupling based conditional flow model) discussed in this paper. I wanted to highlight that the idea to use conditioning variables as an input to the transforming network (whether it is an autoregressive density function, autoregressive transforming network, or coupling layers) is quite universal (as we all know many of the existing codes implementing flow-based models includes additional keyword arguments 'context' to model conditioning). I'm not sure why the fact that the proposed framework is conditioning on high-dimensional variables makes a contribution. There seems to be no particular challenge in doing that and novel design choices to circumvent that (i.e., we can just use existing architectures with minor modifications).\n\nI agree that the binary dequantization should be considered as a contribution, but as significant as to change my decision to accept. Thanks for the clarification on experiments. Considering this, I raise my rating to weak reject...\n\nAnother previous work I forgot to mention in the initial review is \"Structured output learning with the conditional generative flow\", Lu and Huang 2019, ICML 2019 invertible neural network workshop. This paper discusses the conditional flow based on a similar idea, and attacks high-dimensional structured output prediction. I think this should be cited in the paper.\"\n", "reviews": [{"review_id": "rJg3zxBYwH-0", "review_text": "The paper proposes the conditional normalizing flow for structured prediction. The idea is to use conditioning variables as additional inputs to the flow parameter forming networks. The model was demonstrated on image superresolution and vessel segmentation. I find the contribution of this paper minimal. The idea of conditioning has extensively been used during recent years because it is the most natural thing to do (e.g., [1], [2] and numerous other papers). Their's nothing new about the flows used in this paper. The results in table 2 are not convincing; I see no benefit of using the proposed flow model for image super-resolution instead of the SOTA super-resolution methods. This also applies to other experiments. [1] van den Oord et al., Conditional Image Generation with PixelCNN Decoders, 2016. [2] Papamakarios et al., Masked Autoregressive Flow for Density Estimation, 2017.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . We would like to disagree on the topic on novelty of our contribution . In particular , reference [ 1 ] is not a flow . And indeed as you state , class-conditional flow models are not new in the literature . Where we would like to draw our distinction , however , is that we are in fact not considering class-conditional flow models . Instead our generative flow uses high-dimensional images as the conditioning argument . This warrants the use of a different kind of conditional coupling layer , unlike the ones in the papers that you cite . These papers also happen to be autoregressive , which makes sampling computationally expensive . Another contribution we would like to highlight , which was also recognized by reviewers 2 and 3 , is the link we draw between variational dequantization and existing variational inference methods . This new viewpoint allows us to derive a form of variational dequantization adapted to binary random variables in a consistent probabilistic framework . This innovation is important when it comes to finding a good lower bound on the likelihood . For instance , in the retinal vessel segmentation experiments , the log-likelihood scores for uniform dequantization versus our method is about 0.35 BPD versus 0.025 BPD ( 2.s.f ) . In an updated version of the paper , we are going to include this results to stress this improvement . With respect to Table 2 , the specific metrics that are important depend on what task you are willing to solve . In terms of fitting distributions , we outperform our baselines . As stated in our introduction , we want to learn distributions over the data , because they can be easily evaluated in terms of likelihood , they are very interpretable compared to other generative methods such as GANs , there is no mode dropping behavior , and there exists easy tests for overfitting ."}, {"review_id": "rJg3zxBYwH-1", "review_text": "Summary of the paper: The paper proposes an extension of Normalizing flows to conditional distributions. The paper is well written overall and easy to follow. Basically the conditional prior z|x = z=f_{\\phi}(y,x), where x is the conditioning random variable, and we apply the change of variable formula to get the density of y|x . For example in super resolution y is the high res image and x is the low res. image. To sample from the models authors propose to use f^{-1}_{\\phi}(z;x). The conditional modules are natural extensions of invertible blocks used in the literature (coupling layers, split priors, conditional coupling, 1x1 conv), where the conditioning is done on some hidden representations of the conditioning variable x (i.e one or multiple layers of NN). Authors propose a dequantization for binary random variables (useful for segmentation applications), where they give an implicit model for the dequantizer (obtain a continuous variable from a discrete binary variable). Author apply the method in two applications super-resolution and vessel segmentation. the method is compared to supervised learning of the corresponds between x and y and to others competitive methods in the literature and shows some advantage. Minor comments : - Formatting the bibliography is messed up and needs some cleaning , Figure 5 is also making formatting issues of the paper. - Figure 1 for sampling it should be f^-1_{\\phi } and not f_{\\phi} Review: - Figure 2 is hard to get any idea of the sample quality would be good also to put the low resolution input to the algorithm . Also did you use a temperature sampling for the baseline ? otherwise the comparison is not fair. - The Drive database is too small 20 training samples and 20 testing only? can the model be just overfitting? - In the vessel implementation why do you drop the scaling modules? - The conditioning for the vessel implementation on x is on two layers , would be great to put all architectures of the models in details , and to show both sampling and training paths - It would be great to add the details of the skip connection used from the network processing x, and how ensure that the flow remains invertible. Overall this is a well written paper and a good addition to normalizing flows methods , some discussion of related works on conditional normalizing flows and more baselines with other competitive methods based on GANs for example would be helpful but not necessary. It would be great to add details of the architectures and on skip connections and how to ensure invertibility for this part in the model . ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments In Figure 2 , we will include examples of the low resolution input , for an easier comparison of the results . In this figure in particular , we did not use a temperature for sampling of the baseline , since we are displaying the mode of the distribution . Since the distribution is factorized , sampling would add uncorrelated noise , meaning this comparison is actually skewed in favour of the baseline model . Concerning the DRIVE database , it indeed has very few images . Since we are training a likelihood based model , it is very easy for us to check for overfitting and early stop accordingly . In practice , we found that the standard data augmentation implied that we do not overfit . Furthermore , since the task is a per-pixel reconstruction task , the effective number of labels is much higher than the number of training images . In the DRIVE experiments , we dropped the scaling modules since they did not appear to add much benefit to the results . With regards to the exact architectures we have now placed network architecture tables in the appendix to clear up any confusion . Furthermore , we are adding a diagram of the conditional coupling layers in the appendix , which show the invertibility property clearly . We have extended our related work on non-flow-based competing methods from the literature . and we have added some extra references on ( conditional ) normalizing flows as well . We have already cleaned up the bibliography and any formatting issues , which we had at submission time . Thank you also for the sharp observation regarding the missing ^ { -1 } in Figure 1 ."}, {"review_id": "rJg3zxBYwH-2", "review_text": "This paper presented the conditional normalizing flows (CNFs) as a new kind of likelihood-based learning objective. There are two keys in CNFs. One is the parametric mapping function f_{\\phi} and the other is the conditional prior. This paper assumed the conditional prior as Gaussian distribution of x. The mapping function is invertible with x as a parameter. The prior parameter and \\phi are updated by stochastic gradient descent. The latent variable z is then sampled from conditional prior. The output targe y is obtained with dependency on x and f_{\\phi}. Strength: 1. This study adopted the flow-based model to estimate the conditional flow without using any generative model or adversarial method. 2. This method obtained the advanced results on DRIU dataset without the requirement of pretraining. 3. This paper proposed an useful solution to train continuous CNFs for binary problems. Weakness: 1. It is required to address how to design the function f_{\\phi} which depends on x. In particular, the property invertibility should be clarified. 2. Why the issues of mode collapse or training instability in flow are considerable in the experiments? 3. It will be meaningful to evaluate this method by performing the tasks on text to image or label to image.", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . To clarify your concerns , the design is covered in the section 3.1 Conditional modules . In particular the main invertible module is the conditional coupling layer . This takes in a conditioning input x and a latent variable z , which is transformed deterministically into a latent variable y . The transformation y < - > z conditioned on x is invertible . This transformation is similar to the coupling layer of RealNVP , but where every subnetwork in the layer takes and additional x as input . For clarity , we can add a diagram detailing this in the appendix . With regards to your comments about mode collapse and training instability , it has been noted in the literature that normalizing flows do not suffer so much from mode collapse in the same way that GANs do , for instance . And on the topic of training instability , we did not notice any instabilities in the training of our flow models . Thank you for your suggestions on follow up experiments . We agree that a text to image scenario would be interesting , since the conditioning argument in this case is structured ."}], "0": {"review_id": "rJg3zxBYwH-0", "review_text": "The paper proposes the conditional normalizing flow for structured prediction. The idea is to use conditioning variables as additional inputs to the flow parameter forming networks. The model was demonstrated on image superresolution and vessel segmentation. I find the contribution of this paper minimal. The idea of conditioning has extensively been used during recent years because it is the most natural thing to do (e.g., [1], [2] and numerous other papers). Their's nothing new about the flows used in this paper. The results in table 2 are not convincing; I see no benefit of using the proposed flow model for image super-resolution instead of the SOTA super-resolution methods. This also applies to other experiments. [1] van den Oord et al., Conditional Image Generation with PixelCNN Decoders, 2016. [2] Papamakarios et al., Masked Autoregressive Flow for Density Estimation, 2017.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . We would like to disagree on the topic on novelty of our contribution . In particular , reference [ 1 ] is not a flow . And indeed as you state , class-conditional flow models are not new in the literature . Where we would like to draw our distinction , however , is that we are in fact not considering class-conditional flow models . Instead our generative flow uses high-dimensional images as the conditioning argument . This warrants the use of a different kind of conditional coupling layer , unlike the ones in the papers that you cite . These papers also happen to be autoregressive , which makes sampling computationally expensive . Another contribution we would like to highlight , which was also recognized by reviewers 2 and 3 , is the link we draw between variational dequantization and existing variational inference methods . This new viewpoint allows us to derive a form of variational dequantization adapted to binary random variables in a consistent probabilistic framework . This innovation is important when it comes to finding a good lower bound on the likelihood . For instance , in the retinal vessel segmentation experiments , the log-likelihood scores for uniform dequantization versus our method is about 0.35 BPD versus 0.025 BPD ( 2.s.f ) . In an updated version of the paper , we are going to include this results to stress this improvement . With respect to Table 2 , the specific metrics that are important depend on what task you are willing to solve . In terms of fitting distributions , we outperform our baselines . As stated in our introduction , we want to learn distributions over the data , because they can be easily evaluated in terms of likelihood , they are very interpretable compared to other generative methods such as GANs , there is no mode dropping behavior , and there exists easy tests for overfitting ."}, "1": {"review_id": "rJg3zxBYwH-1", "review_text": "Summary of the paper: The paper proposes an extension of Normalizing flows to conditional distributions. The paper is well written overall and easy to follow. Basically the conditional prior z|x = z=f_{\\phi}(y,x), where x is the conditioning random variable, and we apply the change of variable formula to get the density of y|x . For example in super resolution y is the high res image and x is the low res. image. To sample from the models authors propose to use f^{-1}_{\\phi}(z;x). The conditional modules are natural extensions of invertible blocks used in the literature (coupling layers, split priors, conditional coupling, 1x1 conv), where the conditioning is done on some hidden representations of the conditioning variable x (i.e one or multiple layers of NN). Authors propose a dequantization for binary random variables (useful for segmentation applications), where they give an implicit model for the dequantizer (obtain a continuous variable from a discrete binary variable). Author apply the method in two applications super-resolution and vessel segmentation. the method is compared to supervised learning of the corresponds between x and y and to others competitive methods in the literature and shows some advantage. Minor comments : - Formatting the bibliography is messed up and needs some cleaning , Figure 5 is also making formatting issues of the paper. - Figure 1 for sampling it should be f^-1_{\\phi } and not f_{\\phi} Review: - Figure 2 is hard to get any idea of the sample quality would be good also to put the low resolution input to the algorithm . Also did you use a temperature sampling for the baseline ? otherwise the comparison is not fair. - The Drive database is too small 20 training samples and 20 testing only? can the model be just overfitting? - In the vessel implementation why do you drop the scaling modules? - The conditioning for the vessel implementation on x is on two layers , would be great to put all architectures of the models in details , and to show both sampling and training paths - It would be great to add the details of the skip connection used from the network processing x, and how ensure that the flow remains invertible. Overall this is a well written paper and a good addition to normalizing flows methods , some discussion of related works on conditional normalizing flows and more baselines with other competitive methods based on GANs for example would be helpful but not necessary. It would be great to add details of the architectures and on skip connections and how to ensure invertibility for this part in the model . ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments In Figure 2 , we will include examples of the low resolution input , for an easier comparison of the results . In this figure in particular , we did not use a temperature for sampling of the baseline , since we are displaying the mode of the distribution . Since the distribution is factorized , sampling would add uncorrelated noise , meaning this comparison is actually skewed in favour of the baseline model . Concerning the DRIVE database , it indeed has very few images . Since we are training a likelihood based model , it is very easy for us to check for overfitting and early stop accordingly . In practice , we found that the standard data augmentation implied that we do not overfit . Furthermore , since the task is a per-pixel reconstruction task , the effective number of labels is much higher than the number of training images . In the DRIVE experiments , we dropped the scaling modules since they did not appear to add much benefit to the results . With regards to the exact architectures we have now placed network architecture tables in the appendix to clear up any confusion . Furthermore , we are adding a diagram of the conditional coupling layers in the appendix , which show the invertibility property clearly . We have extended our related work on non-flow-based competing methods from the literature . and we have added some extra references on ( conditional ) normalizing flows as well . We have already cleaned up the bibliography and any formatting issues , which we had at submission time . Thank you also for the sharp observation regarding the missing ^ { -1 } in Figure 1 ."}, "2": {"review_id": "rJg3zxBYwH-2", "review_text": "This paper presented the conditional normalizing flows (CNFs) as a new kind of likelihood-based learning objective. There are two keys in CNFs. One is the parametric mapping function f_{\\phi} and the other is the conditional prior. This paper assumed the conditional prior as Gaussian distribution of x. The mapping function is invertible with x as a parameter. The prior parameter and \\phi are updated by stochastic gradient descent. The latent variable z is then sampled from conditional prior. The output targe y is obtained with dependency on x and f_{\\phi}. Strength: 1. This study adopted the flow-based model to estimate the conditional flow without using any generative model or adversarial method. 2. This method obtained the advanced results on DRIU dataset without the requirement of pretraining. 3. This paper proposed an useful solution to train continuous CNFs for binary problems. Weakness: 1. It is required to address how to design the function f_{\\phi} which depends on x. In particular, the property invertibility should be clarified. 2. Why the issues of mode collapse or training instability in flow are considerable in the experiments? 3. It will be meaningful to evaluate this method by performing the tasks on text to image or label to image.", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . To clarify your concerns , the design is covered in the section 3.1 Conditional modules . In particular the main invertible module is the conditional coupling layer . This takes in a conditioning input x and a latent variable z , which is transformed deterministically into a latent variable y . The transformation y < - > z conditioned on x is invertible . This transformation is similar to the coupling layer of RealNVP , but where every subnetwork in the layer takes and additional x as input . For clarity , we can add a diagram detailing this in the appendix . With regards to your comments about mode collapse and training instability , it has been noted in the literature that normalizing flows do not suffer so much from mode collapse in the same way that GANs do , for instance . And on the topic of training instability , we did not notice any instabilities in the training of our flow models . Thank you for your suggestions on follow up experiments . We agree that a text to image scenario would be interesting , since the conditioning argument in this case is structured ."}}