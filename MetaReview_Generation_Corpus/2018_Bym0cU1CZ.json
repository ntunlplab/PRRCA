{"year": "2018", "forum": "Bym0cU1CZ", "title": "Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts", "decision": "Reject", "meta_review": "This work takes dialogue acts into account to generate responses in a human-machine conversation. However, incorporating dialogue acts into open-domain dialogue was already the focus of Zhao et al's ACL 2017 paper, Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human-machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot. Despite the authors' response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re-work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context.", "reviews": [{"review_id": "Bym0cU1CZ-0", "review_text": "The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues. In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement. While this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue. If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems. Some points: 1. The introduction uses \"scalability\" throughout to mean something closer to \"ability to generalize.\" Consider revising the wording here. 2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000). It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora. 3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?) 3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space. A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity. 4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices. If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited. Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable. 5. The comparison against previous work is missing some assurances I'd like to see. While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to. Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison. 6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?) 7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections. It's fine if that pushes the paper somewhat over the 8th page. 8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research. 9. The examples provided in the appendix are great. It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked).", "rating": "7: Good paper, accept", "reply_text": "Thank you for your valuable comments . 1.We replace the word `` scalability '' in Introduction with other words ( e.g. , `` scale to new domains '' ) . 2.We follow your suggestions and cite related work about dialogue acts at the beginning of Section 2.1 . We also mention a public DA corpora `` the Switchboard Corpus '' in the second paragraph of Section 2.1 and clarify that we build a new data set because no one has analyzed open domain dialogues with dialogue acts about conversational context before . 3.We modify Equation ( 4 ) in the previous version as Equation ( 4 ) +Equation ( 5 ) . Now dialogue act selection in our model becomes more general and takes multiple strategies ( top 1 and top K ) as special cases . We can try dialogue generation with top K acts in our future work . 4.We break up some long paragraphs in Section 3.2 for ease of reading and cite ( Li et al. , 2016b ) before the termination strategies , as some of them ( e.g. , regarding to repetitive turns ) are inspired by the work . 5.Although we use a different data set , the average number of turns of the simulated dialogues from RL-S2S in our work is very close to the number reported in ( Li et al. , 2016b ) . Our number is 4.36 ( refer to the machine-machine column in Table 4 ( b ) ) , while the number reported in ( Li et al. , 2016b ) is 4.48 . This might provide an additional evidence to the correctness of the implementation of the baseline model in the work . 6.Table 5 in the previous version becomes Table 6 now . We describe the table right after it . Basically , SL-DAGM and RL-DAGM share the same text generation but differs on how they select dialogue acts , as we only optimize the policy network with RL . The response given by RL-DAGM comes from CS.Q ( clarified after the generated response in Table 6 ) , while the response given by SL-DAGM comes from CS.S . Both are top dialogue acts under the corresponding policy networks . 7.We follow your suggestions and break up long paragraphs . 8.The examples given in Appendix are picked randomly ."}, {"review_id": "Bym0cU1CZ-1", "review_text": "The topic discussed in this paper is interesting. Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling, however this paper is difficult for me to follow. For example: 1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning. 2) the formulation in equation 4 seems to be problematic 3) \"simplify pr(ri|si,ai) as pr(ri|ai,ui\u22121,ui\u22122) since decoding natural language responses from long conversation history is challenging\" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something? 4) about section 3.2, again I didn't get whether the model needs RL for training. 5) \"We train m(\u00b7, \u00b7) with the 30 million crawled data through negative sampling.\" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model. 6) the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your valuable comments . 1.Why we need reinforcement learning As we have mentioned in the paper , open domain dialogue generation needs to be optimized for long-term engagement in practice . Yes , in supervised learning , we have large scale of human dialogues tagged with dialogue acts , but that does not mean the algorithm can learn how to keep conversation going from the data , as more than 45 % training dialogues are not longer than 5 turns ( described in the last paragraph of Section 3.1 in the new version ) . Supervised learning just learns a model by maximizing the likelihood of the observed data including the short dialogues . Dialogue acts are learned only according to the history , and no information of the future influence can flow in . Then , without an additional objective ( i.e. , Equation ( 8 ) in Section 3.2 ) and mechanism ( optimizing for future success ) , how can we ( explicitly ) guarantee that the model is optimized for long-term engagement ? Therefore , supervised learning is to learn human language and reinforcement learning is to further optimize the combination of dialogue acts in order to achieve long-term conversation . Model optimization with reinforcement learning is also encouraged by the experimental results . In Table 5 , response diversity is significantly improved by RL ( see the difference between RL-DAGM and SL-DAGM on distinct-1 and distinct-2 ) , and in Table 4 ( b ) , with RL , both the dialogues from machine-machine simulation and human-machine test become longer . Moreover , as we have analyzed in the last paragraph of Section 4.3 , it is because RL can promote context switch in interactions that the model , after optimized with RL , can lead to better engagement . All the results well support our motivation to learning with RL . 2. > > > the formulation in equation 4 seems to be problematic Thanks for pointing out this problem . We have modified Equation ( 4 ) in the previous version as Equation ( 4 ) +Equation ( 5 ) in the new version . Now the procedure of generation becomes more clear . 3. > > > '' Simplify pr ( ri|si , ai ) as pr ( ri|ai , ui\u22121 , ui\u22122 ) since decoding natural language responses from long conversation history is challenging '' to my understanding , the only difference between the original and simplified model is the encoder part not the decoder part . Did I miss something Yes , from a model perspective , the simplification here just changes the encoder . However , what we mean here is that it is difficult for an RNN to memorize long conversation history , and thus encoding long history means either the response given by the decoder is irrelevant to the early history , or the response will be messed up . 4. > > > '' We train m ( \u00b7 , \u00b7 ) with the 30 million crawled data through negative sampling . '' not sure I understand the connection between training $ m ( \\cdot , \\cdot ) $ and the entire model $ m ( \\cdot , \\cdot ) $ is pre-trained and used to estimate the reward function in Equation ( 9 ) . This is the only connection between $ m ( \\cdot , \\cdot ) $ and the entire model . We have clarified this in the paragraph after Equation ( 9 ) . 5. > > > the experiments are not convincing . At least , it should show the generation texts were affected about DAs in a systemic way . Only a single example in table 5 is not enough . Thanks for your comments . We do three things to show how the generated texts are affected by dialogue acts : ( 1 ) We move Table 7 in the previous version from Appendix to Section 4.2 . Now the table is Table 5 . In the table , one can see that with dialogue acts , the diversity of generated responses is significantly improved ( corresponding to much larger distinct-1 and distinct-2 ) . In the following explanation ( the third paragraph of Section 4.2 ) , we claim that this is one benefit of dialogue acts , as search space now becomes act \u00d7 language . ( 2 ) We add Section 4.4 where we compare responses from different dialogue acts using some metrics . The conclusion is that responses generated from CS . * are longer , more informative , and contain more new words than responses generated from CM . * , and statements and answers are generally more informative than questions in both CS . * and CM . * . Please refer to the new version of the paper to get more details . ( 3 ) In the last paragraph of Section 4.3 , we show that simulated dialogues without CS . * are much shorter than those with CS . * ( SL : 4.78 v.s.8.66 , RL : 2.67 v.s. , 8.18 ) . The result indicates that if we remove CS . * , then the conversation engagement of our model may degrade to the baseline model ."}, {"review_id": "Bym0cU1CZ-2", "review_text": "The paper describes a technique to incorporate dialog acts into neural conversational agents. This is very interesting work. Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act. A very important type of dialog act is \"switching topic\", often done to ensure that the conversation will continue. The paper describes a classifier that predicts the dialog act of the next utterance. The next utterance is then generated based on this dialog act. The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning. This is also very interesting. The empirical evaluation demonstrates the effectiveness of the approach. The paper is also well written. I do not have any suggestion for improvement. This is good work that should be published.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments"}], "0": {"review_id": "Bym0cU1CZ-0", "review_text": "The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues. In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement. While this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue. If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems. Some points: 1. The introduction uses \"scalability\" throughout to mean something closer to \"ability to generalize.\" Consider revising the wording here. 2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000). It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora. 3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?) 3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space. A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity. 4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices. If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited. Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable. 5. The comparison against previous work is missing some assurances I'd like to see. While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to. Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison. 6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?) 7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections. It's fine if that pushes the paper somewhat over the 8th page. 8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research. 9. The examples provided in the appendix are great. It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked).", "rating": "7: Good paper, accept", "reply_text": "Thank you for your valuable comments . 1.We replace the word `` scalability '' in Introduction with other words ( e.g. , `` scale to new domains '' ) . 2.We follow your suggestions and cite related work about dialogue acts at the beginning of Section 2.1 . We also mention a public DA corpora `` the Switchboard Corpus '' in the second paragraph of Section 2.1 and clarify that we build a new data set because no one has analyzed open domain dialogues with dialogue acts about conversational context before . 3.We modify Equation ( 4 ) in the previous version as Equation ( 4 ) +Equation ( 5 ) . Now dialogue act selection in our model becomes more general and takes multiple strategies ( top 1 and top K ) as special cases . We can try dialogue generation with top K acts in our future work . 4.We break up some long paragraphs in Section 3.2 for ease of reading and cite ( Li et al. , 2016b ) before the termination strategies , as some of them ( e.g. , regarding to repetitive turns ) are inspired by the work . 5.Although we use a different data set , the average number of turns of the simulated dialogues from RL-S2S in our work is very close to the number reported in ( Li et al. , 2016b ) . Our number is 4.36 ( refer to the machine-machine column in Table 4 ( b ) ) , while the number reported in ( Li et al. , 2016b ) is 4.48 . This might provide an additional evidence to the correctness of the implementation of the baseline model in the work . 6.Table 5 in the previous version becomes Table 6 now . We describe the table right after it . Basically , SL-DAGM and RL-DAGM share the same text generation but differs on how they select dialogue acts , as we only optimize the policy network with RL . The response given by RL-DAGM comes from CS.Q ( clarified after the generated response in Table 6 ) , while the response given by SL-DAGM comes from CS.S . Both are top dialogue acts under the corresponding policy networks . 7.We follow your suggestions and break up long paragraphs . 8.The examples given in Appendix are picked randomly ."}, "1": {"review_id": "Bym0cU1CZ-1", "review_text": "The topic discussed in this paper is interesting. Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling, however this paper is difficult for me to follow. For example: 1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning. 2) the formulation in equation 4 seems to be problematic 3) \"simplify pr(ri|si,ai) as pr(ri|ai,ui\u22121,ui\u22122) since decoding natural language responses from long conversation history is challenging\" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something? 4) about section 3.2, again I didn't get whether the model needs RL for training. 5) \"We train m(\u00b7, \u00b7) with the 30 million crawled data through negative sampling.\" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model. 6) the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your valuable comments . 1.Why we need reinforcement learning As we have mentioned in the paper , open domain dialogue generation needs to be optimized for long-term engagement in practice . Yes , in supervised learning , we have large scale of human dialogues tagged with dialogue acts , but that does not mean the algorithm can learn how to keep conversation going from the data , as more than 45 % training dialogues are not longer than 5 turns ( described in the last paragraph of Section 3.1 in the new version ) . Supervised learning just learns a model by maximizing the likelihood of the observed data including the short dialogues . Dialogue acts are learned only according to the history , and no information of the future influence can flow in . Then , without an additional objective ( i.e. , Equation ( 8 ) in Section 3.2 ) and mechanism ( optimizing for future success ) , how can we ( explicitly ) guarantee that the model is optimized for long-term engagement ? Therefore , supervised learning is to learn human language and reinforcement learning is to further optimize the combination of dialogue acts in order to achieve long-term conversation . Model optimization with reinforcement learning is also encouraged by the experimental results . In Table 5 , response diversity is significantly improved by RL ( see the difference between RL-DAGM and SL-DAGM on distinct-1 and distinct-2 ) , and in Table 4 ( b ) , with RL , both the dialogues from machine-machine simulation and human-machine test become longer . Moreover , as we have analyzed in the last paragraph of Section 4.3 , it is because RL can promote context switch in interactions that the model , after optimized with RL , can lead to better engagement . All the results well support our motivation to learning with RL . 2. > > > the formulation in equation 4 seems to be problematic Thanks for pointing out this problem . We have modified Equation ( 4 ) in the previous version as Equation ( 4 ) +Equation ( 5 ) in the new version . Now the procedure of generation becomes more clear . 3. > > > '' Simplify pr ( ri|si , ai ) as pr ( ri|ai , ui\u22121 , ui\u22122 ) since decoding natural language responses from long conversation history is challenging '' to my understanding , the only difference between the original and simplified model is the encoder part not the decoder part . Did I miss something Yes , from a model perspective , the simplification here just changes the encoder . However , what we mean here is that it is difficult for an RNN to memorize long conversation history , and thus encoding long history means either the response given by the decoder is irrelevant to the early history , or the response will be messed up . 4. > > > '' We train m ( \u00b7 , \u00b7 ) with the 30 million crawled data through negative sampling . '' not sure I understand the connection between training $ m ( \\cdot , \\cdot ) $ and the entire model $ m ( \\cdot , \\cdot ) $ is pre-trained and used to estimate the reward function in Equation ( 9 ) . This is the only connection between $ m ( \\cdot , \\cdot ) $ and the entire model . We have clarified this in the paragraph after Equation ( 9 ) . 5. > > > the experiments are not convincing . At least , it should show the generation texts were affected about DAs in a systemic way . Only a single example in table 5 is not enough . Thanks for your comments . We do three things to show how the generated texts are affected by dialogue acts : ( 1 ) We move Table 7 in the previous version from Appendix to Section 4.2 . Now the table is Table 5 . In the table , one can see that with dialogue acts , the diversity of generated responses is significantly improved ( corresponding to much larger distinct-1 and distinct-2 ) . In the following explanation ( the third paragraph of Section 4.2 ) , we claim that this is one benefit of dialogue acts , as search space now becomes act \u00d7 language . ( 2 ) We add Section 4.4 where we compare responses from different dialogue acts using some metrics . The conclusion is that responses generated from CS . * are longer , more informative , and contain more new words than responses generated from CM . * , and statements and answers are generally more informative than questions in both CS . * and CM . * . Please refer to the new version of the paper to get more details . ( 3 ) In the last paragraph of Section 4.3 , we show that simulated dialogues without CS . * are much shorter than those with CS . * ( SL : 4.78 v.s.8.66 , RL : 2.67 v.s. , 8.18 ) . The result indicates that if we remove CS . * , then the conversation engagement of our model may degrade to the baseline model ."}, "2": {"review_id": "Bym0cU1CZ-2", "review_text": "The paper describes a technique to incorporate dialog acts into neural conversational agents. This is very interesting work. Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act. A very important type of dialog act is \"switching topic\", often done to ensure that the conversation will continue. The paper describes a classifier that predicts the dialog act of the next utterance. The next utterance is then generated based on this dialog act. The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning. This is also very interesting. The empirical evaluation demonstrates the effectiveness of the approach. The paper is also well written. I do not have any suggestion for improvement. This is good work that should be published.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments"}}